<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>讲一讲目前深度学习下基于单目的三维人体重建技术 | 四一的随写</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">讲一讲目前深度学习下基于单目的三维人体重建技术</h1><a id="logo" href="/.">四一的随写</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">讲一讲目前深度学习下基于单目的三维人体重建技术</h1><div class="post-meta">2021-09-18<span> | </span><span class="category"><a href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.3k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 5</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>近年来，基于深度学习的单目三维人体重建技术已经取得了巨大的进展。特别是基于马普所的 SMPL 参数化人体模型这块，今天就来简单聊聊这一系列的相关工作。</p>
<p align="center">
    <img width="65%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/讲一讲目前深度学习下基于单目的三维人体重建技术-20210914160852.png">
</p>

<span id="more"></span>

<p>如上图所示，给定一张 RGB 图片，我们希望恢复出图中人体的姿态和形状等信息。从前面讲的 <a target="_blank" rel="noopener" href="https://yunyang1994.gitee.io/2021/08/21/%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E6%A8%A1%E5%9E%8B-SMPL-A-Skinned-Multi-Person-Linear-Model/">SMPL 参数化人体模型</a>可知，我们只需要估计出数字人体模型的相关参数即可。例如在 HMR 算法中，则需要预测出 23x3 + 10 + 2 + 1 + 3 = 85 个参数：</p>
<ul>
<li>23 个关节点的 pose 参数，每个参数由轴角表示（axis-angle representation）</li>
<li>10 个人体形状 shape 参数</li>
<li>相机外参数 t (图像平面上的 2D 平移）和缩放尺度 s 以及根结点的旋转轴角 R</li>
</ul>
<p>对这些参数直接回归是比较困难的，所以网络预测的是相对于初始 SMPL 参数的偏移量。一般是通过两层全连接网络作为回归器 Regressor 输出偏移量，然后再与预测的 SMPL 参数拼接相加然后继续迭代 3 次得到。</p>
<p align="center">
    <img width="22%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/讲一讲目前深度学习下基于单目的三维人体重建技术-20210914172101.png">
</p>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pred_pose = init_pose</span><br><span class="line">pred_shape = init_shape</span><br><span class="line">pred_cam = init_cam</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_iter):</span><br><span class="line">    xc = torch.cat([x, pred_pose, pred_shape, pred_cam], <span class="number">1</span>)</span><br><span class="line">    xc = self.fc1(xc)</span><br><span class="line">    xc = self.drop1(xc)</span><br><span class="line">    xc = self.fc2(xc)</span><br><span class="line">    xc = self.drop2(xc)</span><br><span class="line">    pred_pose = self.decpose(xc) + pred_pose</span><br><span class="line">    pred_shape = self.decshape(xc) + pred_shape</span><br><span class="line">    pred_cam = self.deccam(xc) + pred_cam</span><br></pre></td></tr></table></figure>

<p>考虑到目前业界里 3D 动捕数据的稀缺性，而目前市面上能获得很多人工标注的 2D 关键点数据集（比如 <a target="_blank" rel="noopener" href="https://cocodataset.org/#keypoints-2020">COCO</a> 和 <a target="_blank" rel="noopener" href="https://posetrack.net/">PoseTrack</a> 等）。因此会引入 3D -&gt; 2D 的<a target="_blank" rel="noopener" href="https://github.com/mkocabas/VIBE/blob/master/lib/core/loss.py#L149"><strong>重投影损失（reprojection loss）</strong></a>，即将 SMPL 人体的 3D 关键点投影到 2D 图像上与人工标注的 2D 关节点计算损失。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/讲一讲目前深度学习下基于单目的三维人体重建技术-20210914173824.png">
</p>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定义 2D 关键点的损失函数</span></span><br><span class="line">self.criterion_keypoints = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>).to(self.device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 SMPL 人体 3D 关键点重投影到 2D 关键点</span></span><br><span class="line">pred_keypoints_2d = projection(pred_joints, pred_cam)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">keypoint2d_loss</span>(<span class="params">self, pred_keypoints_2d, gt_keypoints_2d, openpose_weight, gt_weight</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute 2D reprojection loss on the keypoints.</span></span><br><span class="line"><span class="string">    The loss is weighted by the confidence.</span></span><br><span class="line"><span class="string">    The available keypoints are different for each dataset.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    conf = gt_keypoints_2d[:, :, -<span class="number">1</span>].unsqueeze(-<span class="number">1</span>).clone()</span><br><span class="line">    conf[:, :<span class="number">25</span>] *= openpose_weight</span><br><span class="line">    conf[:, <span class="number">25</span>:] *= gt_weight              <span class="comment"># 如果没有标注，gt_weight = 0，否则为 1</span></span><br><span class="line">    loss = (conf * self.criterion_keypoints(pred_keypoints_2d, gt_keypoints_2d[:, :, :-<span class="number">1</span>])).mean()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="25%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/讲一讲目前深度学习下基于单目的三维人体重建技术-20210919142654.png">
</p>

<p>尽管 3D 数据集比较难获得，但是 3D 关键点损失也要考虑在内。它的损失函数与 2D 关键点损失一样，都是 nn.MSELoss 函数，输入则为 24 （23+1）个关键点的 3D 标注坐标和预测坐标。详见 <a target="_blank" rel="noopener" href="https://github.com/mkocabas/VIBE/blob/master/lib/core/loss.py#L161">keypoint_3d_loss 损失函数</a>。</p>
<p align="center">
    <img width="30%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/讲一讲目前深度学习下基于单目的三维人体重建技术-20210914175801.png">
</p>

<p>一些数据集如 <a target="_blank" rel="noopener" href="http://vision.imar.ro/human3.6m/description.php">Human3.6M</a> 不仅能获得人体的 3D 关键点坐标和 pose 信息，还用到了 3D scan 扫描获得人体的 mesh 信息。我们可以通过使用 Mosh 工具将这些标注数据转化成 SMPL 的（β，θ）参数，然后对它们直接进行监督，详见 <a target="_blank" rel="noopener" href="https://github.com/mkocabas/VIBE/blob/master/lib/core/loss.py#L185">smpl_losses 损失函数</a>。</p>
<p align="center">
    <img width="40%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/讲一讲目前深度学习下基于单目的三维人体重建技术-20210914180527.png">
</p>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用 MSELoss 损失函数</span></span><br><span class="line">self.criterion_regr = nn.MSELoss().to(self.device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">smpl_losses</span>(<span class="params">self, pred_rotmat, pred_betas, gt_pose, gt_betas</span>):</span></span><br><span class="line">    pred_rotmat_valid = batch_rodrigues(pred_rotmat.reshape(-<span class="number">1</span>,<span class="number">3</span>)).reshape(-<span class="number">1</span>, <span class="number">24</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    gt_rotmat_valid = batch_rodrigues(gt_pose.reshape(-<span class="number">1</span>,<span class="number">3</span>)).reshape(-<span class="number">1</span>, <span class="number">24</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    pred_betas_valid = pred_betas</span><br><span class="line">    gt_betas_valid = gt_betas</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(pred_rotmat_valid) &gt; <span class="number">0</span>:</span><br><span class="line">        loss_regr_pose = self.criterion_regr(pred_rotmat_valid, gt_rotmat_valid)</span><br><span class="line">        loss_regr_betas = self.criterion_regr(pred_betas_valid, gt_betas_valid)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loss_regr_pose = torch.FloatTensor(<span class="number">1</span>).fill_(<span class="number">0.</span>).to(self.device)</span><br><span class="line">        loss_regr_betas = torch.FloatTensor(<span class="number">1</span>).fill_(<span class="number">0.</span>).to(self.device)</span><br><span class="line">    <span class="keyword">return</span> loss_regr_pose, loss_regr_betas</span><br></pre></td></tr></table></figure>

<p>如果你觉得光靠上面几种监督方式就能实现很好地对人体三维重建，那就大错特错了。尽管人体的 2D 关键点可能预测得很准，但是<strong>由于深度的模糊性和人体关节点的自由度比较大</strong>，所以模型预测的人体姿态很容易发生扭曲，而这种扭曲形态显然是正常人类无法做到的。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/讲一讲目前深度学习下基于单目的三维人体重建技术-20210919143136.png">
</p>

<p>常用的方法是使用判别器（discriminator）对模型输出的（β，θ）参数进行监督，用来判断是否属于真实人体。在 HMR 算法中，一共使用了 K+2 个判别器，K 指的是人体的关节数目，2 则指的是 SMPL 的 β 和 θ 参数。每个判别器输出的值在 [0, 1] 范围内，代表了参数来源于真实数据的概率。</p>
<p align="center">
    <img width="60%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/讲一讲目前深度学习下基于单目的三维人体重建技术-20210919145126.png">
</p>

<p>当我们做到这里的时候已经能达到 SOTA 的效果了，但是在实际的野外场景中应用会发现：我们依然无法捕获到人类实际运动的复杂性和可变性以及准确而又自然的运动序列。在 HMR 预训练模型的基础上，VIBE 算法使用了<strong>时序编码器 GRU 来考虑上下帧人体动作的连贯性</strong>，并且增加了一个动作判别器（Motion discriminator）来判断生成器输出的人体动作序列是否接近真实人类。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/讲一讲目前深度学习下基于单目的三维人体重建技术-20210919151301.png">
</p>

<p>结束语：尽管基于 SMPL 参数的人体三维重建技术取得了一定的成就，但是由于这块领域的开拓的时间其实不是很长，我觉得未来的想象空间依然很大。SMPL 算法其实早在 2015 年就提出来了，但是这块的人体重建算法也就这两年才开始爆发和逐渐成熟。目前基于纯视觉的人体重建技术的阻力依然很大，例如基于室外场景的 3D 标注数据很稀缺、多人场景下的遮挡问题和模型的 real-time 性等，这其中的任何一个子问题都亟待我们去探索和解决。</p>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2021/09/18/%E8%AE%B2%E4%B8%80%E8%AE%B2%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF/" data-id="ckw5vsrcl002u1vra17nd6na7" data-qrcode="data:image/png;base64,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">分享</a><div class="tags"></div><div class="post-nav"><a class="next" href="/2021/09/09/FairMOT-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E9%87%8C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%86%8D%E8%AF%86%E5%88%AB%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7/">FairMOT：讨论多目标跟踪里检测与再识别的公平性</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">杂七杂八</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/">目标跟踪</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/anchor-free/" style="font-size: 15px;">anchor free</a> <a href="/tags/DeepSort/" style="font-size: 15px;">DeepSort</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/sort/" style="font-size: 15px;">sort</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/TensorRT-%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">TensorRT 部署</a> <a href="/tags/INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/" style="font-size: 15px;">INT8 加速原理</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/C-%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">C++ 编程学习</a> <a href="/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/" style="font-size: 15px;">卡尔曼滤波</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/09/18/%E8%AE%B2%E4%B8%80%E8%AE%B2%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF/">讲一讲目前深度学习下基于单目的三维人体重建技术</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/09/FairMOT-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E9%87%8C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%86%8D%E8%AF%86%E5%88%AB%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7/">FairMOT：讨论多目标跟踪里检测与再识别的公平性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/08/%E6%BB%9A%E8%9B%8B%E5%90%A7-Anchor-%E5%90%9B-%E6%97%B7%E8%A7%86%E6%96%B0%E7%A7%91%E6%8A%80-YOLOX/">滚蛋吧，Anchor 君！旷视新科技，YOLOX</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Distilling-the-Knowledge-in-a-Neural-Network/">知识蒸馏：Distilling the Knowledge in a Neural Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/05/UnitBox%E4%B8%80%E7%A7%8D%E6%96%B0%E7%9A%84-iou-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E6%8A%8A%20box-%E5%BD%93%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%95%B4%E4%BD%93%E5%8E%BB%E9%A2%84%E6%B5%8B/">UnitBox：一种新的 IoU 损失函数，把 box 当作一个整体去预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/03/%E8%A7%A3%E5%86%B3%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94Focal-Loss/">老生常谈 Focal Loss —— 解决正负样本不均衡问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/02/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS：Fully Convolutional One-Stage Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CenterNet-Objects-as-Points/">CenterNet 和 CenterTrack：以点代物，同时进行目标检测和跟踪</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CornerNet-Detecting-Objects-as-Paired-Keypoints/">CornerNet：Detecting Objects as Paired Keypoints</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.zhihu.com/people/yang-xiao-yun-tong-xue" title="我的知乎" target="_blank">我的知乎</a><ul></ul><a href="https://github.com/YunYang1994" title="我的 GitHub" target="_blank">我的 GitHub</a><ul></ul><a href="https://leetcode-cn.com/u/yunyang1994/" title="我的力扣" target="_blank">我的力扣</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的随写.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>