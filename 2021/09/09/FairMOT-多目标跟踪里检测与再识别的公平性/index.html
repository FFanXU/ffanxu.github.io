<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>FairMOT：讨论多目标跟踪里检测与再识别的公平性 | 四一的随写</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">FairMOT：讨论多目标跟踪里检测与再识别的公平性</h1><a id="logo" href="/.">四一的随写</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">FairMOT：讨论多目标跟踪里检测与再识别的公平性</h1><div class="post-meta">2021-09-09<span> | </span><span class="category"><a href="/categories/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/">目标跟踪</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 928</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 3</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>基于 anchor-free 的目标跟踪 family 又迎来了一位新成员，FairMOT. 它是在 CenterNet 基础上进行创新的，并真正意义上实现了端到端地将 Detection 和 ReId 任务进行联合训练。</p>
<p align="center">
    <img width="50%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/FairMOT-多目标跟踪里检测与再识别的公平性-20210909145011.gif">
</p>
<span id="more"></span>

<p>以往大多数的目标跟踪都是采用 Detection + ReId 的方式，没有实现 jointly 端到端地联合训练，使得算法的跟踪精度也有限。FairMOT 分析了这种结果不佳的原因，总结下来主要有三点：</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/FairMOT-多目标跟踪里检测与再识别的公平性-20210909153247.png">
</p>

<ul>
<li><p>作者认为 anchor-base 不适合 ReId 任务，应该使用 anchor-free 的方法。原因是可能会出现<strong>一个 anchor 响应多个目标或者多个 anchor 响应一个目标</strong>的情况（如上图所示），导致歧义性。如果我们只用一个中心点去看待，那么就不会出现这种情况。</p>
</li>
<li><p>现有的目标跟踪算法过度地依赖 Detection 精度，导致 ReId 任务受到不公平的忽视。ReId 任务需要高低层不同分辨率的特征融合，这在目前大多数的 Detection + ReId 框架里不太好做到。</p>
</li>
<li><p>在 MOT 中 ReID 特征的维数不宜过高，因为 MOT 的数据集一般来说都比较小。维度过高容易造成过拟合，而且显存和计算量都会增大。</p>
</li>
</ul>
<p>FairMOT 网络有两个分支：Detection 分支和 ReID 分支。Detection 分支与 CenterNet 里的基本一样，这里不做介绍，让我们重点来看看 ReID 分支。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/FairMOT-多目标跟踪里检测与再识别的公平性-20210909154840.png">
</p>

<p>ReID 分支的作用是在于输出每个目标的 embedding 向量。在 backbone 顶部设有一个 128 核的卷积层来提取<strong>每个位置</strong>的 embedding，如上图所示输出的是一个维度为 [128, H, W] 的 Re-ID Embeddings，然后喂入分类器：<a target="_blank" rel="noopener" href="https://github.com/ifzhang/FairMOT/blob/ca63d27f19e8d2170b84edb80cc2dc348c3dcd5a/src/lib/trains/mot.py#L32">nn.Linear(self.emb_dim, self.nID)</a> 计算 loss（需要注意的是，作者通过 reg_mask 对正样本进行了<strong>挑选</strong>，也就是说<strong>只有正样本才会计算 regression loss 和 ReID loss</strong>)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># https://github.com/ifzhang/FairMOT/blob/ca63d27f19e8d2170b84edb80cc2dc348c3dcd5a/src/detection_demo.py#L33</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#--------------- 每个分支和它所对应的输出维度 ---------------#</span></span><br><span class="line">reid_dim = <span class="number">128</span></span><br><span class="line">heads = &#123;<span class="string">&#x27;hm&#x27;</span>: num_classes, <span class="string">&#x27;wh&#x27;</span>: <span class="number">2</span> <span class="keyword">if</span> <span class="keyword">not</span> ltrb <span class="keyword">else</span> <span class="number">4</span>, <span class="string">&#x27;id&#x27;</span>: reid_dim, <span class="string">&#x27;reg&#x27;</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#------------ 根据 heads 字典和它的维度创建每个分支-----------#</span></span><br><span class="line"><span class="comment"># https://github.com/ifzhang/FairMOT/blob/ca63d27f19e8d2170b84edb80cc2dc348c3dcd5a/src/lib/models/networks/resnet_dcn.py#L155</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> head <span class="keyword">in</span> self.heads:          <span class="comment"># 创建每个分支</span></span><br><span class="line">    classes = self.heads[head]   <span class="comment"># classes 为每个分支的输出维度，如 hm(高斯热图): num_classes</span></span><br><span class="line">    <span class="keyword">if</span> head_conv &gt; <span class="number">0</span>:            <span class="comment">#                                  wh(宽和高): 2</span></span><br><span class="line">        fc = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, head_conv,</span><br><span class="line">            kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">True</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(head_conv, classes,           <span class="comment"># 分支的最后一层卷积核为 1x1，输出对应维度</span></span><br><span class="line">            kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>,</span><br><span class="line">            padding=<span class="number">0</span>, bias=<span class="literal">True</span>))</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;hm&#x27;</span> <span class="keyword">in</span> head:</span><br><span class="line">            fc[-<span class="number">1</span>].bias.data.fill_(-<span class="number">2.19</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            fill_fc_weights(fc)</span><br><span class="line"></span><br><span class="line"><span class="comment">#---------------------- 计算 re-ID 的 loss ------------------#</span></span><br><span class="line"><span class="comment"># https://github.com/ifzhang/FairMOT/blob/ca63d27f19e8d2170b84edb80cc2dc348c3dcd5a/src/lib/trains/mot.py#L56</span></span><br><span class="line"></span><br><span class="line">self.classifier = nn.Linear(self.emb_dim, self.nID)</span><br><span class="line">self.IDLoss = nn.CrossEntropyLoss(ignore_index=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> opt.id_weight &gt; <span class="number">0</span>:</span><br><span class="line">    id_head = _tranpose_and_gather_feat(output[<span class="string">&#x27;id&#x27;</span>], batch[<span class="string">&#x27;ind&#x27;</span>])</span><br><span class="line">    id_head = id_head[batch[<span class="string">&#x27;reg_mask&#x27;</span>] &gt; <span class="number">0</span>].contiguous()   <span class="comment"># 选出正样本</span></span><br><span class="line">    id_head = self.emb_scale * F.normalize(id_head)</span><br><span class="line">    id_target = batch[<span class="string">&#x27;ids&#x27;</span>][batch[<span class="string">&#x27;reg_mask&#x27;</span>] &gt; <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    id_output = self.classifier(id_head).contiguous()</span><br><span class="line">    id_loss += self.IDLoss(id_output, id_target)</span><br></pre></td></tr></table></figure>

<table><center><td bgcolor=Plum><font color=black>
作者很巧妙地将整个过程中每个目标 ID 设置成一个类别，这样 ReID  Loss 就变成了一个多分类交叉熵损失函数。在训练阶段，ReID task 成了多分类任务；在测试阶段，砍掉 Linear 层直接取出 embedding 向量计算余弦距离，这其实和人脸识别过程一模一样。
</font></strong></td></center></table>

<p>至于何时创建和销毁 tracker 以及它和 detection 之间怎么关联，这和 DeepSort 里的流程基本一致，具体请移步于<a target="_blank" rel="noopener" href="https://yunyang1994.gitee.io/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a>。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2004.01888">[1] FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking,</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ifzhang/FairMOT">[2] https://github.com/ifzhang/FairMOT</a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2021/09/09/FairMOT-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E9%87%8C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%86%8D%E8%AF%86%E5%88%AB%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7/" data-id="ckw5vsrau00091vraeian9y2u" data-qrcode="data:image/png;base64,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">分享</a><div class="tags"></div><div class="post-nav"><a class="pre" href="/2021/09/18/%E8%AE%B2%E4%B8%80%E8%AE%B2%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF/">讲一讲目前深度学习下基于单目的三维人体重建技术</a><a class="next" href="/2021/09/08/%E6%BB%9A%E8%9B%8B%E5%90%A7-Anchor-%E5%90%9B-%E6%97%B7%E8%A7%86%E6%96%B0%E7%A7%91%E6%8A%80-YOLOX/">滚蛋吧，Anchor 君！旷视新科技，YOLOX</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">杂七杂八</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/">目标跟踪</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/anchor-free/" style="font-size: 15px;">anchor free</a> <a href="/tags/DeepSort/" style="font-size: 15px;">DeepSort</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/sort/" style="font-size: 15px;">sort</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/TensorRT-%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">TensorRT 部署</a> <a href="/tags/INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/" style="font-size: 15px;">INT8 加速原理</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/C-%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">C++ 编程学习</a> <a href="/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/" style="font-size: 15px;">卡尔曼滤波</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/09/18/%E8%AE%B2%E4%B8%80%E8%AE%B2%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF/">讲一讲目前深度学习下基于单目的三维人体重建技术</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/09/FairMOT-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E9%87%8C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%86%8D%E8%AF%86%E5%88%AB%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7/">FairMOT：讨论多目标跟踪里检测与再识别的公平性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/08/%E6%BB%9A%E8%9B%8B%E5%90%A7-Anchor-%E5%90%9B-%E6%97%B7%E8%A7%86%E6%96%B0%E7%A7%91%E6%8A%80-YOLOX/">滚蛋吧，Anchor 君！旷视新科技，YOLOX</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Distilling-the-Knowledge-in-a-Neural-Network/">知识蒸馏：Distilling the Knowledge in a Neural Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/05/UnitBox%E4%B8%80%E7%A7%8D%E6%96%B0%E7%9A%84-iou-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E6%8A%8A%20box-%E5%BD%93%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%95%B4%E4%BD%93%E5%8E%BB%E9%A2%84%E6%B5%8B/">UnitBox：一种新的 IoU 损失函数，把 box 当作一个整体去预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/03/%E8%A7%A3%E5%86%B3%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94Focal-Loss/">老生常谈 Focal Loss —— 解决正负样本不均衡问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/02/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS：Fully Convolutional One-Stage Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CenterNet-Objects-as-Points/">CenterNet 和 CenterTrack：以点代物，同时进行目标检测和跟踪</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CornerNet-Detecting-Objects-as-Paired-Keypoints/">CornerNet：Detecting Objects as Paired Keypoints</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.zhihu.com/people/yang-xiao-yun-tong-xue" title="我的知乎" target="_blank">我的知乎</a><ul></ul><a href="https://github.com/YunYang1994" title="我的 GitHub" target="_blank">我的 GitHub</a><ul></ul><a href="https://leetcode-cn.com/u/yunyang1994/" title="我的力扣" target="_blank">我的力扣</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的随写.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>