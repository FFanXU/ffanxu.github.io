<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>CornerNet：Detecting Objects as Paired Keypoints | 四一的随写</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">CornerNet：Detecting Objects as Paired Keypoints</h1><a id="logo" href="/.">四一的随写</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">CornerNet：Detecting Objects as Paired Keypoints</h1><div class="post-meta">2021-09-01<span> | </span><span class="category"><a href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.9k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 7</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>CornerNet 是一种用于目标检测的新方法，<strong>它使用单个卷积神经网络将物体的边界框检测为一对关键点，即左上角和右下角</strong>。 通过这种新思路，它摆脱了以往目标检测中使用的 anchor-base 机制，设计了一种新的池化方式 —— <strong>角点池化（corner pooling），可以帮助网络更好地定位角点。</strong>实验表明，CornerNet 在 MS COCO 数据集上到了 <strong>42.2%</strong> 的 AP 值，碾压了当时所有的 one-stage 检测算法。</p>
<p align="center">
    <img width="60%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/CornerNet-Detecting-Objects-as-Paired-Keypoints--20210830145823.jpg">
</p>

<span id="more"></span>

<h2 id="1-CornerNet-介绍"><a href="#1-CornerNet-介绍" class="headerlink" title="1. CornerNet 介绍"></a>1. CornerNet 介绍</h2><p>现在基于 anchor-base 机制的目标检测算法的思路为：<strong>将一张图片下采样为低分辨率的特征图，在特征图的每个像素点上放置一些不同大小和宽高比的 anchor，然后让这些 anchor 框与 ground-truth 框进行响应并预测出对应的偏移量。</strong>anchor 的本质其实是候选框，由于目标的形状和位置具有多样性，因而 anchor 的数量往往会被设置得非常庞大从而保证足够多的 anchor 能与 ground-truth 框重叠。但是这也带来了以下两个缺点：</p>
<ul>
<li>需要大量的 anchor（例如 DSSD 需要 40K 个，RetinaNet 需要 100K 个），大量的 anchor 中其实只有少部分 anchor 和 ground-truth 相重合，其他则是负样本，这就造成了正负样本不均衡的局面。</li>
<li>anchor 框引入了许多超参，并且需要进行细致设计。包括 anchor 框的数量、尺寸、宽高比例。特别是在单一网络在多尺度进行预测的情况下会变得非常复杂，并且每个尺度都需要独立设计。</li>
</ul>
<p>基于上述两点原因，<strong>受 keypoint 问题的启发，就想到用关键点检测的思路来处理 detection 问题：只要找到左上角（top-left）和右下角（bottom-right）两个角点，就可以准确框出一个目标了。</strong>作者认为预测物体的角点比预测中心更容易，因为预测一个角点只需要物体的 2 个边，而预测中心却需要 4 个边。其次，角点检测的搜索复杂度仅为 O(wh)，而 proposal bboxes 的搜索复杂度却为 O(w^2h^2）（这是因为在  proposal bboxes 范围里又检索了一次特征，这导致大量  proposal bboxes 之间的特征存在冗余）。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/CornerNet-Detecting-Objects-as-Paired-Keypoints--20210830203636.png">
</p>

<p>如下图所示：CornerNet 使用一个单一网络来预测所有目标的角点热图（heat map）和 embedding 向量，找出那些最有可能的角点并根据它们之间的 embedding 向量距离来进行分组。</p>
<p align="center">
    <img width="70%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/CornerNet-Detecting-Objects-as-Paired-Keypoints--20210830161133.png">
</p>

<h2 id="2-检测角点"><a href="#2-检测角点" class="headerlink" title="2. 检测角点"></a>2. 检测角点</h2><h3 id="2-1-heatmap"><a href="#2-1-heatmap" class="headerlink" title="2.1 heatmap"></a>2.1 heatmap</h3><p>CornerNet 会输出两个热图（heat map）分别预测出所有目标的左上角和右下角的位置，每个热图的 shape 为 [C, H, W]，其中 C 是目标的类别（没有background channel），H 和 W 分别是高和宽。每个角点都有一个对应的 ground-truth 正位置，其他地方则是负位置，这里并没有对所有对负位置进行相同惩罚。<strong>这是因为在一些在离 ground-truth 一定半径范围内的角点，它们仍然能产生与 ground-truth 充分重叠的 bbox</strong>。</p>
<p align="center">
    <img width="40%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/CornerNet-Detecting-Objects-as-Paired-Keypoints--20210830164142.png">
</p>

<p>那么这个半径范围选多大呢，作者是这样说的：<strong>通过判断这些角点构成的预测框与 ground-truth 框之间的 iou 大于 t 时计算出半径</strong>，在论文里 t 设置为 0.3 。这个半径范围内，惩罚权重的衰减则由一个二维的高斯分布给出。为了减少正负样本不均衡性带来的影响，作者还用了 focal loss 来计算预测热图和真实热图之间的损失。</p>
<h3 id="2-2-location-offset"><a href="#2-2-location-offset" class="headerlink" title="2.2 location offset"></a>2.2 location offset</h3><p><strong>物体的位置精度在下采样时通常会丢失，比如图片上某个像素点 [x，y] 在经过 n 倍下采样后得到新位置为 [floor(x/n)，floor(y/n)]，floor 表示向下取整。</strong>当我们将这个新位置 remap 回原图时会偏离原来的位置，这会严重影响到一些小目标的预测。为了解决这个问题，CornerNet 采用<strong>预测位置偏移量（location offset）</strong>的方法来对角点位置进行微调。对于位置偏移量的定义，作者是这么定义的：</p>
<p align="center">
    <img width="35%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/CornerNet-Detecting-Objects-as-Paired-Keypoints--20210830181448.png">
</p>

<p>从公式中可以看出，<strong>物体角点的精确位置减去其向下取整的位置（其实就是角点左上角最近的网格）就是偏移量。这种方法没什么新奇的，和 YOLOv3 对物体中心点位置预测的思路基本一致。</strong>对于偏移量的损失，作者采用了 smooth L1 Loss 函数。</p>
<h2 id="3-角点成对"><a href="#3-角点成对" class="headerlink" title="3. 角点成对"></a>3. 角点成对</h2><p>前面介绍的角点预测工作都是孤立的，不涉及一对角点构成一个检测框的概念。受 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.05424">Newell 等人</a>姿态估计工作的启发，可以基于不同角点的 embedding 向量之间的距离来对它们进行关联。<strong>embedding 特征图的维度为 [N, H, W]，即每个角点都会产生一个 N 维的向量。考虑到 MOT 的数据集一般来说都比较小，ReID 特征的维度不宜过高，文章将 N 设置成了 1，</strong>这样 embedding 向量就成了一个标量。</p>
<p align="center">
    <img width="45%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/CornerNet-Detecting-Objects-as-Paired-Keypoints--20210830204425.png">
</p>

<p>这个向量编码了这个角点对应目标的特征，<strong>如果一个左上角和右下角属于同一个目标，那对应的这两个 embedding 向量应该很相似，因而它们之间的距离应该最小。网络在训练时，使用了 push 损失让同一个目标之间的角点距离最小，push 损失让不同目标之间的角点距离最大。</strong></p>
<h2 id="4-Corner-Pooling"><a href="#4-Corner-Pooling" class="headerlink" title="4. Corner Pooling"></a>4. Corner Pooling</h2><h3 id="4-1-角池化介绍"><a href="#4-1-角池化介绍" class="headerlink" title="4.1 角池化介绍"></a>4.1 角池化介绍</h3><p>预测框的角点通常在物体范围外，这使得角点附近没有可用的物体特征。例如为了确定像素是否在左上角，我们需要在水平方向上沿着物体的最上边界朝右看，而在垂直方向上沿着物体的最左边界朝下看。因此我们提出了一种角池化（corner pooling）操作，确保在池化过程中能够编码到整个物体的特征。</p>
<p align="center">
    <img width="30%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/CornerNet-Detecting-Objects-as-Paired-Keypoints--20210831123656.png">
</p>

<p>下面有 2 个输入特征图，宽高分别用 W 和 H 表示。<strong>假设接下来要对红点（i，j）做 corner pooling：在纵向上就要计算 (i, j) 到 (i, H) 的最大值，在横向上就要计算 (i, j) 到 (W, j) 的最大值，然后将这两个值相加即可。</strong></p>
<p align="center">
    <img width="50%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/CornerNet-Detecting-Objects-as-Paired-Keypoints--20210831124339.png">
</p>

<p>动态规划其实可以减少这个计算的复杂度，我们可以在横向上从右往左、在纵向上从下往上去扫描计算最大值，这样大大减少了复杂度。如下图所示：以 2, 1, 3, 0, 2 这一行为例，最后的 2 保持不变，倒数第二个是 max(0,2) = 2，然后倒数第三个为 max(3,2)=3 …… 依次类推。</p>
<p align="center">
    <img width="50%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/CornerNet-Detecting-Objects-as-Paired-Keypoints--20210831124856.png">
</p>

<h3 id="4-2-角池化位置"><a href="#4-2-角池化位置" class="headerlink" title="4.2 角池化位置"></a>4.2 角池化位置</h3><p>corner pooling 层放在预测模块（prediction module）里，用于预测热图和 embedding 向量。预测模块对何凯明的残差模块做了修改：将第一个 3×3 卷积模块替换为一个 corner pooling 模块，它通过两个具有 128 个通道的 3×3 卷积模块处理来自主干网的特征，然后再应用于 corner pooling 层，接着将特征合并送入 256 个通道的 3×3 Conv-BN 层中并与 short-cut 特征融合，以生成热图、embedding 向量和位置偏移量。</p>
<p align="center">
    <img width="80%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/CornerNet-Detecting-Objects-as-Paired-Keypoints--20210831130846.png">
</p>

<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwicmO2w29jyAhURwosBHSAECHUQFnoECAQQAQ&url=https://heilaw.github.io/slides/CornerNet.pptx&usg=AOvVaw3MegcZlGlGI-F7tM6Pp8qP">[1] CornerNet: Detecting Objects as Paired Keypoints - PPT</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.01244">[2] CornerNet: Detecting Objects as Paired Keypoints - paper</a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2021/09/01/CornerNet-Detecting-Objects-as-Paired-Keypoints/" data-id="ckw5vsran00021vra0duv7z78" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPYAAAD2CAAAAADAeSUUAAADLklEQVR42u3aSW7jQBAEQP//0x5gTgOMRWVWSYDYDp4ILc2O1qFUy9dXfH3/va5ff3T//+f/vR69/miF63VefGFjY2PfhP19eSWPuf7Wo/v2ul4nt2BjY2Ofyk6C0yNwGyM2oXH2SWxsbGzsJGgl6UEbAhMANjY2NvaGvSlOXeNnVGxsbOzfzM5LObMiUdtayPfw9loaNjY29sezN4Hn0+7f2N/GxsbG/kj2ptWaPCz/1mz9oQIbGxv7IHb717/dxKadPBseyveDjY2NfSr7+vGbpmxe7kmaxJvGMDY2NvZJ7P3YTXsoszCZJCqrJAQbGxv7tuw8hWjLTPnoTFtOygNetCY2Njb2zdmz7bbBaRO09sf3w3OxsbGxD2LPAtWqvfrSBCMZ3yniNjY2NvbN2fnf+rx4lK8/O+JN4xkbGxv7bPbske3Wrw90c/T14BE2Njb2bdl5+NmP7+RXklq0x/2kloaNjY19W3byofzdNhwWcfVFiVP0NjY2Nvat2JvhmFk5KUls9of1ZJ/Y2NjYB7HzQlJedp+N48xay23oikYwsbGxsW/InhXo80Npy0CzZsBw0BMbGxv7IPbmOJK0oW3KvqoUNWxdYGNjY9+E3Q7fbEZt3oGZBUtsbGzsk9h5Qzcp8bfsNr3Z/DDY2NjYZ7OTUFQU3MsV8oLRppGMjY2NfSp7U+LflJDylV9b6sLGxsY+m71vjraYzdjNpiGBjY2NfTY7SRI25aTZQOfm0H94HRsbG/s4dlvK3ychs9JS29J4EsCwsbGxj2C3j2xDS1vKbwtGSRPiyZ6xsbGxb85OvlyHhJi9KSflIztFVwQbGxv75uw8JCQbzUPdpvTfpkbY2NjY57Ff1azNU4K2CfHadKguLWFjY2N/PPu7vNrW7IzXJjDtT4WNjY19Entf6H93gyFZYb8+NjY29t3ZSdCaBbN85fw+SXWiQ8HGxsY+jt02XPMZmPwo22HNWYTCxsbG/s3sdrhnE2zaZkPbkMbGxsbGbhOAvPHQjunMnoKNjY19HntWVMq3niQM+XG/ZToJGxsb+7bsWaN3VnK63kT+mbwJvToCbGxs7E9n/wHFMc5KOZ+wgAAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/anchor-free/"><i class="fa fa-tag"></i>anchor free</a></div><div class="post-nav"><a class="pre" href="/2021/09/01/CenterNet-Objects-as-Points/">CenterNet 和 CenterTrack：以点代物，同时进行目标检测和跟踪</a><a class="next" href="/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">杂七杂八</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/">目标跟踪</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/anchor-free/" style="font-size: 15px;">anchor free</a> <a href="/tags/DeepSort/" style="font-size: 15px;">DeepSort</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/sort/" style="font-size: 15px;">sort</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/TensorRT-%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">TensorRT 部署</a> <a href="/tags/INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/" style="font-size: 15px;">INT8 加速原理</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/C-%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">C++ 编程学习</a> <a href="/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/" style="font-size: 15px;">卡尔曼滤波</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/09/18/%E8%AE%B2%E4%B8%80%E8%AE%B2%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF/">讲一讲目前深度学习下基于单目的三维人体重建技术</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/09/FairMOT-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E9%87%8C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%86%8D%E8%AF%86%E5%88%AB%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7/">FairMOT：讨论多目标跟踪里检测与再识别的公平性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/08/%E6%BB%9A%E8%9B%8B%E5%90%A7-Anchor-%E5%90%9B-%E6%97%B7%E8%A7%86%E6%96%B0%E7%A7%91%E6%8A%80-YOLOX/">滚蛋吧，Anchor 君！旷视新科技，YOLOX</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Distilling-the-Knowledge-in-a-Neural-Network/">知识蒸馏：Distilling the Knowledge in a Neural Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/05/UnitBox%E4%B8%80%E7%A7%8D%E6%96%B0%E7%9A%84-iou-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E6%8A%8A%20box-%E5%BD%93%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%95%B4%E4%BD%93%E5%8E%BB%E9%A2%84%E6%B5%8B/">UnitBox：一种新的 IoU 损失函数，把 box 当作一个整体去预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/03/%E8%A7%A3%E5%86%B3%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94Focal-Loss/">老生常谈 Focal Loss —— 解决正负样本不均衡问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/02/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS：Fully Convolutional One-Stage Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CenterNet-Objects-as-Points/">CenterNet 和 CenterTrack：以点代物，同时进行目标检测和跟踪</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CornerNet-Detecting-Objects-as-Paired-Keypoints/">CornerNet：Detecting Objects as Paired Keypoints</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.zhihu.com/people/yang-xiao-yun-tong-xue" title="我的知乎" target="_blank">我的知乎</a><ul></ul><a href="https://github.com/YunYang1994" title="我的 GitHub" target="_blank">我的 GitHub</a><ul></ul><a href="https://leetcode-cn.com/u/yunyang1994/" title="我的力扣" target="_blank">我的力扣</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的随写.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>