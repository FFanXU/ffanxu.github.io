<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>三维人体动捕模型 SMPL：A Skinned Multi Person Linear Model | 四一的随写</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">三维人体动捕模型 SMPL：A Skinned Multi Person Linear Model</h1><a id="logo" href="/.">四一的随写</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">三维人体动捕模型 SMPL：A Skinned Multi Person Linear Model</h1><div class="post-meta">2021-08-21<span> | </span><span class="category"><a href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.2k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 12</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>在<strong>人体动作捕捉</strong>（motion capture）领域，SMPL 算法最为常见，它是由德国马普所提出的一种参数化的三维人体动捕模型，具有通用性、易于渲染和兼容现有商业软件（比如 UE4 和 Unity）的优点。</p>
<p align="center">
    <img width="60%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210816165705.png">
</p>

<p>「这是一篇鸽了很久的文章，今天补上。」</p>
<span id="more"></span>

<h2 id="1-人体动捕介绍"><a href="#1-人体动捕介绍" class="headerlink" title="1. 人体动捕介绍"></a>1. 人体动捕介绍</h2><h3 id="1-1-动作捕捉技术"><a href="#1-1-动作捕捉技术" class="headerlink" title="1.1 动作捕捉技术"></a>1.1 动作捕捉技术</h3><p>目前人体动作捕捉技术在影视制作和游戏领域已经应用得很成熟了，最常见的就是基于可穿戴设备（比如 IMU）的人体动捕技术。当动作人做出运动时，穿戴的传感器会捕捉人体的姿态数据并回传给虚拟角色，进而驱动虚拟角色做出与动作人相同的角色。</p>
<p align="center">
    <img width="60%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210819170007.png">
</p>
美术师在制作一个虚拟形象模型时，会让其呈现 T-pose 摆放，并定义一套<strong>人体关节树（skeleton tree）</strong>。该关节树的特点在于每个<strong>关节点（joint）</strong>都有一个<strong>父节点（parent joint）</strong>，并且是<strong>一个父节点和一个子节点连接成一个关节</strong>。整个骨架的旋转和平移则通过<strong>根结点（root）</strong>实现，例如下图中 PELVIS 则是根节点。
<p align="center">
    <img width="50%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210819173522.png">
</p>

<table>
<thead>
<tr>
<th align="center">Index</th>
<th align="center">Joint name</th>
<th align="center">Parent joint</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0</td>
<td align="center">PELVIS</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">SPINE_NAVAL</td>
<td align="center">PELVIS</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">SPINE_CHEST</td>
<td align="center">SPINE_NAVAL</td>
</tr>
<tr>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
</tbody></table>
<p>如下图所示：<strong>每个关节点都有一套自己的坐标系，当人体在运动时，每个关节点就会相对其父节点发生旋转，这个旋转过程可以用一个四元数表达。</strong>如果两套虚拟形象的关节点坐标系朝向不一致，那么还需要进行一些适配工作（相当枯燥乏味）。</p>
<p align="center">
    <img width="50%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210819180003.png">
</p>

<h3 id="1-2-线性混合蒙皮"><a href="#1-2-线性混合蒙皮" class="headerlink" title="1.2 线性混合蒙皮"></a>1.2 线性混合蒙皮</h3><p>SMPL 涉及到游戏制作和渲染技术里面的一些东西，特别是计算机图形学领域。考虑到 SMPL 是在 LBS（Linear Blending Skinning，线性混合蒙皮）的基础上开发的，因此先对 LBS 做个简单的介绍。</p>
<p>对于一个虚拟形象，其实可以大致分为两大块：<strong>骨架（bones）</strong>和<strong>表皮（skin）</strong>。骨架一般由一套<strong>关节树（skeleton tree）</strong>构成，表皮则由一系列的<strong>网格顶点（vertices）</strong>组成，每个 vertices 都有坐标位置 xyz，然后这些 vertices 就组成了面（也就是表皮）。 在虚拟形象的美术制作过程中，通常是先制作出一套骨架，然后将这些网格顶点（皮）在 rest-pose 状态下按照一定的权重绑定在每个关节上，这个过程我们称之为<strong>蒙皮（skinning）</strong>。</p>
<p align="center">
    <img width="80%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210818173252.png">
</p>

<p>当虚拟形象发生运动时，骨骼的每个关节也会发生相应的旋转和位移，这个时候所绑定的网格顶点就需要根据每个绑定的关节点的影响加权求和算出运动后的位置。由于整个过程都是可以通过矩阵的线性运算得到，并且考虑到了所有关节点的混合影响，因此称为<strong>线性混合蒙皮</strong>。</p>
<p>LBS 主要是用来计算蒙皮后的网格顶点位置，假设虚拟人物一共有 <code>&#123;1，2，3，...，m&#125;</code> 个关节点，<code>n</code> 个网格顶点，其数学表达式如下：</p>
<p align="center">
    <img width="20%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210818175355.png">
</p>

<p><code>p&#39;</code> 为蒙皮后的网格顶点新位置，维度为 <code>[n, 3]</code>；<code>w</code> 为<strong>权重矩阵</strong>，维度为 <code>[n, m]</code>；<code>T</code> 则是每个关节点的<strong>仿射变换矩阵</strong>，维度为 <code>[m, 4, 4]</code>，该矩阵代表了关节点的旋转和平移；<code>p</code> 为蒙皮前的网格顶点位置。</p>
<h2 id="2-SMPL-模型"><a href="#2-SMPL-模型" class="headerlink" title="2. SMPL 模型"></a>2. SMPL 模型</h2><h3 id="2-1-SMPL-的背景"><a href="#2-1-SMPL-的背景" class="headerlink" title="2.1 SMPL 的背景"></a>2.1 SMPL 的背景</h3><p>LBS 面临的一个难点是：<strong>线性混合蒙皮算法会出现皮肤塌陷和皱褶的问题</strong>，作者称之为<strong> “taffy”（太妃糖） </strong>和<strong> “bowtie”（领结）</strong>。比如下图中当手臂弯曲的时候，LBS 的效果（青绿色）就折叠得比较夸张，而且在关节连接处不能提供平滑自然的过渡。<strong>目前商业上普遍的做法是通过人工绑定（rigging）和手工雕刻 blend shape 来改善这个问题，这个过程会比较耗费人力。</strong></p>
<p align="center">
    <img width="40%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210819151954.png">
</p>

<p>SMPL 较好地解决了上述的痛点，并且一开始的出发点就是为了提出业界兼容、简单易用和渲染速度快的三维人体重建模型，作者认为像这种以往需要人工绑定（rigging）和手工雕刻 blend shape 的过程其实是可以通过大量数据学习得到。</p>
<h3 id="2-2-SMPL-参数定义"><a href="#2-2-SMPL-参数定义" class="headerlink" title="2.2 SMPL 参数定义"></a>2.2 SMPL 参数定义</h3><p>SMPL 模型一共定义了 <code>N=6890</code> 个 vertices 和 <code>K=23</code> 个 joints，并且通过以下两类统计参数对人体进行描述。</p>
<ul>
<li><p><strong>体型参数 β：</strong>拥有 10 个维度去描述一个人的身材形状，每一个维度的值都可以解释为人体形状的某个指标，比如高矮，胖瘦等。</p>
</li>
<li><p><strong>姿态参数 θ：</strong>拥有 24×3 个维度去描述人体的动作姿态，其中 24 指的是 23 个关节点 + 1 个根结点，3 则指的是轴角（axis-angle）里的数值。</p>
</li>
</ul>
<p>在 python 代码中，我们可以这样随机设置 SMPL 参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置身材参数 betas 和姿态参数 poses</span></span><br><span class="line">betas = np.random.rand(<span class="number">10</span>) * <span class="number">0.03</span></span><br><span class="line">poses = np.random.rand(<span class="number">72</span>) * <span class="number">0.20</span></span><br></pre></td></tr></table></figure>

<h3 id="2-3-SMPL-的过程"><a href="#2-3-SMPL-的过程" class="headerlink" title="2.3 SMPL 的过程"></a>2.3 SMPL 的过程</h3><p>SMPL 过程主要可以分为以下三大阶段：</p>
<h4 id="1-基于形状的-blend-shape-（混合变形）"><a href="#1-基于形状的-blend-shape-（混合变形）" class="headerlink" title="1. 基于形状的 blend shape （混合变形）"></a>1. 基于形状的 blend shape （混合变形）</h4><p>人体的网格顶点（vertices）会随着 shape 参数 β 变化而变化，这个变化过程是在一个<strong>基模版（或者称之为统计上的均值模版，mean template）</strong>上线性叠加的。关于这个<strong>线性叠加偏量</strong>，作者使用了 <code>Bs(β)</code> 函数来计算：</p>
<p align="center">
    <img width="20%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210819202619.png">
</p>

<p>它表示每个 shape 参数对 vertices 的影响。其中 S （对应 <code>smpl[&#39;shapedirs&#39;]</code>）是通过数据学习出来的，它的维度为 (6890, 3, 10)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 根据 betas 调整 T-pose, 计算 vertices</span></span><br><span class="line">v_shaped = smpl[<span class="string">&#x27;shapedirs&#x27;</span>].dot(betas) + smpl[<span class="string">&#x27;v_template&#x27;</span>]  <span class="comment"># 还要与基模版相加</span></span><br></pre></td></tr></table></figure>
<h4 id="2-基于姿态的-blend-shape"><a href="#2-基于姿态的-blend-shape" class="headerlink" title="2. 基于姿态的 blend shape"></a>2. 基于姿态的 blend shape</h4><p>前面计算的是人体在静默姿态（T-pose）下的 blend shape，这里将计算人体在不同 pose 参数 θ 下的影响。同样的定义了一个函数 <code>Bp(θ)</code> 计算该线性叠加偏量：</p>
<p align="center">
    <img width="35%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210819204307.png">
</p>

<p>在上面公式中，<strong>因为我们是计算相对 T-pose 状态下的线性叠加偏量，所以人体的位姿应该也是要相对 T-pose 状态下进行变化，因此括号里减去了 T-pose 位姿的影响。</strong>每个 pose 参数都用旋转矩阵 R 表示，所以是 9K。同样 P （即权重矩阵，对应 <code>smpl[&#39;posedirs&#39;]</code>）也是通过数据学习出来的，它的维度为 (6890, 3, 207），其中 207 是因为 23x9 得到。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">posemap</span>(<span class="params">p</span>):</span></span><br><span class="line">    p = p.ravel()[<span class="number">3</span>:]   <span class="comment"># 跳过根结点</span></span><br><span class="line">    <span class="keyword">return</span> np.concatenate([(cv2.Rodrigues(</span><br><span class="line">        np.array(pp))[<span class="number">0</span>]-np.eye(<span class="number">3</span>)).ravel() <span class="keyword">for</span> pp <span class="keyword">in</span> p.reshape((-<span class="number">1</span>,<span class="number">3</span>))]).ravel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算受 pose 影响下调整臀部之后的 vertices</span></span><br><span class="line">v_posed = v_shaped + smpl[<span class="string">&#x27;posedirs&#x27;</span>].dot(utils.posemap(poses))</span><br></pre></td></tr></table></figure>
<h4 id="3-蒙皮过程（blend-skinning）"><a href="#3-蒙皮过程（blend-skinning）" class="headerlink" title="3. 蒙皮过程（blend skinning）"></a>3. 蒙皮过程（blend skinning）</h4><p>当人体关节（joint）运动时，由网格顶点（vertex）组成的“皮肤”将会随着关节的运动而变化，这个过程称之为蒙皮。<strong>蒙皮过程可以认为是皮肤节点随着关节的变化而产生的加权线性组合。</strong>简单来说，就是距离某个具体的关节越近的端点，其跟随着该关节旋转/平移等变化的影响越强。</p>
<p>由于输入的 pose 参数是每个子关节点相对父关节点进行旋转的（ relative rota- tion of part k with respect to its parent in the kinematic tree），因此需要计算每个关节坐标系变换到相机坐标系的 transform 矩阵 T：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rodrigues = <span class="keyword">lambda</span> x: cv2.Rodrigues(x)[<span class="number">0</span>]</span><br><span class="line">Ts = np.zeros([<span class="number">24</span>,<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 首先计算根结点 (0) 的相机坐标变换, 或者说是根结点相对相机坐标系的位姿</span></span><br><span class="line">T = np.zeros([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">T[:<span class="number">3</span>, :<span class="number">3</span>] = rodrigues(poses[<span class="number">0</span>])     <span class="comment"># 轴角转换到旋转矩阵，相对相机坐标而言</span></span><br><span class="line">T[:<span class="number">3</span>, <span class="number">3</span>] = J[<span class="number">0</span>]                     <span class="comment"># 根结点在相机坐标系下的位置</span></span><br><span class="line">T[<span class="number">3</span>, <span class="number">3</span>] = <span class="number">1</span>                         <span class="comment"># 齐次矩阵，1</span></span><br><span class="line">Ts[<span class="number">0</span>] = T</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算子节点 (1~24) 的相机坐标变换</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">24</span>):</span><br><span class="line">    <span class="comment"># 首先计算子节点相对父节点坐标系的位姿 [R|t]</span></span><br><span class="line">    T = np.zeros([<span class="number">4</span>,<span class="number">4</span>])</span><br><span class="line">    T[<span class="number">3</span>, <span class="number">3</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算子节点相对父节点的旋转矩阵 R</span></span><br><span class="line">    T[:<span class="number">3</span>, :<span class="number">3</span>] = rodrigues(poses[i])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算子节点相对父节点的偏移量 t</span></span><br><span class="line">    T[:<span class="number">3</span>, <span class="number">3</span>]  = J[i] - J[parent[i]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 然后计算子节点相对相机坐标系的位姿</span></span><br><span class="line">    Ts[i] = np.matmul(Ts[parent[i]], T) <span class="comment"># 乘上其父节点的变换矩阵</span></span><br><span class="line">    <span class="built_in">print</span>(Ts[i])</span><br><span class="line"></span><br><span class="line">global_joints = Ts[:, :<span class="number">3</span>, <span class="number">3</span>].copy() <span class="comment"># 所有关节点在相机坐标系下的位置</span></span><br></pre></td></tr></table></figure>

<p><strong>在人体动作捕捉领域中，描述人体关节点的刚性运动指的是每个关节点在运动时相对于静默姿态（T-pose）时的旋转平移。</strong>例如对于左腿抬起这样一个动作，1 号节点 <code>L_HIP</code> 可以通过 T1 矩阵从静默姿态变换到该姿态，并且底下的子节点都会发生相应的变换（这在上一步骤子节点乘上父节点的变换矩阵已体现）。</p>
<p align="center">
    <img width="50%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210821150131.png">
</p>

<p><strong>由于 SMPL 模型的子节点在 T-pose 状态下坐标系的朝向和相机坐标系相同</strong>，因此旋转矩阵不用发生变化, 只需要减去 T-pose 时的关节点位置得到相对偏移量就行：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 计算每个子节点相对 T-pose 时的位姿矩阵</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">24</span>):</span><br><span class="line">    R = Ts[i][:<span class="number">3</span>, :<span class="number">3</span>]</span><br><span class="line">    t = Ts[i][:<span class="number">3</span>, <span class="number">3</span>] - R.dot(J[i]) <span class="comment"># 子节点相对T-pose的偏移 t</span></span><br><span class="line">    Ts[i][:<span class="number">3</span>, <span class="number">3</span>] = t</span><br></pre></td></tr></table></figure>

<p>以上 <code>Ts</code> 就是各个子节点相对各自在 T-pose 情况下的变换矩阵（transform matrix)，该矩阵可以使得每个 vertices 在 T-pose 状态下的位置映射到发生运动时的新位置。蒙皮时还要考虑所有关节对每个 vertice 的加权影响，因此乘上一个维度为 (6890, 24) 的加权矩阵 <code>smpl[&#39;weights&#39;]</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 开始蒙皮操作，LBS 过程</span></span><br><span class="line">vertices_homo = np.matmul(smpl[<span class="string">&#x27;weights&#x27;</span>].dot(Ts.reshape([<span class="number">24</span>,<span class="number">16</span>])).reshape([-<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>]),</span><br><span class="line">        v_posed_homo.T.reshape([-<span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>]))</span><br><span class="line">vertices = vertices_homo.reshape([-<span class="number">1</span>, <span class="number">4</span>])[:,:<span class="number">3</span>]    <span class="comment"># 由于是齐次矩阵，取前3列</span></span><br></pre></td></tr></table></figure>

<p>在得到网格顶点 vertices 后，还可以通过乘上一个 J_regressor 矩阵（通过大量数据学习得到）得到每个关节点的位置 joints （与 global_joints 的值基本相同）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">joints = smpl[<span class="string">&#x27;J_regressor&#x27;</span>].dot(vertices)     <span class="comment"># 计算 pose 下 joints 位置，其值基本与 global_joints 一致</span></span><br></pre></td></tr></table></figure>

<h3 id="2-4-3D-到-2D-投影"><a href="#2-4-3D-到-2D-投影" class="headerlink" title="2.4 3D 到 2D 投影"></a>2.4 3D 到 2D 投影</h3><p><strong>上述过程中计算得到的网格顶点 vertices 和 joints 位置，都是在相机坐标系内。</strong>在一些相关的 SMPL 算法（如 vibe 和 expose）中，SMPL 的相机内参都是假定为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fx = <span class="number">5000</span></span><br><span class="line">fy = <span class="number">5000</span></span><br><span class="line">cx = width / <span class="number">2.</span> </span><br><span class="line">cy = height / <span class="number">2.</span></span><br><span class="line"></span><br><span class="line">K = np.zeros([<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">K[<span class="number">0</span>,<span class="number">0</span>] = fx</span><br><span class="line">K[<span class="number">1</span>,<span class="number">1</span>] = fy</span><br><span class="line">K[<span class="number">2</span>,<span class="number">2</span>] = <span class="number">1.</span></span><br><span class="line">K[:-<span class="number">1</span>,-<span class="number">1</span>] = np.array([cx, cy])</span><br></pre></td></tr></table></figure>

<p>这里一般不怎么涉及到相机的外参（假定为单位矩阵），因为相机的外参描述的是相机坐标系和世界坐标系之间的转换关系，这通常在虚拟角色和UE4里渲染场景相融合时才会用到。</p>
<p>有了相机的内参后，我们就可以将关节点的位置从相机坐标系变换到图像坐标系中。不妨先将三维 SMPL 人体基模版关节点投影到一张分辨率为 256x256 的图片中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">root_transl = [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">50</span>]    <span class="comment"># 根结点位移(不是位置)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基模版的关节点</span></span><br><span class="line">global_joints = smpl[<span class="string">&#x27;J_regressor&#x27;</span>].dot(smpl[<span class="string">&#x27;v_template&#x27;</span>])</span><br><span class="line">points = global_joints + root_transl</span><br><span class="line">points = points / points[:,<span class="number">2</span>:]  <span class="comment"># 归一化坐标</span></span><br><span class="line"></span><br><span class="line">projected_joints = points.dot(K.T)</span><br><span class="line">projected_joints = projected_joints[:, :<span class="number">2</span>].astype(np.<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> projected_joint <span class="keyword">in</span> projected_joints:</span><br><span class="line">    image = cv2.circle(image, <span class="built_in">tuple</span>(projected_joint), <span class="number">3</span>, [<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>], <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(projected_joints)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;skeleton.png&quot;</span>, image)</span><br></pre></td></tr></table></figure>

<p>需要说明的是：<strong>SMPL 模型一开始就假定人体是位于相机前方正中央的。但是在很多实际的场景中，人体位置复杂多变，因此就需要一个三维变量 <code>translation</code> 来描述与原始假定位置的偏移。</strong> 在上面代码中，我们依然设置人体位于相机的正前方，只不过远离了相机 50 个单元的距离，然后将三维人体关节点投影到图像中得到：</p>
<p align="center">
    <img width="60%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/三维人体模型-SMPL-A-Skinned-Multi-Person-Linear-Model-20210823180106.png">
</p>

<p>为什么人体是倒立的呢？<strong>你要知道在相机坐标系中：向前是 z 轴，向下是 y 轴，向右是 x 轴，所以基模版的关节点这么看它就是倒立了。</strong>假如你要把它变换到我们常见的世界坐标系（向前 x 轴，向右 y 轴，向上 z 轴）中，那么将它们再乘以一个相应的 transform 矩阵就行了。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a target="_blank" rel="noopener" href="https://files.is.tue.mpg.de/black/papers/SMPL2015.pdf">[1] SMPL: A Skinned Multi-Person Linear Model</a></li>
<li><a target="_blank" rel="noopener" href="http://xb.sut.edu.cn/CN/abstract/abstract1733.shtml">[2] 基于李代数的人体手臂惯性动作捕捉算法</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/YunYang1994/openwork/tree/master/smpl">[3] https://github.com/YunYang1994/openwork/tree/master/smpl</a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2021/08/21/%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E6%A8%A1%E5%9E%8B-SMPL-A-Skinned-Multi-Person-Linear-Model/" data-id="ckw5vsrb500101vra3l174zao" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEjklEQVR42u3aS4rjQBAFwL7/pT0wq4HB8nuZMrjcoVUj2yUp1JCVn5+f+Hj8Pf4/8+/56zPX5/NfPbu3ZLXrdW44MGHChAnTRzI9Lo/kYteI1+vkt57fZ8Kar4kJEyZMmL6Dab9cErATrNmayUuaUWLChAkTpt/D1KadCfddqXW+ccGECRMmTJiumfIQ+wiOfYk52V7MNjGYMGHChOmbmNrlkgduS8YJX/LprOh8Wy0cEyZMmDB9GFMbkr/p7zfON2HChAkTpg9gepRHnujmwTtvQLbpa775ePHUmDBhwoTpWKa2xbhPMtuQnCeibSOzviImTJgwYTqQaQOR32LeOs2vuBkzaovRmDBhwoTpdKa8eZkPzSSsdyXM7avNnwUTJkyYMJ3OtEk+2+GeNqHNKWcEbUqMCRMmTJhOZMpDe1IwTdqTswT1Hel3cW+YMGHChOlwplmamqeXyYO1yWf7q9lre1oLx4QJEyZMhzDNyqyzkmubyraI7xtjxYQJEyZM5zK1jcbNtiBPsNvgnTdW2/o3JkyYMGE6nSnfNbQcs7JvjpWk1vnGJfoXwYQJEyZMBzLl6WXyzdmQzV2tyn179ek3MWHChAnTsUzvDuebku4+tM9K1S/2TZgwYcKE6SuYkrC9GYhps/O2nblpvr4o8mLChAkTpgOZZqXVtonYpqn7jUg7hFTcOiZMmDBhOoRpFubfPawzY90Ui1+8DEyYMGHCdDjTLE2djc7MNhCbrcws/caECRMmTL+BaRaq89LwbFtwb7MzuhYmTJgwYTqcaVN4nbU82/bnXTTt35gwYcKE6TuYrtPF5NO2xThrW+Yvsk2VX6yJCRMmTJiOZZqVX3OgWTqdB/72xQyLvJgwYcKE6SuY3tGA3G8R8sR79lLzEVhMmDBhwnQi0yM49jTtN9uicL4puWFEFRMmTJgwHcjUBuD2wWbF3Huvm2wUnn6KCRMmTJiOZSpKnGXgT4Z7Zg+Zt12T8/kZTJgwYcJ0ItN+HCcpts4euC0Bt8lw8VIxYcKECdOxTPmttDfUln1niW5b5L1ttAgTJkyYMB3ClIfeu8q4myA9217MNgdP25mYMGHChOkopjYkb8q7+5GaNpDn163bmZgwYcKE6UCmWUieDc3clQZvNgT7lB4TJkyYMH0+0yx8zoJo0ha9a0x2tsLTe8aECRMmTIcz5cnhfgSnTWLzQvBscCfadmDChAkTpq9gSi6Zp7hJOzP5NC8B50XeuqCMCRMmTJi+gilfrh272eDmqfL+JT1dARMmTJgwHcu0AWp/tWl2tg/ZtjOLHi8mTJgwYTqK6VEet72NEvHeVmWdHmPChAkTpmOZZo/Upr5J2twG+wS6Xb9tdmLChAkTplOYkk3AbIjnHUF6U25uQz4mTJgwYfompvaB8+GeWYI6a3Oudj3Xa2LChAkTpl/AtAnJm1BdB+9Rkvxio4AJEyZMmH4l02akZpbizl5MXmJe1bkxYcKECdMHM+WJa7ItaEur7QPPtgt5+/OGWjgmTJgwYfoYprzomQfvJAC36LMHeMeamDBhwoTpEKY//1697x1R16MAAAAASUVORK5CYII=">分享</a><div class="tags"></div><div class="post-nav"><a class="pre" href="/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a><a class="next" href="/2021/08/14/%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%BF%BD%E8%B8%AASORT%E7%AE%97%E6%B3%95-Simple-Online-and-Realtime-Tracking/">多目标追踪 SORT 算法：Simple Online and Realtime Tracking</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">杂七杂八</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/">目标跟踪</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/anchor-free/" style="font-size: 15px;">anchor free</a> <a href="/tags/DeepSort/" style="font-size: 15px;">DeepSort</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/sort/" style="font-size: 15px;">sort</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/TensorRT-%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">TensorRT 部署</a> <a href="/tags/INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/" style="font-size: 15px;">INT8 加速原理</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/C-%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">C++ 编程学习</a> <a href="/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/" style="font-size: 15px;">卡尔曼滤波</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/09/18/%E8%AE%B2%E4%B8%80%E8%AE%B2%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF/">讲一讲目前深度学习下基于单目的三维人体重建技术</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/09/FairMOT-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E9%87%8C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%86%8D%E8%AF%86%E5%88%AB%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7/">FairMOT：讨论多目标跟踪里检测与再识别的公平性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/08/%E6%BB%9A%E8%9B%8B%E5%90%A7-Anchor-%E5%90%9B-%E6%97%B7%E8%A7%86%E6%96%B0%E7%A7%91%E6%8A%80-YOLOX/">滚蛋吧，Anchor 君！旷视新科技，YOLOX</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Distilling-the-Knowledge-in-a-Neural-Network/">知识蒸馏：Distilling the Knowledge in a Neural Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/05/UnitBox%E4%B8%80%E7%A7%8D%E6%96%B0%E7%9A%84-iou-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E6%8A%8A%20box-%E5%BD%93%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%95%B4%E4%BD%93%E5%8E%BB%E9%A2%84%E6%B5%8B/">UnitBox：一种新的 IoU 损失函数，把 box 当作一个整体去预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/03/%E8%A7%A3%E5%86%B3%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94Focal-Loss/">老生常谈 Focal Loss —— 解决正负样本不均衡问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/02/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS：Fully Convolutional One-Stage Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CenterNet-Objects-as-Points/">CenterNet 和 CenterTrack：以点代物，同时进行目标检测和跟踪</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CornerNet-Detecting-Objects-as-Paired-Keypoints/">CornerNet：Detecting Objects as Paired Keypoints</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.zhihu.com/people/yang-xiao-yun-tong-xue" title="我的知乎" target="_blank">我的知乎</a><ul></ul><a href="https://github.com/YunYang1994" title="我的 GitHub" target="_blank">我的 GitHub</a><ul></ul><a href="https://leetcode-cn.com/u/yunyang1994/" title="我的力扣" target="_blank">我的力扣</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的随写.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>