<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>理解英伟达 TensorRT 的 INT8 加速原理 | 四一的随写</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">理解英伟达 TensorRT 的 INT8 加速原理</h1><a id="logo" href="/.">四一的随写</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">理解英伟达 TensorRT 的 INT8 加速原理</h1><div class="post-meta">2021-06-06<span> | </span><span class="category"><a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 11</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>目前的神经网络推理大部分是利用 32bit float 类型的数据进行计算的，bit 位数的多少直接限制了数据类型能够表达的数据范围，比如 float 32 的数据是由 1bit 表示符号，8bit 表示整数部，23 位表示分数部组成。但是这种运算比较耗时和消耗计算资源，因此诞生了 int8 量化算法。</p>
<p align="center">
    <img width="70%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/理解英伟达-TensorRT-的-INT8-加速原理-20210609152045.png">
</p>


<p>int8 量化是将数据保存为 int8 格式，这样一样计算时间和占用内存大大减小。目前量化有两种方式：一种是通过训练量化 finetune 原来的模型，另一种是直接对模型和计算进行量化。后者的代表便是英伟达的方案了，目前 PPT 已经公开，但是代码并没有开源。</p>
<span id="more"></span>

<h2 id="1-量化卷积核权重"><a href="#1-量化卷积核权重" class="headerlink" title="1. 量化卷积核权重"></a>1. 量化卷积核权重</h2><p>量化的目的是为了把原来的 float32 位的卷积操作，转换为 int8 的卷积操作，这样<strong>计算就变为原来的 <code>1/4</code>，但是访存并没有变少哈，因为我们是在 kernel 里面才把 float32 变为 int8 进行计算的</strong>。</p>
<p>比如说 <code>float32</code> 位的变量 <code>a=6.238561919405008</code>，可以通过 <code>scale=23.242536 </code> 映射到 <code>int8</code> 空间上到整数 <code>a*scale=145</code>。</p>
<p align="center">
    <img width="50%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/理解英伟达-TensorRT-的-INT8-加速原理-20210609152310.png" style="max-width:70%;"></p>
 
<p> 如上所示，这个 scale 是根据最大的权重绝对值 <code>thresh</code> 决定的，然后计算 <code>127</code> 与它的比值，便得到了 <code>scale</code> 值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quantize_weight</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对该层的卷积核权重进行量化, 计算出 scale</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    weights = self.layer.weight.cpu().detach().numpy()      <span class="comment"># 剥离每一层的卷积权重</span></span><br><span class="line">    group_weights = np.array_split(weights, self.channels)  <span class="comment"># 将卷积权重按通道划分</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, group_weight <span class="keyword">in</span> <span class="built_in">enumerate</span>(group_weights):        <span class="comment"># 对每个通道的卷积权重进行遍历</span></span><br><span class="line">        max_val = np.<span class="built_in">max</span>(group_weight)</span><br><span class="line">        min_val = np.<span class="built_in">min</span>(group_weight)</span><br><span class="line"></span><br><span class="line">        thresh  = <span class="built_in">max</span>(<span class="built_in">abs</span>(max_val), <span class="built_in">abs</span>(min_val))           <span class="comment"># 求出阈值 thresh 从而求出 scale</span></span><br><span class="line">        <span class="keyword">if</span> thresh &lt; <span class="number">0.0001</span>:</span><br><span class="line">            self.weight_scales[i] = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.weight_scales[i] = <span class="number">127</span> / thresh            <span class="comment"># int8: -127 ~ 127</span></span><br></pre></td></tr></table></figure>

<p><strong>由于卷积运算是卷积核(<code>weights</code>)和数据流(<code>blob</code>)之间乘加操作，因此光对卷积核量化是不够的，还需要对数据流进行量化！</strong></p>
<h2 id="2-量化输入数据流"><a href="#2-量化输入数据流" class="headerlink" title="2. 量化输入数据流"></a>2. 量化输入数据流</h2><p>我们发现，量化的过程与数据的分布有关。如下图所示：当数据的直方图分布比较均匀时，高精度向低精度进行映射就会将刻度利用比较充分；如果分布不均匀，就会浪费很大空间。通俗地来讲，就会出现很多数字挤在一个刻度里：比如 <code>scale=23.456</code> 时，<code>6.1817</code> 和 <code>6.1823</code> 都表示成了 <code>145</code>。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/理解英伟达-TensorRT-的-INT8-加速原理-20210609152346.png" style="max-width:100%;"></p>

<p>关于上面这种直接将量化阈值设置为 <code>|max|</code> 的方法，它的显著特点是低精度的 <code>int8</code> 空间没有充分利用，因此称为<strong><font color=red>不饱和量化(no saturation quantization)</font></strong>。针对这种情况，我们可以选择一个合适的<strong>量化阈值(threshold)</strong>，舍弃那些超出范围的数进行量化，这种量化方式充分利用了低精度空间，因此称为<strong><font color=red>饱和量化(saturation quantization)</font></strong>。</p>
<p align="center">
    <img width="75%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/理解英伟达-TensorRT-的-INT8-加速原理-20210609152419.png" style="max-width:80%;"></p>

<blockquote>
<p>图中黄色部分被舍弃掉了，它们将直接被量化为 -127</p>
</blockquote>
<p>通过对比两种量化方式我们可以发现，它们各有优缺点：不饱和量化方式的量化范围大，但是可能会浪费一些低精度空间从而导致量化精度低；饱和量化方式虽然充分利用了低精度空间，但是会舍弃一些量化范围。<strong>因此这两种方式其实是一个量化精度和量化范围之间的平衡</strong>。那么如何选择合适的量化方式呢，英伟达说了：卷积核权重量化应该使用不饱和量化，数据流量化应该使用饱和量化方式。那么问题来了，对于数据流的饱和量化，<strong>怎么在数据流中找到这个最佳阈值(threshold) ？</strong></p>
<p>我们首先应该将经过网络每一层的数据流(<code>Blob</code>)给提取出来，得到它们的直方图分布。为了提取每一层的输入流，我们可以使用 <a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/understanding-register-forward-pre-hook-and-register-backward-hook/61457"><code>torch.nn.Module.register_forward_pre_hook</code></a> 函数来操作。它就像一个钩子，可以把我们想要的东西给钩出来，并且不会对数据进行修改。不妨先在 <code>QuantizeLayer</code> 设置一个 hook 函：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hook</span>(<span class="params">self, modules, <span class="built_in">input</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    VGG16 模型每次 forward 时，都会调用 QuantizeLayer.hook 函数，更新数据流的直方图分布</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    self.blob = <span class="built_in">input</span>[<span class="number">0</span>].cpu().detach().numpy().flatten()</span><br><span class="line">    max_val = np.<span class="built_in">max</span>(self.blob)</span><br><span class="line">    min_val = np.<span class="built_in">min</span>(self.blob)</span><br><span class="line">    self.blob_max = <span class="built_in">max</span>(self.blob_max, <span class="built_in">max</span>(<span class="built_in">abs</span>(max_val), <span class="built_in">abs</span>(min_val)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将数据的绝对值范围 (0, blob_max) 划分为 2048 个区间，然后计算每个区间内的数据的总数, 即一个直方图分布</span></span><br><span class="line">    count, _ = np.histogram(self.blob, bins=self.grids, <span class="built_in">range</span>=(<span class="number">0</span>, self.blob_max))</span><br><span class="line">    self.blob_count = self.blob_count + count</span><br><span class="line"></span><br><span class="line">    threshold_bin = self.quantize_blob()</span><br><span class="line">    threshold_val = (threshold_bin + <span class="number">0.5</span>) * (self.blob_max / <span class="number">2048</span>)</span><br><span class="line">    self.blob_scale = <span class="number">127</span> / threshold_val</span><br></pre></td></tr></table></figure>
<p>然后模型每次执行 <code>forward</code> 函数时，都会去执行 <code>hook</code> 函数里的内容：把该层的输入流复制给 <code>blob</code> 变量。</p>
<p><strong>跟权重量化的原理类似，数据流量化也需要找到最大绝对值 blob_max</strong>。理论上这个值应该是全局的，但是模型的测试图片数量和分布都具有不可穷举性，因此我们往往会选择一些图片进行<font color=red><strong>校准 (calibration)</strong></font>，使得<code>blob_max</code>尽量接近理论值。因此我们在上面这个钩子函数里设置了一个动态规划，数据流每次经过该层时都会对该值进行更新。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, layer <span class="keyword">in</span> model.named_modules():</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer, nn.Conv2d):</span><br><span class="line">        Qlayer = QuantizeLayer(name, layer)</span><br><span class="line">        Qlayer.quantize_weight()</span><br><span class="line">        <span class="comment"># Qlayer.quantize_blob()</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对每一层 layer 注册 hook，目的是每次 forward 时更新 blob_max 值</span></span><br><span class="line">        layer.register_forward_pre_hook(Qlayer.hook)</span><br></pre></td></tr></table></figure>

<p>在获得这个 <code>blob_max</code> 值之后，我们会在 (0, blob_max) 划分 <code>2048</code> 个刻度（你也可以划分成4096个刻度，只要你喜欢），然后统计每个刻度范围内的数字出现的个数。例如，VGG16 最后一层输入流的直方图分布为：</p>
<p align="center">
    <img width="40%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/理解英伟达-TensorRT-的-INT8-加速原理-20210609152447.png" style="max-width:60%;"></p>

<blockquote>
<p>可以看到第 <code>0</code> 个刻度的概率特别大，这表明大部分的数字都集中在(<code>-blob_max/2048, blob_max/2048</code>）区间内。</p>
</blockquote>
<p>假如我们设定阈值刻度 <code>Threshold=512</code>，因此需要将 <code>512</code> 个刻度合并成 <code>128</code> 个刻度（因为 int8 的正数范围为 0～127，一共 128 个刻度）。假设合并前 <code>(0,512)</code> 的分布为 P， 合并成 <code>(0, 127)</code> 后的分布为 Q。那么我们肯定要计算这两个分布的差异性，并希望它们之间的差异越小越好。</p>
<p align="center">
    <img width="43%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/理解英伟达-TensorRT-的-INT8-加速原理-20210609152516.png" style="max-width:80%;"></p>
    
<p>怎么计算两个分布的差异性呢？使用<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E7%9B%B8%E5%AF%B9%E7%86%B5/4233536?fromtitle=KL%E6%95%A3%E5%BA%A6&fromid=23238109&fr=aladdin"><strong><code>KL 散度</code></strong></a>就可以！它又称为交叉熵，等于概率分布的信息熵(<strong>Shannon entropy</strong>)的差值。我们可以使用<a target="_blank" rel="noopener" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html">scipy.stats.entropy(p, q)</a>进行计算，但是它要求两个输入的长度必须相等：<code>len(p) == len(q)</code>。而<code>P</code>和<code>Q</code>的长度分别为512和128，因此我们需要将 <code>Q</code> 拓展回去，其长度也变成512。例如：</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/理解英伟达-TensorRT-的-INT8-加速原理-20210609152539.png" style="max-width:100%;"></p>

<table><center><td bgcolor= LightSalmon><font color=blue>
这里是将阈值刻度 Threshold 假设为 512，但在实际过程中我们需要遍历得到。由于量化刻度为 128，因此我们从 128 起遍历至最后一个刻度，计算出每个对应的 KL 散度值，最后那个最小 KL 散度所对应的刻度即为最佳阈值。</font></strong></td></center></table>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quantize_blob</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对该层的输入数据流进行量化, 计算出最佳阈值</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    target_bin=<span class="number">128</span></span><br><span class="line">    distribution = self.blob_count[<span class="number">1</span>:] <span class="comment"># 第一刻度的量化不在考虑范围内，因为它映射到 int8 为0</span></span><br><span class="line">    length = distribution.size</span><br><span class="line">    threshold_sum = <span class="built_in">sum</span>(distribution[target_bin:])</span><br><span class="line">    kl_divergence = np.zeros(length - target_bin)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> threshold <span class="keyword">in</span> <span class="built_in">range</span>(target_bin, length):                 <span class="comment"># 遍历每个刻度值，并求出相应的 kl 散度</span></span><br><span class="line">        sliced_nd_hist = copy.deepcopy(distribution[:threshold])</span><br><span class="line"></span><br><span class="line">        p = sliced_nd_hist.copy()</span><br><span class="line">        p[threshold - <span class="number">1</span>] += threshold_sum <span class="comment"># boundary sum</span></span><br><span class="line">        threshold_sum = threshold_sum - distribution[threshold]</span><br><span class="line"></span><br><span class="line">        is_nonzeros = (p != <span class="number">0</span>).astype(np.int64)</span><br><span class="line">        quantized_bins = np.zeros(target_bin, dtype=np.int64)</span><br><span class="line">        num_merged_bins = sliced_nd_hist.size // target_bin</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(target_bin):</span><br><span class="line">            start = j * num_merged_bins</span><br><span class="line">            stop = start + num_merged_bins</span><br><span class="line">            quantized_bins[j] = sliced_nd_hist[start:stop].<span class="built_in">sum</span>()</span><br><span class="line">        quantized_bins[-<span class="number">1</span>] += sliced_nd_hist[target_bin * num_merged_bins:].<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">        q = np.zeros(sliced_nd_hist.size, dtype=np.float64)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(target_bin):</span><br><span class="line">            start = j * num_merged_bins</span><br><span class="line">            <span class="keyword">if</span> j == target_bin - <span class="number">1</span>:</span><br><span class="line">                stop = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stop = start + num_merged_bins</span><br><span class="line">            norm = is_nonzeros[start:stop].<span class="built_in">sum</span>()</span><br><span class="line">            <span class="keyword">if</span> norm != <span class="number">0</span>:</span><br><span class="line">                q[start:stop] = <span class="built_in">float</span>(quantized_bins[j]) / <span class="built_in">float</span>(norm)</span><br><span class="line">        q[p == <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        p[p == <span class="number">0</span>] = <span class="number">0.0001</span></span><br><span class="line">        q[q == <span class="number">0</span>] = <span class="number">0.0001</span></span><br><span class="line">        kl_divergence[threshold - target_bin] = stats.entropy(p, q)</span><br><span class="line"></span><br><span class="line">    min_kl_divergence = np.argmin(kl_divergence)        <span class="comment"># 求出最小的 kl 散度</span></span><br><span class="line">    threshold_bin = min_kl_divergence + target_bin      <span class="comment"># 求出最小 kl 散度对应的刻度</span></span><br><span class="line">    <span class="keyword">return</span> threshold_bin</span><br></pre></td></tr></table></figure>

<p>一旦获得了最佳阈值刻度后，我们就可以求出每层数据流的 blob_scale 值了，这一步发生在上面的 <code>hook</code> 函数中：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hook</span>(<span class="params">self, modules, <span class="built_in">input</span></span>):</span></span><br><span class="line">    ... </span><br><span class="line">    threshold_bin = self.quantize_blob()</span><br><span class="line">    threshold_val = (threshold_bin + <span class="number">0.5</span>) * (self.blob_max / <span class="number">2048</span>)</span><br><span class="line">    self.blob_scale = <span class="number">127</span> / threshold_val</span><br></pre></td></tr></table></figure>

<h2 id="3-INT8-推理过程"><a href="#3-INT8-推理过程" class="headerlink" title="3. INT8 推理过程"></a>3. INT8 推理过程</h2><p>在上面的过程，其实无非就是求<strong><font color=red>卷积核权重和每层 feature map（即数据流）</font></strong>的 scale 值。有了这个 scale 值后，就可以实现 float32 和 int8 数据类型之间的映射。</p>
<table><center><td bgcolor= LightSalmon><font color=blue>
整个 INT8 推理过程可以简述为：输入流 x 在喂入每层卷积之前，需要先乘以 blob_scale 映射为 int8 类型数据，然后得到 int8 类型的卷积结果 x。由于卷积层的偏置 bias 没有被量化，它仍然是 float32 类型，因此我们需要将卷积结果 x 再映射回 float32，然后再与偏置 bias 相加。</font></strong></td></center></table>

<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/理解英伟达-TensorRT-的-INT8-加速原理-20210803170924.gif">
</p>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    <span class="comment"># x= round(x * blobscale) →  input to int8</span></span><br><span class="line">    x = x * <span class="number">51.91119082631764</span>                     <span class="comment"># 乘以 feature map 的 scale，即 blob_scale</span></span><br><span class="line">    x = torch.<span class="built_in">round</span>(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># int8 conv due to dp4a-gpu  cudnn cublas support  we got int32 and transform to float32</span></span><br><span class="line">    x = self.conv1(x)</span><br><span class="line">    x = x - self.conv1.bias.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output float32 /div weight scale(every channel)</span></span><br><span class="line">    <span class="keyword">for</span> i,scale <span class="keyword">in</span> <span class="built_in">enumerate</span>(conv1_param_0):</span><br><span class="line">        x[:,i,:,:]/=<span class="built_in">float</span>(scale)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output float32 /div blobscale(input scale)</span></span><br><span class="line">    x = x / <span class="number">51.91119082631764</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># output = x +  conv&#x27;s fp32 bias</span></span><br><span class="line">    x = x + self.conv1.bias.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">    x = F.relu(x)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>需要说明的是：在上面的 python 代码中，我们首先将卷积结果 x 减去 bias，然后又分别按照 channel 通道除以 scale 和除以 blob_scale，最后再重新加上 bias 的值。这是因为卷积过程 <code>self.conv1(x)</code> 已经实现了 bias 相加的过程，因此我们要将结果先减去 bias 才能得到真正的卷积结果 x，由于卷积结果 x 的数据类型为 int8，所以要映射回 float32 再与 bias 相加。而实际应用的部署代码中是会将卷积计算和偏置相加的两个过程剥离开来的，这样就不用多此一举地将 bias 相减和相加了。</p>
<blockquote>
<p>思考一下，为什么英伟达的 TensorRT 没有对偏置 bias 进行 INT8 量化？我觉得可能是基于以下几点原因：</p>
</blockquote>
<ul>
<li>偏置 bias 是加法运算，其性能和开销本身就比卷积核的乘法运算要小很多。</li>
<li>NVIDIA 的研究人员已经用实验说明了偏置项量化并不太重要，并不能带来很大的性能提升。既然如此，本着奥卡姆剃刀原则，那就不必要牺牲精度来做量化。</li>
</ul>
<table><center><td bgcolor= LightSalmon><font color=blue>
最后想说说量化适合的应用场景：由于量化是牺牲了部分精度（虽然比较小）来压缩和加速网络，因此不适合精度非常敏感的任务。由于图片的信息冗余是非常大的，比如相邻一块的像素几乎都一模一样，因此用量化处理一些图像任务，比如目标检测、分类等对于精度不是非常敏感的 CV 任务而言是很适合的，但是对于一些回归任务比如深度估计就不太适合了。</font></strong></td></center></table>

<h2 id="4-如何处理-batchnorm-层"><a href="#4-如何处理-batchnorm-层" class="headerlink" title="4. 如何处理 batchnorm 层"></a>4. 如何处理 batchnorm 层</h2><p>对于卷积层之后带batchnorm的网络，因为一般在实际使用阶段，为了优化速度，batchnorm 的参数都会提前融合进卷积层的参数中，所以训练模拟量化的过程也要按照这个流程。首先把 batchnorm 的参数与卷积层的参数融合，然后再对这个参数做量化。</p>
<p align="center">
    <img width="30%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/理解英伟达-TensorRT-的-INT8-加速原理-20210803163328.png">
</p>

<p>从而可以得到新的卷积权重和偏置：</p>
<p align="center">
    <img width="42%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/理解英伟达-TensorRT-的-INT8-加速原理-20210803163525.png">
</p>



<ul>
<li><a target="_blank" rel="noopener" href="https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">[1] nvdia官方的 tensorrt-int8 文档 </a></li>
<li><a target="_blank" rel="noopener" href="https://paddleslim.readthedocs.io/zh_CN/v1.2.0/algo/algo.html#">[2] paddleslim 的 int8 量化文档，写得很赞👍 </a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2021/06/06/%E7%90%86%E8%A7%A3%E8%8B%B1%E4%BC%9F%E8%BE%BE-TensorRT-%E7%9A%84-INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/" data-id="ckw5vsrcd002d1vra819d0hd4" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEnElEQVR42u3ayY4jNxAFwPn/n24DcxqjLfV7mbQtSlEnQUsVGRRA5vLrV3x9/b6ev/7zne/X90+Tdx7d59E3k0+/f/PYhQkTJkyYXpIpGVZCM4PLF+P5r5L7P7/zD3fAhAkTJkyXM+W3S341+zTZi5+//3wJk0V9+D4mTJgwYfoApmSI+dCTwDVfkn9jsTFhwoQJ06cxJUFjsoW3ydazqV5MmDBhwvSZTPm0k+ltWE/dPy9tHs6FY8KECROmF2M61bhz4+v/ob8JEyZMmDD9h0xf5dVuz20qtr1PO/mv0YUJEyZMmO5lyptdZi0vs8ls2m7a1HMxQkyYMGHCdCHTpvElGe5s4z9V7GzfeTgqTJgwYcL0FkxR80pZAjxF2U5+NoaHn2LChAkTpmuZ2sA1D1DbKeXJ1nxRk8Wb0WPChAkTpluYTkHMipqzAmdeRj0WumPChAkTpsuZhmeHsmA5a+vJg9UZWfQpJkyYMGF6C6Y8OGz58nB31qAza+gpxoAJEyZMmC5n2gSWSah86in5VPPjRbRsmDBhwoTpWqY8pZtsvZtWnrME+XFhVr7FhAkTJkx3MSVDbJO8p6baLkbblFNv/5gwYcKE6Sqm9pHtNQtB20Tw/m4//DkwYcKECdO1TPne127weQtOXmLcPLcNhuswGBMmTJgwvSRTXmJs1fODxVdwzQqfm3kV/U2YMGHChOkSpjxM3SRwN1feHlSfg5L+JkyYMGHCdCHTJpmbPDJvrGnTxG2A3c7oh/8UJkyYMGG6hCkv7OXBZBs256nYNgCeHR3+4VeYMGHChOmNmNpE7SZwzdOsbSA9S+k+XAxMmDBhwnQ50yzpmadNk418U8KcpYaLeWHChAkTpmuZ2iabVS55kXhtjyyz8Lj4T2HChAkTpquY8i05T4+2KdrZNGbbfDK7h/PChAkTJkyXM7UNMbOUbv7ETaPqppj6cBaYMGHChOlypvwokFDmgeVsw5416KxCZUyYMGHCdCHTbLjJtDcTOPX0fYETEyZMmDDdzpQAnQpZ84HO0rizdp/6WIAJEyZMmK5i2hQmN8FnfgTZNAa1YTMmTJgwYXo/plnAmQwo2Yb3oXi+5efl2Ie/xYQJEyZMVzEljz9bsJy1/rRh8D6V/LfnYsKECROma5k27ZtJMjcJX5MDQZsgTpYwX2xMmDBhwnQ70/7HbYlx9n4bZifzaokxYcKECdONTGe39jycbrk3ZDlr0dmECRMmTJhenulU8XITuM4abjZBdf23wIQJEyZMb8GUBITJ92dlzvao0ZZCV8uJCRMmTJiuZdo3u+RJ1XoocQPQrBlodtzBhAkTJkx3Me2bcmaJ1DyVPCN4zloXQTFhwoQJ07VM+dU++FQ6OLlzO+b8WZgwYcKE6T2Y8kPAqY1z0x60CW6TZxVZAUyYMGHCdBXTvo1mnwieHU1mTTz10QcTJkyYMH0AU74/bgLUNtHcJpTzY8qBOi0mTJgwYfoApryUeKphqC3QRocVTJgwYcJ0OVO7Abd3aIPSDVl7h+gpmDBhwoTpWqY29Zl/Z9a+k/8qX5g2qJ4loDFhwoQJ04sx/QWiRN4/4/gmMAAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/TensorRT-%E9%83%A8%E7%BD%B2/"><i class="fa fa-tag"></i>TensorRT 部署</a><a href="/tags/INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/"><i class="fa fa-tag"></i>INT8 加速原理</a></div><div class="post-nav"><a class="pre" href="/2021/07/10/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E7%AE%97%E6%B3%95-%E6%B0%B8%E8%BF%9C%E6%BB%B4%E7%A5%9E/">卡尔曼滤波算法，永远滴神！</a><a class="next" href="/2021/05/23/C++_Primer-%E7%AC%AC5%E7%89%88-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"> 去年阅读 《C++ Primer 中文版(第5版)》 的印象笔记</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">杂七杂八</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/">目标跟踪</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/anchor-free/" style="font-size: 15px;">anchor free</a> <a href="/tags/DeepSort/" style="font-size: 15px;">DeepSort</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/sort/" style="font-size: 15px;">sort</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/TensorRT-%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">TensorRT 部署</a> <a href="/tags/INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/" style="font-size: 15px;">INT8 加速原理</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/C-%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">C++ 编程学习</a> <a href="/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/" style="font-size: 15px;">卡尔曼滤波</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/09/18/%E8%AE%B2%E4%B8%80%E8%AE%B2%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF/">讲一讲目前深度学习下基于单目的三维人体重建技术</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/09/FairMOT-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E9%87%8C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%86%8D%E8%AF%86%E5%88%AB%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7/">FairMOT：讨论多目标跟踪里检测与再识别的公平性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/08/%E6%BB%9A%E8%9B%8B%E5%90%A7-Anchor-%E5%90%9B-%E6%97%B7%E8%A7%86%E6%96%B0%E7%A7%91%E6%8A%80-YOLOX/">滚蛋吧，Anchor 君！旷视新科技，YOLOX</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Distilling-the-Knowledge-in-a-Neural-Network/">知识蒸馏：Distilling the Knowledge in a Neural Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/05/UnitBox%E4%B8%80%E7%A7%8D%E6%96%B0%E7%9A%84-iou-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E6%8A%8A%20box-%E5%BD%93%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%95%B4%E4%BD%93%E5%8E%BB%E9%A2%84%E6%B5%8B/">UnitBox：一种新的 IoU 损失函数，把 box 当作一个整体去预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/03/%E8%A7%A3%E5%86%B3%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94Focal-Loss/">老生常谈 Focal Loss —— 解决正负样本不均衡问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/02/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS：Fully Convolutional One-Stage Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CenterNet-Objects-as-Points/">CenterNet 和 CenterTrack：以点代物，同时进行目标检测和跟踪</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CornerNet-Detecting-Objects-as-Paired-Keypoints/">CornerNet：Detecting Objects as Paired Keypoints</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.zhihu.com/people/yang-xiao-yun-tong-xue" title="我的知乎" target="_blank">我的知乎</a><ul></ul><a href="https://github.com/YunYang1994" title="我的 GitHub" target="_blank">我的 GitHub</a><ul></ul><a href="https://leetcode-cn.com/u/yunyang1994/" title="我的力扣" target="_blank">我的力扣</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的随写.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>