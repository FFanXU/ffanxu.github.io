<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>全卷积神经网络（FCN) | 四一的世界</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">全卷积神经网络（FCN)</h1><a id="logo" href="/.">四一的世界</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">全卷积神经网络（FCN)</h1><div class="post-meta">2019-07-12<span> | </span><span class="category"><a href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.4k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 8</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>在我还是实习生的时候，我们组的 leader 讲了 FCN 网络。由于当时对图像分割还不是很了解，所以也没太听懂，只记得他当时讲这篇文章拿了 CVPR-2015 的最佳论文奖。现在学习 FCN 就觉得，这应该是图像分割领域里最经典也是最适合入门的网络了吧。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/67369222-33df5d80-f5ab-11e9-95d4-3d7813cfa0a8.png">
</p>

<span id="more"></span>

<h2 id="分割思想"><a href="#分割思想" class="headerlink" title="分割思想"></a>分割思想</h2><p>在我的代码里，使用了 VGG16 作为 backbone 来提取图片特征（其实作者也使用了 VGG19 作为backbone，但是发现效果和 VGG16 差不多)。如果把 FCN 看成是一个黑箱子，那么我们只要关心网络的输入和输出就行了。如果我们使用 VOC 数据集进行训练，输入图片的维度为 [H,W,C]，那么 FCN 输出的 feature map 形状则为 [H, W, 21]。其中，数字 21 代表的 VOC 的 20 个类别还有 1 个背景。</p>
<p>FCN 解决的实际问题就是针对图片里的每个像素进行分类，从而完成精确分割。按照以往 CNN 解决分类问题的思路，一般都会在 feature map 后面接一个全连接层，这个全连接层应该有 21 个神经元，每个神经元输出各个类别的概率。但是由于全连接的特征是一个二维的矩阵，因此我们在全连接层之前会使用 Flatten 层将三维的 feature map 展平。这就带来了2个问题：</p>
<ul>
<li>使用了 Flatten 层抹平了图片的空间信息；</li>
<li>一旦网络训练好，图片的输入尺寸将无法改变。</li>
</ul>
<p>FCN 网络很好地解决了这两个问题，它可以接受任意尺寸的输入图像，并保留了原始输入图像中的空间信息，最后直接在 feature map 上对像素进行分类。</p>
<h2 id="跳跃连接"><a href="#跳跃连接" class="headerlink" title="跳跃连接"></a>跳跃连接</h2><p>在刚开始的时候，作者将输入图片经过卷积和下采样操作一头走到尾，最后宽和高都被缩放了 32 倍。为了将 feature map 上采样到原来的尺寸，因此作者将 vgg16 的输出扩大了 32 倍，并将该模型称为 FCN-32s。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/67386859-53d14a00-f5c8-11e9-9d62-ccb1c2e61a80.jpg">
</p>

<p>但是发现FCN-32s的分割效果并不够好，如下图所示。尽管最后的 feature map 输出经过了 32 倍的上采样操作，但是图片里的边缘细节信息还是被 VGG16 网络里的卷积和下采样操作所模糊掉了。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/67385904-9003ab00-f5c6-11e9-87da-3dbf0dcb079a.png">
</p>

<p>作者把它称作是一个<strong>what</strong>和<strong>where</strong>的问题，请看下面作者的原话：</p>
<blockquote>
<p>Semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where.</p>
</blockquote>
<p>说白了就是<strong>全局信息能够预测这个物体是哪个类别，而局部的细粒度信息能够实现对物体的定位与检测</strong>。为了解决这个问题，作者通过缓慢地（分阶段地）对编码特征进行上采样，从浅层添加了“skip connections(跳跃连接)”，并将这两个特征映射相加，并最终将它上采样 8 或者 16 倍进行输出，分别称为 FCN-8s 和 FCN-16s 模型。</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/67389318-f4c20400-f5cc-11e9-9769-acb912aa8292.png" alt="image"></p>
<p>添加 skip connections 结构后，就能将深层的，粗糙的语义信息与浅层的，精细的表面信息融合起来，从而在一定程度上解决图像边缘分割效果较差的问题。</p>
<blockquote>
<p>We define a skip architecture to take advantage of this feature spectrum that combines deep, coarse, semantic information and shallow, fine, appearance information</p>
</blockquote>
<p><strong>这里需要抛出一个问题，为什么这个 “跳跃连接” 这么牛逼有效?</strong></p>
<p>这还得从感受野(Receptive Field)说起，卷积神经网络中感受野的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。再通俗点的解释是，特征图上的一个点对应输入原来图片上的区域。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/67464190-82592e80-f675-11e9-9ad0-2b4ac870eb52.png">
</p>

<p>前面讲到深层的特征图在空间尺寸上往往会越来越小，这就意味着它的感受野区域会越来越大，从而更富含图片的全局信息，能较好地解决 what 问题；浅层特征图的空间尺寸较大，这就意味着它的感受野会更小，因而容易捕捉到物体的边缘信息和丰富的细粒特征,能较好地解决 where 问题。感受野大的特征，可以很容易的识别出大物体的，但是在实际分割中，<strong>大物体边缘信息和小物体本身是很容易被深层网络一次次的降采样和一次次升采样给弄丢的，这个时候就可能需要感受野小的特征来帮助</strong>。</p>
<blockquote>
<p>在上图中，如果把 conv1 和 conv2 分别比作浅层特征和深层特征的话。那么深层特征里一个数字 “5” 的感受野尺寸就是 3x3，而浅层特征里 4 个 数字 “3” 的感受野也是这个区域，但是平均下来 1 个数字 “3” 的感受野尺寸则 1x1 都不到。</p>
</blockquote>
<p><strong>深层特征的感受野较大，浅层特征的感受野较小，它们分别解决 what 和 where 问题。反正如果将它们联合起来，那就牛逼了！</strong></p>
<h2 id="反卷积层"><a href="#反卷积层" class="headerlink" title="反卷积层"></a>反卷积层</h2><p>FCN的上采样层使用的是反卷积层，反卷积也称为转置卷积操作(Transposed convolution)。要了解反卷积是怎么回事，得先回顾一下正向卷积的实现过程。假设输入的图片 input 尺寸为 4x4，元素矩阵为:</p>
<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757491425.jpg">
</p>

<p>卷积核的尺寸为 3x3，其元素矩阵为：</p>
<p align="center">
    <img width="25%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757544885.jpg">
</p>

<p>正向卷积操作：步长 strides = 1, 填充 padding = 0,输出形状为 2x2，该过程如下图所示：</p>
<p align="center">
    <img width="20%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/no_padding_no_strides.gif">
</p>

<p>在上面这幅图中，底端为输入，上端为输出，卷积核为 3x3。如果我们用矩阵乘法去描述这个过程: 把 input 元素矩阵展开成一个列向量 X</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757584674.jpg">
</p>

<p>把输出图像 output 的元素矩阵展开成一个列向量 Y</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757621835.jpg">
</p>

<p>对于输入元素矩阵 X 和输出元素矩阵 Y ，用矩阵运算描述这个过程:</p>
<p align="center">
    <img width="9%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757652012.jpg">
</p>

<p>通过推导，我们可以获得稀疏矩阵 C</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757682778.jpg">
</p>

<p>稀疏矩阵 C 的形状为 4x16, X 形状为 16x1，Y 的形状为 4x1，将 Y 进行 reshape 后便是我们的期望输出形状 2x2。那么，反卷积的操作就是要对这个矩阵运算过程进行转置，通过输出 Y 得到输入 X：</p>
<p align="center">
    <img width="9%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757652012.jpg">
</p>

<p>从矩阵元素形状的角度出发，可以理解为：16x1=16x4x4x1，下面这个动画比较生动地描述了反卷积过程:</p>
<p align="center">
    <img width="10%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757723514.jpg">
</p>

<p>值得注意的是，反卷积操作并不是卷积操作的可逆过程，也就是说图像经过卷积操作后是不能通过反卷积操作恢复原来的样子。这是因为反卷积只是转置运算，并非可逆运算。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>在 PASCAL VOC 数据集中，每个类别对应一个色彩【RGB】, 因此我们需要对SegmentationClass文件夹里的每张 mask 图片根据像素的色彩来标定其类别，在代码 parser_voc.py是这样进行处理的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">   write_line = []</span><br><span class="line">   <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">   	pixel_color = label_image[i, j].tolist() <span class="comment"># 得到该像素点的 RGB 值</span></span><br><span class="line">        <span class="keyword">if</span> pixel_color <span class="keyword">in</span> colormap:</span><br><span class="line">       	    cls_idx = colormap.index(pixel_color) <span class="comment"># 索引该 RGB 值的类别</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cls_idx = <span class="number">0</span></span><br></pre></td></tr></table></figure>


<blockquote>
<p>考虑到在批量训练图片时的 batch_size &gt;= 1，因此必须将图片 resize 成相同的尺寸，这里采用的是最近邻插值法，从而保证新插值的像素分类问题。</p>
</blockquote>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>如果你要训练 FCN-8s 的话，还是推荐你加载 VGG16 模型的，否则会变得非常耗时。还有一点的就是，其实训练图片里的像素类别是非常不均衡的。例如 75% 的图片像素都属于背景（见上图），因此你会发现在训练时其精度很快就达到了80%，但此时的预测结果却是一片黑，即预测的类别都为背景。</p>
<p>一般对于语义分割的训练，学术界有两种办法： Patchwise training 和类别损失加权的方法来进行训练。</p>
<ul>
<li>Patchwise training: 补丁式训练方法，它旨在避免全图像训练的冗余。在语义分割中，由于要对图像中的每个像素进行分类，如果输入整个图像可能会有大量的冗余。因此在训练分割网络时，避免这种情况的一种标准方法是从训练集而不是完整图像中给网络提供成批的随机补丁（感兴趣对象周围的小图像区域）。从另一种角度出发，我们也可以使得这些补丁区域尽量减少背景信息，从而缓解类别不均衡问题。</li>
<li>类别损失加权: 根据类别数量的分布比例对各自的损失函数进行加权，比如有些样本的数量较少，我就给它的损失函数比重增大一些。</li>
</ul>
<p>对此，作者根据实验结果非常霸气地放话了：</p>
<blockquote>
<p> explore training with sampling in Section 4.3, and do not find that it yields faster or better convergence for dense prediction. Whole image training is effective and efficient.</p>
</blockquote>
<p>补丁式训练完全没有必要，训练 FCN 还是输入整张图片比较好。并且解决这种类别不均衡的问题，只需要给损失函数按比例加权重就行。最后作者还对此进行了学术上的解释，我这里就不讲了，话讲多了你们会觉得我在胡言乱语…</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Jonathan Long, Evan Shelhamer, Trevor Darrell. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1411.4038">Fully Convolutional Networks for Semantic Segmentation</a>. CVPR 2015</li>
<li>[2] TensorFlow2.0-Example code: <a target="_blank" rel="noopener" href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/5-Image_Segmentation/FCN">FCN</a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2019/07/12/FCN/" data-id="ckofioh67000cn3rabfvdbrc9" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACK0lEQVR42u3aUa7iMAwFUPa/aWYBT4V74zJS0tMvVArNyYfl2H694ut9cSXP/H3y7/NXd26+MDAwtmUkr09e8JmXfP783i9bg4GB8QDG1bI+v3LyfEv9smYMDAyMmNEG7qs7GBgYGBNG9KeDIysGBgbG5BDbHlyTb9vD7Q1ncQwMjA0ZedX9/3/+SX8DAwNjK8a7vNqCflJuy8Pr5aowMDCOZkyOoPlSJrBiPRgYGIcy8uGteTksP8omv33lJ2YMDIzNGclJ8BeNgbXxryhkY2BgHM1oq1htMa4N9+0wGQYGxqmMPAnLA+UoOJY8DAyMJzDmCVw+KpHcaTfxtjwUAwNjE0a7uDaBWwvKdYaLgYFxKKMdp5gUxdbSxIiNgYFxKGPSLGwbmZPt+JIgYmBgPICxVqBPlthu1uKgGAYGxgMY8/5nUp6b3FlMDTEwMI5grC1rHo7zqn7xJAYGxqGMdlltKpmH1xxTzIxgYGAcwZgE2XxZ88UVrQUMDIxDGXkz4K5Brnwjkt9iYGA8jdG2GNeakXlbNN8ODAyM5zDahmKeCBbDEzF11P/EwMDYhPEur2ShSdjNv40CNwYGxtGMtYNrHoInhbn23zAwMM5mtAfR+RBG2wS9IUHEwMA4gtEGvvwImj8zaktgYGBgBNllPgTWNkGjMh8GBgZGHC7bVC8JtckGYWBgPIGRj1asNTLXMD8pt2FgYGzIaEcckoSv/VX79sWmJgYGxn6Mf0Z3iYdlrOdiAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"><i class="fa fa-tag"></i>全卷积网络</a><a href="/tags/Skip-Connection/"><i class="fa fa-tag"></i>Skip Connection</a></div><div class="post-nav"><a class="pre" href="/2019/09/27/RPN/">Faster-rcnn 里的区域生成网络（RPN）</a><a class="next" href="/2019/07/09/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82%EF%BC%88Batch-Normalization/">批量归一化层（Batch Normalization)</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/04/20/%E4%BF%AE%E6%94%B9-YOLOv5-%E6%BA%90%E7%A0%81%E5%9C%A8-DOTAv1.5-%E9%81%A5%E6%84%9F%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E6%97%8B%E8%BD%AC%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">修改 YOLOv5 源码在 DOTAv1.5 遥感数据集上进行旋转目标检测</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/19/%E7%94%A8-Python-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%95%E7%9B%AE-Slam-%E4%BE%8B%E5%AD%90/">用 Python 手撸一个单目视觉里程计的例子</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/10/2D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E8%BF%87%E5%8E%BB%EF%BC%8C%E7%8E%B0%E5%9C%A8%E5%92%8C%E6%9C%AA%E6%9D%A5/">2D人体姿态估计的总结和梳理</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/16/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B9%8B%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/">人脸识别之人脸矫正</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/TensorFlow-%E7%9A%84%E5%A4%9A%E5%8D%A1-GPU-%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%88%B6/">TensorFlow 的多卡 GPU 训练机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/22/AffineTransformation/">说说图像的仿射变换</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/21/SGD/">能不能用梯度下降法求解平方根 ？</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/20/SGM_02/">手写双目立体匹配 SGM 算法（下)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/17/SGM_01/">手写双目立体匹配 SGM 算法（上)</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/29/StereoVision/">双目测距和三维重建</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/YunYang1994" title="YunYang1994's GitHub" target="_blank">YunYang1994's GitHub</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a><ul></ul><a href="http://pengzhihui.xyz" title="稚晖的个人站" target="_blank">稚晖的个人站</a><ul></ul><a href="https://wizyoung.github.io" title="CaptainChen" target="_blank">CaptainChen</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的世界.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>