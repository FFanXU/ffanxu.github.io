<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>Faster-rcnn 里的区域生成网络（RPN） | 四一的随写</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Faster-rcnn 里的区域生成网络（RPN）</h1><a id="logo" href="/.">四一的随写</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Faster-rcnn 里的区域生成网络（RPN）</h1><div class="post-meta">2019-09-27<span> | </span><span class="category"><a href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 7</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>我觉得 RPN 是目标检测领域里最经典也是最容易入门的网络了。如果你想学好目标检测，那一定不能不知道它！今天讲的 RPN 是来一篇来自 CVPR 2017 的论文 Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters， 作者在 Faster-rcnn 的 RPN 基础上进行了改进，用于行人检测。</p>
<p align="center">
    <img width="65%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-里的区域生成网络RPN-20210508221255.png">
</p>

<span id="more"></span>

<h2 id="1-网络结构"><a href="#1-网络结构" class="headerlink" title="1. 网络结构"></a>1. 网络结构</h2><p>上图是 RPN 的网络结构，它采用了 VGG16 网络进行特征提取。从 VGG16 的整体架构来看，作者为了提高 RPN 在不同分辨率图片下的检测率，分别将 Pool3 层、Pool4 层和 Pool5 层的输出进行卷积和融合得到了一个 45 x 60 x 1280 尺寸的 feature map。最后将这个 feature map 分别输入两个卷积层中得到 softmax 分类层与 bboxes 回归层。</p>
<h2 id="2-Anchor-机制"><a href="#2-Anchor-机制" class="headerlink" title="2. Anchor 机制"></a>2. Anchor 机制</h2><p>目标检测其实是生产很多框，然后在消灭无效框的过程。生产很多框的过程利用的是 Anchor 机制，消灭无效框则采用非极大值抑制过程进行处理。RPN 网络输入的图片为 720 x 960，输出的 feature map 尺寸为 45 x 60，由于它们每个点上会产生 9 个 anchor boxes，因此最终一共会得到 45 x 60 x 9 个 anchor boxes。</p>
<p align="center">
    <img width="50%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-里的区域生成网络RPN-20210508221316.png">
</p>

<p>直接利用这些 anchor boxes 对真实框进行预测会有些困难，因此作者采用了 <strong><font color=red>anchor boxes 与 ground-truth boxes 的偏移量机制</font></strong>进行回归预测。</p>
<p align="center">
    <img width="45%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-里的区域生成网络RPN-20210508221322.jpg">
</p>

<p><code>x, y, w, h</code> 分别表示 boxes 的中心坐标和宽高；变量 <code>x, x_&#123;a&#125;, x^&#123;*&#125;</code> 则分别代表预测框，anchor 框和 ground-truth 框的中心坐标 <code>x</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_regression</span>(<span class="params">box1, box2</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    box1: predicted boxes</span></span><br><span class="line"><span class="string">    box2: anchor boxes</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    target_reg = np.zeros(shape=[<span class="number">4</span>,])</span><br><span class="line">    w1 = box1[<span class="number">2</span>] - box1[<span class="number">0</span>]</span><br><span class="line">    h1 = box1[<span class="number">3</span>] - box1[<span class="number">1</span>]</span><br><span class="line">    w2 = box2[<span class="number">2</span>] - box2[<span class="number">0</span>]</span><br><span class="line">    h2 = box2[<span class="number">3</span>] - box2[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    target_reg[<span class="number">0</span>] = (box1[<span class="number">0</span>] - box2[<span class="number">0</span>]) / w2</span><br><span class="line">    target_reg[<span class="number">1</span>] = (box1[<span class="number">1</span>] - box2[<span class="number">1</span>]) / h2</span><br><span class="line">    target_reg[<span class="number">2</span>] = np.log(w1 / w2)</span><br><span class="line">    target_reg[<span class="number">3</span>] = np.log(h1 / h2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> target_reg</span><br></pre></td></tr></table></figure>

<h2 id="3-损失函数"><a href="#3-损失函数" class="headerlink" title="3. 损失函数"></a>3. 损失函数</h2><p>RPN 的损失函数和 YOLO 非常像，不过从发表论文时间顺序来看，应该是 YOLO 借鉴了 RPN 。在 Faster-rcnn 论文里，RPN 的损失函数是这样的:</p>
<ul>
<li>为了训练 RPN， 我们首先给每个 anchor boxes 设置了两个标签，分别为 0: 背景, 1: 前景；</li>
<li>与 ground-truth boxes 重合度 (iou) 最高的那个 anchor boxes 设置为正样本;</li>
<li>只要这个 anchor boxes 与任何一个 ground-truth boxes 的 iou 大于 0.7，那么它也是一个正样本；</li>
<li>如果 anchor boxes 与所有的 ground-truth boxes 的 iou 都小于 0.3， 那么它就是一个负样本，表示不包含物体；</li>
<li>在前面这几种情况下，已经能够产生足够多的正、负样本了，剩下的则既不是正样本，也不是负样本，它们不会参与到 RPN 的 loss 的计算中去。</li>
</ul>
<p>在我的代码 <a target="_blank" rel="noopener" href="https://github.com/YunYang1994/TensorFlow2.0-Examples/blob/master/4-Object_Detection/RPN/demo.py">demo.py</a> 里将正负样本都可视化出来了，大家只要配置好 image 和 label 的路径然后直接执行 python demo.py 就可以看到以下图片。</p>
<p align="center">
    <img width="50%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-里的区域生成网络RPN-20210508221332.png">
</p>

<p>在上图中，蓝色框为 anchor boxes，它们就是正样本，红点为这些正样本 anchor boxes 的中心位置，黑点表示的是负样本 anchor boxes 的中心位置。从图中可以看出：在有人的区域，正样本框的分布比较密集，并且红点都在人体中心区域；而在没有人的区域则布满了黑点,它们表示的是负样本，都属于背景。</p>
<p>在前面讲到，RPN 网络预测的是 <strong><font color=red>anchor boxes 与 ground-truth boxes 的偏移量</font></strong>，那如果我们将这些正样本 anchor boxes 的偏移量映射回去的话：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">=&gt; Decoding positive sample: 20, 20, 0</span><br><span class="line">=&gt; Decoding positive sample: 20, 20, 7</span><br><span class="line">...</span><br><span class="line">=&gt; Decoding positive sample: 36, 31, 1</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="50%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-里的区域生成网络RPN-20210508221338.png">
</p>

<p>你会发现，这就是 ground-truth boxes 框（绿色框）和物体中心点（红色点）的位置。事实上，RPN 的损失是一个多任务的 loss function，集合了分类损失与回归框损失，它们两者之间的优化可以通过 λ 系数去实现平衡。</p>
<p align="center">
    <img width="70%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-里的区域生成网络RPN-20210508221630.jpg">
</p>

<p>初次看这个损失函数有点迷，<strong><font color=red>它其实是一个 smooth-L1 损失函数, 它的优点在于解决了 L1 损失函数在 0 点附近的不可导问题，而且相比于 L2 损失函数而言，它在训练初始阶段的梯度回传会更加稳定</font></strong>。如下图所示，正负样本都会参与到分类损失的反向传播中去（因为你需要告诉网络什么是正样本和负样本），而回归框的损失只有正样本参与计算（只有正样本才有回归框损失，负样本作为背景是没有回归框损失的)。</p>
<p align="center">
    <img width="47%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-里的区域生成网络RPN-20210508221635.jpg">
</p>

<p>其中：</p>
<p align="center">
    <img width="45%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-里的区域生成网络RPN-20210508221744.jpg">
</p>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">target_scores, target_bboxes, target_masks, pred_scores, pred_bboxes</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    target_scores shape: [1, 45, 60, 9, 2],  pred_scores shape: [1, 45, 60, 9, 2]</span></span><br><span class="line"><span class="string">    target_bboxes shape: [1, 45, 60, 9, 4],  pred_bboxes shape: [1, 45, 60, 9, 4]</span></span><br><span class="line"><span class="string">    target_masks  shape: [1, 45, 60, 9]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    score_loss = tf.nn.softmax_cross_entropy_with_logits(labels=target_scores, logits=pred_scores)</span><br><span class="line">    foreground_background_mask = (np.<span class="built_in">abs</span>(target_masks) == <span class="number">1</span>).astype(np.<span class="built_in">int</span>)</span><br><span class="line">    score_loss = tf.reduce_sum(score_loss * foreground_background_mask, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) / np.<span class="built_in">sum</span>(foreground_background_mask)</span><br><span class="line">    score_loss = tf.reduce_mean(score_loss)</span><br><span class="line"></span><br><span class="line">    boxes_loss = tf.<span class="built_in">abs</span>(target_bboxes - pred_bboxes)</span><br><span class="line">    boxes_loss = <span class="number">0.5</span> * tf.<span class="built_in">pow</span>(boxes_loss, <span class="number">2</span>) * tf.cast(boxes_loss&lt;<span class="number">1</span>, tf.float32) + (boxes_loss - <span class="number">0.5</span>) * tf.cast(boxes_loss &gt;=<span class="number">1</span>, tf.float32)</span><br><span class="line">    boxes_loss = tf.reduce_sum(boxes_loss, axis=-<span class="number">1</span>)</span><br><span class="line">    foreground_mask = (target_masks &gt; <span class="number">0</span>).astype(np.float32)</span><br><span class="line">    boxes_loss = tf.reduce_sum(boxes_loss * foreground_mask, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) / np.<span class="built_in">sum</span>(foreground_mask)</span><br><span class="line">    boxes_loss = tf.reduce_mean(boxes_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> score_loss, boxes_loss</span><br></pre></td></tr></table></figure>

<h2 id="4-k-means-造框"><a href="#4-k-means-造框" class="headerlink" title="4. k-means 造框"></a>4. k-means 造框</h2><p>如果 Anchor boxes 的尺寸选得好，那么就使得网络更容易去学习。刚开始我以为反正网络预测的都是 Bounding Boxes 的偏移量，那么 Anchor boxes 尺寸就没那么重要了。但我在复现算法和写代码的过程中发现，看来我还是太年轻了。我使用的是 synthetic_dataset 数据集进行训练，该数据集里所有检测的目标都为 “person”，假如我直接用作者论文里的原始 anchor，那么得到的正样本为如下左图；而如果我使用 k-means算法对该数据集所有的 ground-truth boxes 进行聚类得到的 anchor，那么效果就如下右图所示，显然后者的效果比前者好得多。</p>
<table>
<thead>
<tr>
<th align="center">论文原始 anchor</th>
<th align="center">k-means 的 anchor</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-%E9%87%8C%E7%9A%84%E5%8C%BA%E5%9F%9F%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9CRPN-20210508221346.png" alt="论文原始 anchor"></td>
<td align="center"><img src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-%E9%87%8C%E7%9A%84%E5%8C%BA%E5%9F%9F%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9CRPN-20210508221350.png" alt="k-means 的 anchor"></td>
</tr>
</tbody></table>
<p>不仅如此，事实上一些其他超参数也会影响正负样本的分布情况，从而直接影响到网络的学习过程。所有这些事实都告诉我们，学习神经网络不能靠从网上看一些浅显的教程就够了的，关键还得自己去多多看源码并实践，才能成为一名合格的深度学习炼丹师。</p>
<table>
<thead>
<tr>
<th align="center">pos_thresh=0.2, neg_thresh=0.1</th>
<th align="center">pos_thresh=0.7, neg_thresh=0.2</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-%E9%87%8C%E7%9A%84%E5%8C%BA%E5%9F%9F%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9CRPN-20210508221358.png" alt="pos_thresh=0.2, neg_thresh=0.1"></td>
<td align="center"><img src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/Faster-rcnn-%E9%87%8C%E7%9A%84%E5%8C%BA%E5%9F%9F%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9CRPN-20210508221405.png" alt="pos_thresh=0.7, neg_thresh=0.2"></td>
</tr>
</tbody></table>
<p>最后在测试集上的效果，还是非常赞的! 训练的 score loss基本降到了零，boxes loss 也是非常非常低。但是由于是 RPN 网络，所以我们又不能对它抱太大期望，不然 Faster-RCNN 后面更精确的回归层和分类层意义就不大了。按照对这个算法的理解，我用 TensorFlow 对它进行了复现，感兴趣的话可以看看<a target="_blank" rel="noopener" href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/4-Object_Detection/RPN">这里</a>。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.01497">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>, CVPR 2016</li>
<li>[2] Shiyu Huang. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.06283.pdf">Expecting the Unexpected:Training Detectors for Unusual Pedestrians with Adversarial Imposters</a>, CVPR 2017</li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2019/09/27/Faster-rcnn-%E9%87%8C%E7%9A%84%E5%8C%BA%E5%9F%9F%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9CRPN/" data-id="ckw5vsraw000c1vra40yx18q4" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEkElEQVR42u3aSW7jQBAEQP//0xpgrh7RmVX0QC0FT4Ys9RI8dNfy9RU/j7/P9d/tCN+fZ/9NVvL9t9effP3GgwkTJkyYXpLpcfkkkyVLebbtfOkz6Nk4//gtJkyYMGE6nCkfLtnG5qqRzJtfF9orxdPPMWHChAnTBzC1276+dlz/ahbWJoE6JkyYMGHClCd2r4PPOry8xJ0d/EmQjAkTJkyYPoFpM1y+xORoT5LFs2tKMu8NuXBMmDBhwvRiTHc17pz4980PJkyYMGF6MaZH+bQlzPZo31wvZkXTaNeYMGHChOlYpjbsnCVek1Jlm+RNvtk2Hv3wTUyYMGHCdCBTXozMj9iEKT/s2xlnzUA/jIkJEyZMmA5nalth8kVsGn1mM+YherEqTJgwYcJ0OFN70Oap0vzonaVlZ6nkOrTGhAkTJkxvxLQpCibHeTvCJjxuX+fTNWDChAkTpsOZ9jQtUJ4ITsbcFGLrJC8mTJgwYTqKKQdqNzBrx2lD0z1cngLAhAkTJkznMm2O+ZZgP8ssAb15nZgwYcKE6T2YZgf2pqEnP/Jnl4zkxRThOiZMmDBhOpCpTcXOCoeb9p1Ncjbf3Q+vHBMmTJgwHc7ULjefoG2UyS8HbdPPLOeNCRMmTJjeg2lzDN8VyubEs/B7swZMmDBhwnQuUx645sttuZNyZjJjwpc/mDBhwoTpE5jylpq7EPOtbhK4m2AYEyZMmDCdxTQrLraNNcmFoD3m87RvPuPTcTBhwoQJ0+FMebibH7ezA76+xYzSzXU4jQkTJkyYDmdKps+LiLMGnbuSvHm4m8+LCRMmTJhOZ5otsQ1KZ8T3BsDtXJgwYcKE6XSm2ZNv4xE8s7k2l4z9vjBhwoQJ0ylMeQtLsty8kJlfEZL08aaYGo2JCRMmTJgOZ0oO7CRE3KC0ZdH2k2QvT3eNCRMmTJiOZZo1uOwTuJvk8iy92768f3wHEyZMmDAdyDQ7dDftPrPgNl/VpsgaNe5gwoQJE6ajmGbp3X2C+K4UcJ6GblEwYcKECdN7MN1bXJxhtZeJWTK6TVJHuQFMmDBhwnQI06zNZVYanM24oZy11T5tUcWECRMmTEcxtU0wm/B40/RzveF9+060fkyYMGHCdCBTG7jm32kvAfkVYfbJpjkJEyZMmDCdy3Rv00yekP2NNHEbQicXl69kS5gwYcKE6YWZ6kTn4sBuy6XtOHmZs+bDhAkTJkxvwXR99LY0s+B205STXGvaQB0TJkyYML0T012B6ybUbK8ReVi+eW2YMGHChOlcpgQon3Lz21mQ3DLlIXSdF8eECRMmTC/JdNdh34bN16FyG3jnoe/wdWLChAkTpmOZ9kdysvTfK2dGKdpy/LaYigkTJkyYXp8pvwS0V4RN802brs3XkDf3YMKECROmd2JqD+w8GG4LlneVOWep5KcvAxMmTJgwfQBTkjzdJH+T60hehsyLoNFcmDBhwoTpY5ja4l++lDxR2xYmhzSYMGHChOmNmPIkb35FmM01S93mfMMrDiZMmDBhOpbp/6RZ24JicVTHLT6zV4sJEyZMmI5l+gP7Tckqe8IqSwAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/Faster-rcnn/"><i class="fa fa-tag"></i>Faster-rcnn</a></div><div class="post-nav"><a class="pre" href="/2019/10/01/Think-different/">Think different</a><a class="next" href="/2019/07/12/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CFCN/">全卷积神经网络（FCN)</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">杂七杂八</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/">目标跟踪</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/anchor-free/" style="font-size: 15px;">anchor free</a> <a href="/tags/DeepSort/" style="font-size: 15px;">DeepSort</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/sort/" style="font-size: 15px;">sort</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/TensorRT-%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">TensorRT 部署</a> <a href="/tags/INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/" style="font-size: 15px;">INT8 加速原理</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/C-%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">C++ 编程学习</a> <a href="/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/" style="font-size: 15px;">卡尔曼滤波</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/09/18/%E8%AE%B2%E4%B8%80%E8%AE%B2%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF/">讲一讲目前深度学习下基于单目的三维人体重建技术</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/09/FairMOT-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E9%87%8C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%86%8D%E8%AF%86%E5%88%AB%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7/">FairMOT：讨论多目标跟踪里检测与再识别的公平性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/08/%E6%BB%9A%E8%9B%8B%E5%90%A7-Anchor-%E5%90%9B-%E6%97%B7%E8%A7%86%E6%96%B0%E7%A7%91%E6%8A%80-YOLOX/">滚蛋吧，Anchor 君！旷视新科技，YOLOX</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Distilling-the-Knowledge-in-a-Neural-Network/">知识蒸馏：Distilling the Knowledge in a Neural Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/05/UnitBox%E4%B8%80%E7%A7%8D%E6%96%B0%E7%9A%84-iou-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E6%8A%8A%20box-%E5%BD%93%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%95%B4%E4%BD%93%E5%8E%BB%E9%A2%84%E6%B5%8B/">UnitBox：一种新的 IoU 损失函数，把 box 当作一个整体去预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/03/%E8%A7%A3%E5%86%B3%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94Focal-Loss/">老生常谈 Focal Loss —— 解决正负样本不均衡问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/02/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS：Fully Convolutional One-Stage Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CenterNet-Objects-as-Points/">CenterNet 和 CenterTrack：以点代物，同时进行目标检测和跟踪</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CornerNet-Detecting-Objects-as-Paired-Keypoints/">CornerNet：Detecting Objects as Paired Keypoints</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.zhihu.com/people/yang-xiao-yun-tong-xue" title="我的知乎" target="_blank">我的知乎</a><ul></ul><a href="https://github.com/YunYang1994" title="我的 GitHub" target="_blank">我的 GitHub</a><ul></ul><a href="https://leetcode-cn.com/u/yunyang1994/" title="我的力扣" target="_blank">我的力扣</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的随写.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>