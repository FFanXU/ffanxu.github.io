<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>基于卷积神经网络的 2D-to-3D 视频转换 | 四一的世界</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">基于卷积神经网络的 2D-to-3D 视频转换</h1><a id="logo" href="/.">四一的世界</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">基于卷积神经网络的 2D-to-3D 视频转换</h1><div class="post-meta">2019-12-10<span> | </span><span class="category"><a href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.3k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 5</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>目前制作 3D 电影的方法有两种：一种是直接用昂贵的立体相机设备进行拍摄，这种制作成本非常庞大。另一种则是通过图像处理技术将 2D 电影转化成 3D 格式，这种转换处理通常依赖于“深度艺术家”，他们手工地为每一帧创造深度图，然后利用标准的基于深度图像的渲染算法将与原始图像相结合，得到一个立体的图像对，这需要大量的人力成本。现在来说，每年只有 20 左右部新的 3D 电影发行。</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Deep3D/Deep3D_01.png" alt="image"></p>
<span id="more"></span>

<p>在这样强烈需求的工业背景下，这篇文章的目的虽然是为了解决如何利用神经网络将 2D 电影转化成具有立体感的 3D 视频的问题，并且不需要人力来标注图片的深度信息。但是它提出的方法太新颖(很多论文都引用了，可见影响力)，所以也把它拎出来讲。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>作者的网络如上图所示：双目图片的左图作为模型的输入，每经过一次卷积和池化层后都会有两个分支：分支 1 会进行下一个卷积池化层进行特征提取，而分支 2 会进入反卷积层进行上采样得到一个与原图分辨率一致的视差图 (disparity map)，如此反复经过 5 层循环，得到 5 个视差图。最终作者会将这 5 个视差图相加，然后再经过一层卷积层并使用 softmax 激活函数，最后会输出一个与原图分辨率一致的视差概率分布 (probalistic disparity map)。</p>
<p>其实该网络的结构设计和其他语义分割类型的网络大同小异，这里没什么讲的。关键是它的损失函数设计以及 image-to-image 的方法，揭开了深度估计无监督学习的序幕。所以让我们直奔主题，给你一张左图和视差图，你如何去重构出右图？</p>
<h2 id="重构右图"><a href="#重构右图" class="headerlink" title="重构右图"></a>重构右图</h2><p>我们的直觉做法是将左视角点向右平移视差 D 个单位，然后便得到了右视角点。由于受极线约束，因此计算复杂度为 o(n)。但是这个方法在神经网络里无法进行反向传播，因为它对视差 D 是不可导的，因此我们无法训练。针对这个问题，作者引入了视差的概率分布对网络进行优化。利用左视角点和视差概率分布对右视角点进行重构的过程如下公式所示：</p>
<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Deep3D/MommyTalk1600758530618.jpg">
</p>

<ul>
<li><code>O_&#123;i, j&#125;</code> 表示在图片坐标 (i, j) 上重构的右视角点</li>
<li><code>I_&#123;i, j&#125;^&#123;d&#125;</code> 表示左视角点 (i, j) 平移 d 个位置后得到的右视角</li>
<li><code>D_&#123;i, j&#125;^&#123;d&#125;</code> 表示左视角点 (i, j) 的视差概率分布在视差 d 时的概率值</li>
</ul>
<h2 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h2><p>上述公式有点晦涩难懂，我琢磨了半天，写了个小程序进行实践：假如我们现在有一对分辨率为 200x200 的双目图片，整张图片上的像素视差都是 20。为了感受视差的偏移性质，我们在图片的中间区域设置了一块 10x10 的白点。可以看到从左图到右图，小白点很明显地移动了一小段距离，这就是视差造成的。</p>
<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Deep3D/lr_img.png">
</p>

<p>我们假定整张图片的最大视差值为 30， 那么就需要划分 0,1,…,30 共 31 个等级。因此图片的视差概率分布的形状为 [200， 200， 31]，由于真实的视差值为 20， 因此该等级属于 onehot 状态，接着左图上每个像素点在每个等级 i 上都会向右平移 i 个单位，这样一来我们便总共得到了 31 张图片， 程序里用 shift_images 表示，最后再将它与视差的概率分布相乘并求和便得到重构的右图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">l_img = np.zeros([<span class="number">200</span>, <span class="number">200</span>])</span><br><span class="line">l_img[<span class="number">95</span>:<span class="number">105</span>, <span class="number">95</span>:<span class="number">105</span>] = <span class="number">255</span></span><br><span class="line"></span><br><span class="line">r_img = np.zeros([<span class="number">200</span>, <span class="number">200</span>])</span><br><span class="line">r_img[<span class="number">95</span>:<span class="number">105</span>, <span class="number">115</span>:<span class="number">125</span>] = <span class="number">255</span></span><br><span class="line"></span><br><span class="line">cv2.imwrite(<span class="string">&quot;l_img.jpg&quot;</span>, l_img)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;r_img.jpg&quot;</span>, r_img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设整张图片的视差都是20</span></span><br><span class="line">gt_disp = np.ones([<span class="number">200</span>, <span class="number">200</span>]) * <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># construct disparity map</span></span><br><span class="line">max_disp = <span class="number">30</span></span><br><span class="line">prob_disp = np.zeros([<span class="number">200</span>, <span class="number">200</span>, max_disp])</span><br><span class="line">prob_disp[:, :, <span class="number">20</span>] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">shift_images = np.zeros(shape=[<span class="number">200</span>, <span class="number">200</span>, max_disp])</span><br><span class="line"><span class="keyword">for</span> disp <span class="keyword">in</span> <span class="built_in">range</span>(max_disp):</span><br><span class="line">    shift_images[:, :, disp] = np.roll(l_img, disp, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">pred_r_img = np.<span class="built_in">sum</span>(shift_images * prob_disp, axis=<span class="number">2</span>)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;pred_r_img.jpg&quot;</span>, pred_r_img)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;reconstruction loss: &quot;</span>, np.<span class="built_in">sum</span>(pred_r_img - r_img)) <span class="comment"># 0.0</span></span><br></pre></td></tr></table></figure>

<p>由于我们给的是真实的视差概率分布，因此重构损失(reconstruction loss)的值为0. 反过来：如果重构损失不为 0， 那么神经网络将会朝着预测正确的视差概率分布去优化。最后我们将预测出来的右图和真实的右图进行了对比，结果一致。</p>
<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Deep3D/pred_rimg.png">
</p>

<p>总结： 这篇文章的新颖之处在于，通过 image-to-image 训练的方式，打开了深度估计网络通往无监督训练的大门。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Junyuan Xie, Ross Girshick, Ali Farhadi. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.03650">Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks</a>, CVPR 2016</li>
<li>[2] Deep3D: <a target="_blank" rel="noopener" href="https://github.com/piiswrong/deep3d">Automatic 2D-to-3D Video Conversion with CNNs</a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2019/12/10/Deep3D/" data-id="ckofioh630007n3ra5r7a5vtv" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACIklEQVR42u3aS27DMAwFwNz/0u42QGDlkVIKWBqtitSWMy7A8qPXK17X2/r8/O7Kz9/e7Tx+yrKFgYHxWMY1XPk1My9iwXMxMDAOYCShMAms87zxc28/x8DAwBgGwZnrk4CLgYGB0Qu440DcK3ExMDAwxow88FVL2V8HZQwMjBMYean5/z//ZL6BgYHxKMZVXEnCt3bkEH0rDAyMrRl5gOsh84SyGnAxMDDOYYwDZbJpFdA7FvYFhoGBsTUjb8fPt/tnEsEIjIGBsSmjl96t4iWj0OguDAyMAxjVIxS9QjffLT+QgYGBcRqj2tbPDz1UE8fqDhgYGHszetvlLbNkbDnTaCv/ZTAwMB7IqCZheUBcMFnNhw0YGBhbM3492qwevFjVvMPAwNiJ0Rs6zofCmdRz6q1jYGA8lpGHyLwNl7+O/EV8yXAxMDC2Zqw6bNH7PKfe/qvAwMDYmlHNr3ot/vzeJLksT2UxMDAOYPSOViQtswVJ4fw0FQMD4yGMwsQgbsCtghV2w8DAOIBRva3aYqumg+V9MDAwjmEkBWR+aKxX3PbCPQYGxq6Mq7jyL9TboZd6YmBg7M3IV96A6x3gyJPRXuGNgYHxdEYeIntlZ3XAkFz5ZTCAgYGxKWNV4Fs8We3V4hgYGBhBIM5HnuOnF4pYDAwMjCBoJulmXsrmrwYDA+McRt7KnxlVrg36GBgY5zDy0rEXcAtBs3U0DQMDY2vGH7Zn8C6RRhGDAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/"><i class="fa fa-tag"></i>深度估计</a></div><div class="post-nav"><a class="pre" href="/2019/12/23/CameraParam/">相机的内参和外参</a><a class="next" href="/2019/11/18/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C-%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%B0%E8%A7%86%E7%95%8C/">可变形卷积网络：计算机新“视”界</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/04/20/%E4%BF%AE%E6%94%B9-YOLOv5-%E6%BA%90%E7%A0%81%E5%9C%A8-DOTAv1.5-%E9%81%A5%E6%84%9F%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E6%97%8B%E8%BD%AC%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">修改 YOLOv5 源码在 DOTAv1.5 遥感数据集上进行旋转目标检测</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/19/%E7%94%A8-Python-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%95%E7%9B%AE-Slam-%E4%BE%8B%E5%AD%90/">用 Python 手撸一个单目视觉里程计的例子</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/10/2D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E8%BF%87%E5%8E%BB%EF%BC%8C%E7%8E%B0%E5%9C%A8%E5%92%8C%E6%9C%AA%E6%9D%A5/">2D人体姿态估计的总结和梳理</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/16/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B9%8B%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/">人脸识别之人脸矫正</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/TensorFlow-%E7%9A%84%E5%A4%9A%E5%8D%A1-GPU-%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%88%B6/">TensorFlow 的多卡 GPU 训练机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/22/AffineTransformation/">说说图像的仿射变换</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/21/SGD/">能不能用梯度下降法求解平方根 ？</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/20/SGM_02/">手写双目立体匹配 SGM 算法（下)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/17/SGM_01/">手写双目立体匹配 SGM 算法（上)</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/29/StereoVision/">双目测距和三维重建</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/YunYang1994" title="YunYang1994's GitHub" target="_blank">YunYang1994's GitHub</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a><ul></ul><a href="http://pengzhihui.xyz" title="稚晖的个人站" target="_blank">稚晖的个人站</a><ul></ul><a href="https://wizyoung.github.io" title="CaptainChen" target="_blank">CaptainChen</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的世界.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>