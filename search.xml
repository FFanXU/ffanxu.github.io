<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>说说图像的仿射变换</title>
    <url>/2020/01/22/AffineTransformation/</url>
    <content><![CDATA[<p>仿射变换（<strong>Affine Transformation</strong>）是图像处理中很常见的操作，它在数学上可以表述为乘以一个矩阵 (线性变换) 接着再加上一个向量 (平移)。</p>
<p align="center">
    <img width="25%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Affine/MommyTalk1600746596629.jpg">
</p>

<p>其中：</p>
<p align="center">
    <img width="35%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Affine/MommyTalk1600746710209.jpg">
</p>

<span id="more"></span>

<h2 id="更紧凑的表示"><a href="#更紧凑的表示" class="headerlink" title="更紧凑的表示"></a>更紧凑的表示</h2><p>在学术上，更习惯用一个 2x3 的 M 矩阵来表示这层关系，因此得到:</p>
<p align="center">
    <img width="18%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Affine/MommyTalk1600746755271.jpg">
</p>

<p>其中：</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Affine/MommyTalk1600746798994.jpg">
</p>


<h2 id="如何求-M-？"><a href="#如何求-M-？" class="headerlink" title="如何求 M ？"></a>如何求 M ？</h2><p>如果用线性方程表示它们之间的转换关系，便得到：</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Affine/MommyTalk1600746863415.jpg">
</p>


<p>方程中有 6 个未知的参数，如果我们需要求解它们，则至少需要 6 个方程。由于每个像素点都包含了 2 个方程，因此只需要 3 个像素点。好在 OpenCV 提供了函数 <font color=Red><code>cv2.getAffineTransform</code></font>来根据变换前后 3 个点的对应关系来自动求解 M，例如对于图片</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Affine/dog.jpg">
</p>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">image = cv2.imread(<span class="string">&quot;./dog.jpg&quot;</span>)</span><br><span class="line">h, w = image.shape[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<p>我们选取图片的三个顶点进行仿射变换，它们分别是左上角：[0, 0]，左下角：[0, h-1]，右上角：[w-1, 0]。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">src_points = np.float32([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, h-<span class="number">1</span>], [w-<span class="number">1</span>, <span class="number">0</span>]]) </span><br><span class="line">dst_points = np.float32([[<span class="number">50</span>, <span class="number">50</span>], [<span class="number">200</span>, h-<span class="number">100</span>], [w-<span class="number">100</span>, <span class="number">200</span>]]) </span><br><span class="line">matAffine = cv2.getAffineTransform(src_points, dst_points)</span><br></pre></td></tr></table></figure>

<p>变量<code>matAffine</code>就是仿射变换矩阵 <code>M</code>，如果将它打印出来：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">array([[9.38086304e-01, 4.14937759e-02, 5.00000000e+01],</span><br><span class="line">       [3.12695435e-02, 9.17842324e-01, 5.00000000e+01]])</span><br></pre></td></tr></table></figure>

<h2 id="仿射变换"><a href="#仿射变换" class="headerlink" title="仿射变换"></a>仿射变换</h2><p>现在可以将刚才求出的仿射变换应用至源图像，使用的是 <code>cv2.warpAffine</code> 函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">affine_result = cv2.warpAffine(image, matAffine, (w,h)) </span><br><span class="line">cv2.imwrite(<span class="string">&quot;affine_result.jpg&quot;</span>, affine_result)</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Affine/affine_result.jpg">
</p>





]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>仿射变换</tag>
      </tags>
  </entry>
  <entry>
    <title>相机的内参和外参</title>
    <url>/2019/12/23/CameraParam/</url>
    <content><![CDATA[<p>相机的内参和外参是立体视觉的基础，今天做个笔记记录下。</p>
<h2 id="相机模型"><a href="#相机模型" class="headerlink" title="相机模型"></a>相机模型</h2><p>照片的本质是真实的 3D 场景在相机的成像平面上留下的一个投影，最早的相机是在小孔成像的基础上发展起来的，下面这幅图简单地解释了相机的成像过程。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraParam/01.png">
</p>

<span id="more"></span>

<p>现在来对这个简单的针孔模型进行几何建模。设 <code>O − x − y − z</code> 为相机坐标系，习惯上我们让 <code>z</code> 轴指向相机前方，<code>x</code> 向右，<code>y</code> 向下。<code>O</code> 为摄像机的光心，也是针孔模型中的针孔。现实世界的空间点 <code>P</code> 经过小孔 <code>O</code> 投影之后，落在物理成像平面 <code>O&#39;-x&#39;-y&#39;</code> 上，成像点为 <code>P&#39;</code>。设 <code>P</code> 的坐标为 <code>[X, Y, Z]</code>，<code>P&#39;</code> 为 <code>[x&#39;, Y&#39;, Z&#39;]</code> 并且设物理成像平面到小孔的距离为 <code>f</code>(焦距)。那么根据三角形相似关系</p>
<p align="center">
    <img width="21%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraParam/MommyTalk1600752018793.jpg">
</p>

<p>通过整理便得到：</p>
<p align="center">
    <img width="12%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraParam/MommyTalk1600752199460.jpg">
</p>

<h2 id="相机内参"><a href="#相机内参" class="headerlink" title="相机内参"></a>相机内参</h2><p>上式描述了点 P 和它的像之间的空间关系。不过在相机中，我们最终获得的是 一个个的像素，这需要在成像平面上对像进行采样和量化。</p>
<p>像素坐标系通常的定义方式是：原点 <code>O&#39;</code> 位于图像的左上角，<code>u</code> 轴向右与 <code>x</code> 轴平行，<code>v</code> 轴向下与 <code>y</code> 轴平行。<font color=OrangeRed>因此，像素坐标系与成像平面之间，相差了一个缩放和一个原点的平移。</font></p>
<p>我们设像素坐标在 $u$ 轴上缩放了 <code>α</code> 倍，在 <code>v</code> 上缩放了 <code>β</code> 倍。同时，原点平移了 <code>[c_x, c_y]</code>。那么，<code>P&#39;</code> 的坐标与像素坐标 <code>[u, v]</code> 的关系为:</p>
<p align="center">
    <img width="18%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraParam/MommyTalk1600752330250.jpg">
</p>

<p>把 <code>αf</code> 合并成 <code>f_&#123;x&#125;</code>，<code>βf</code> 合并成 <code>f_&#123;y&#125;</code>，得:</p>
<p align="center">
    <img width="21%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraParam/MommyTalk1600752386899.jpg">
</p>

<p>其中，<code>f</code> 的单位为米，<code>α, β</code> 的单位为像素每米，所以 <code>fx , fy</code> 的单位为像素。把该式写成矩阵形式，会更加简洁，不过左侧需要用到齐次坐标:</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraParam/MommyTalk1600752440675.jpg">
</p>

<p>综上，可以整理得到一个非常简洁的公式如下：</p>
<p align="center">
    <img width="15%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraParam/MommyTalk1608809010870.jpg">
</p>

<p align="center">
    <img width="42%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraParam/MommyTalk1608808840490.jpg">
</p>

<p>其中，我们把矩阵 K 称为<font color=OrangeRed>相机的内参(Camera Intrinsics)，它描述了相机坐标系到图像坐标系之间的投影关系。</font></p>
<h2 id="相机外参"><a href="#相机外参" class="headerlink" title="相机外参"></a>相机外参</h2><p>在上面的推导过程中，我们使用的是 <code>P_&#123;c&#125;</code> 在相机坐标系下的坐标。如果我们使用世界坐标系下的 <code>P_&#123;w&#125;</code> 的话，那么就应该使用相机的当前位姿变换到相机坐标系下：</p>
<p align="center">
    <img width="26%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraParam/MommyTalk1608811852149.jpg">
</p>

<p>其中，<font color=OrangeRed>相机的位姿 <code>R, t</code> 又称为相机的外参数 (Camera Extrinsics)，它描述了点 P 的世界坐标到相机坐标的投影关系。</font>因此世界坐标系下点 P 投影到图像坐标系的整个过程为：</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraParam/MommyTalk1608811797060.jpg">
</p>]]></content>
      <categories>
        <category>立体视觉</category>
      </categories>
      <tags>
        <tag>立体视觉</tag>
        <tag>相机参数</tag>
      </tags>
  </entry>
  <entry>
    <title>什么是相机的位姿 ？</title>
    <url>/2019/12/27/CameraPose/</url>
    <content><![CDATA[<p>在视觉 slam 领域里，相机的位姿是一个特别重要的概念。简单来说，相机的位姿（pose）就是相机的位置和姿态的合称，它描述了世界坐标系与相机坐标系之间的转换关系。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraPose/RT.png">
</p>

<span id="more"></span>

<p>如上图所示：点 <code>P</code> 的世界坐标为 <code>P_&#123;w&#125;</code>，可以通过相机的位姿矩阵 <code>T</code> 转换到相机坐标系下为 <code>P_&#123;c&#125;</code> ：</p>
<p align="center">
    <img width="17%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraPose/MommyTalk1608812694553.jpg">
</p>

<p>当然你可以将点 <code>P</code> 从相机坐标系转换到世界坐标系中：</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraPose/MommyTalk1608812750071.jpg">
</p>

<p>其中 <code>T_&#123;cw&#125;</code> 为该点从世界坐标系变换到相机坐标系的变换矩阵， <code>T_&#123;wc&#125;</code> 为该点从相机坐标系变换到世界坐标系的变换矩阵。<strong>它们二者都可以用来表示相机的位姿，前者称为相机的外参</strong>。</p>
<blockquote>
<p>实践当中使用 <code>T_&#123;cw&#125;</code> 来表示相机位姿更加常见。然而在可视化程序中使用 <code>T_&#123;wc&#125;</code> 来表示相机位姿更为直观，因为此时它的平移向量即为相机原点在世界坐标系中的坐标。视觉 Slam 十四讲中的第五讲的 joinMap 使用的就是 <code>T_&#123;wc&#125;</code> 来表示相机位姿进行点云拼接。</p>
</blockquote>
<p>相机位姿矩阵 <code>T</code> 其实主要由旋转矩阵 <code>R</code> 和平移向量 <code>t</code> 组成：</p>
<p align="center">
    <img width="24%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraPose/01.png">
</p>

<p>其中旋转矩阵 <code>R</code> 一共有 9 个量，但是一次旋转只有 3 个自由度，因此这种表达方式是冗余的。可以使用欧拉角来描述这种旋转行为，它使用了 3 个分离的转角，把一个旋转分解成了3次绕不同轴的旋转，如下所示：</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraPose/angle.jpg">
</p>

<p>因此旋转矩阵 <code>R</code> 可以由三个转角来表示，它们分别是：</p>
<ul>
<li>偏航角 yaw，绕物体的 <code>Z</code> 轴旋转的角度, 用 <code>gamma</code> 表示；</li>
<li>俯仰角 pitch，<font color=Red><strong>旋转之后</strong></font>绕 <code>Y</code> 轴旋转的角度, 用 <code>alpha</code> 表示；</li>
<li>滚转角 roll，<font color=Red><strong>旋转之后</strong></font>绕 <code>X</code> 轴旋转的角度, 用 <code>beta</code> 表示；</li>
</ul>
<p>既然欧拉角可以表示物体的旋转状态，那么旋转矩阵 <code>R</code> 应该也能被这个三个角度所表示:首先，旋转矩阵 R 可以被三个矩阵分解得到</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraPose/MommyTalk1600755628141.jpg">
</p>

<p>其中：</p>
<p align="center">
    <img width="90%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraPose/MommyTalk1600755686966.jpg">
</p>


<p>因此它们三者相乘便得到旋转矩阵 <code>R</code> 的表达形式:</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraPose/MommyTalk1600755742741.jpg">
</p>

<p>使用这种方法表示的一个重大缺点就是会碰到著名的<font color=red>万向锁问题</font>：在俯仰角为正负 90 度时，第一次旋转与第三次旋转将会使用同一个轴，使得系统失去了一个自由度（由 3 次旋转变成了 2 次旋转）。理论上可以证明，只要想用 3 个实数来表达三维旋转，都不可避免遇到这种问题。</p>
<p align="center">
    <img width="80%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/CameraPose/Lock.png">
</p>


<p>参考文献：</p>
<ul>
<li><a href="https://blog.csdn.net/CharmingSun/article/details/97445425">《视觉SLAM十四讲》相机位姿与相机外参的区别与联系</a></li>
</ul>
]]></content>
      <categories>
        <category>立体视觉</category>
      </categories>
      <tags>
        <tag>相机位姿</tag>
        <tag>视觉 Slam</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch 的数据集准备</title>
    <url>/2018/06/11/Dataloader/</url>
    <content><![CDATA[<p>深度学习的绝大部分工作都是在准备数据集，<strong>Pytorch</strong> 提供了很多工具使数据加载变得更简单。在本节内容中，我们来看看是如何利用 <strong>torch.utils.data.DataLoader</strong> 加载数据集的。</p>
<p>首先需要 import 一些必要的库：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> skimage</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, utils</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br></pre></td></tr></table></figure>

<p>然后从<a href="https://download.pytorch.org/tutorial/faces.zip">这里</a>下载一个名为 <strong>faces</strong> 的文件夹，该文件夹里包含了一些 <strong>68 个特征点（part_0 ~ part_67)</strong> 的人脸图片和 <code>face_landmarks.csv</code> </p>
<p align="center">
    <img width="20%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/DataLoader/facial_lmks.jpg">
</p>

<span id="more"></span>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">landmarks_frame = pd.read_csv(<span class="string">&#x27;faces/face_landmarks.csv&#x27;</span>)</span><br><span class="line">landmarks_frame.head()</span><br></pre></td></tr></table></figure>

<h2 id="Dataset-class"><a href="#Dataset-class" class="headerlink" title="Dataset class"></a>Dataset class</h2><p><code>torch.utils.data.Dataset</code> 是一个抽象的类，我们构造的数据集需要继承它得到，并且重载下面 2 个成员函数：</p>
<ul>
<li><code>__len__</code> 函数，通过<code>len(dataset)</code>返回数据集大小；</li>
<li><code>__getitem__</code> 函数，通过索引<code>dataset[i]</code>而得到一个样本。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FaceLandmarksDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Face Landmarks dataset.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, csv_file, root_dir, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_file (string): Path to the csv file with annotations.</span></span><br><span class="line"><span class="string">            root_dir (string): Directory with all the images.</span></span><br><span class="line"><span class="string">            transform (callable, optional): Optional transform to be applied</span></span><br><span class="line"><span class="string">                on a sample.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.landmarks_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.landmarks_frame)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line"></span><br><span class="line">        img_name = os.path.join(self.root_dir, self.landmarks_frame.iloc[idx, <span class="number">0</span>])</span><br><span class="line">        image = skimage.io.imread(img_name)</span><br><span class="line">        </span><br><span class="line">        landmarks = self.landmarks_frame.iloc[idx, <span class="number">1</span>:]</span><br><span class="line">        landmarks = np.array([landmarks])</span><br><span class="line">        landmarks = landmarks.astype(<span class="string">&#x27;float&#x27;</span>).reshape(-<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        sample = &#123;<span class="string">&#x27;image&#x27;</span>: image, <span class="string">&#x27;landmarks&#x27;</span>: landmarks&#125;</span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure>

<p>现在我们可以对 <code>FaceLandmarksDataset</code> 类构建一个实例 <code>face_dataset</code>。其中每个样本都是一个字典，分别是 <code>&#39;image&#39;</code> 和 <code>&#39;landmarks&#39;</code> 。我们可以索引第 65 个样本将它们的数组形状打印出来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">face_dataset = FaceLandmarksDataset(csv_file=<span class="string">&#x27;faces/face_landmarks.csv&#x27;</span>,</span><br><span class="line">                                    root_dir=<span class="string">&#x27;faces/&#x27;</span>)</span><br><span class="line">sample = face_dataset[<span class="number">65</span>]             <span class="comment"># 取第 65 个样本</span></span><br><span class="line"><span class="built_in">print</span>(sample[<span class="string">&#x27;image&#x27;</span>].shape)          <span class="comment"># (160, 160, 3)</span></span><br><span class="line"><span class="built_in">print</span>(sample[<span class="string">&#x27;landmarks&#x27;</span>].shape)      <span class="comment"># (68, 2)</span></span><br></pre></td></tr></table></figure>

<h2 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h2><p>上述过程完成了对人脸图片和 65 个特征点的读取，接下来需要对它们进行一些预处理操作。本文将介绍 3 种 Transforms 操作：</p>
<ul>
<li>Rescale，对图片进行 <code>resize</code> 操作</li>
<li>RandomCrop，随机地裁剪图片</li>
<li>ToTensor，将 numpy 的 <code>array</code> 类型转变为 torch 的 <code>tensor</code> 类型</li>
</ul>
<h3 id="Rescale"><a href="#Rescale" class="headerlink" title="Rescale"></a>Rescale</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rescale</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Rescale the image in a sample to a given size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        output_size (tuple or int): Desired output size. If tuple, output is</span></span><br><span class="line"><span class="string">            matched to output_size. If int, smaller of image edges is matched</span></span><br><span class="line"><span class="string">            to output_size keeping aspect ratio the same.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, output_size</span>):</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(output_size, <span class="built_in">tuple</span>)</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, sample</span>):</span></span><br><span class="line">        image, landmarks = sample[<span class="string">&#x27;image&#x27;</span>], sample[<span class="string">&#x27;landmarks&#x27;</span>]</span><br><span class="line">        </span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        img = skimage.transform.resize(image, (new_h, new_w))</span><br><span class="line">        landmarks = landmarks * [new_w / w, new_h / h]</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;image&#x27;</span>: img, <span class="string">&#x27;landmarks&#x27;</span>: landmarks&#125;</span><br></pre></td></tr></table></figure>
<p>然后我们对人脸图片的尺寸 resize 到 (256, 256)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rescale_transform = Rescale((<span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">rescale_sample = rescale_transform(sample)</span><br><span class="line"><span class="built_in">print</span>(rescale_sample[<span class="string">&#x27;image&#x27;</span>].shape)           <span class="comment"># (256,  256, 3)</span></span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="27%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/DataLoader/facial_lmks.jpg">
</p>

<h3 id="RandomCrop"><a href="#RandomCrop" class="headerlink" title="RandomCrop"></a>RandomCrop</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomCrop</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Crop randomly the image in a sample.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        output_size (tuple): Desired output size is made.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, output_size</span>):</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(output_size, <span class="built_in">tuple</span>)</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, sample</span>):</span></span><br><span class="line">        image, landmarks = sample[<span class="string">&#x27;image&#x27;</span>], sample[<span class="string">&#x27;landmarks&#x27;</span>]</span><br><span class="line"></span><br><span class="line">        h, w = image.shape[:<span class="number">2</span>]</span><br><span class="line">        new_h, new_w = self.output_size</span><br><span class="line"></span><br><span class="line">        top = np.random.randint(<span class="number">0</span>, h - new_h)</span><br><span class="line">        left = np.random.randint(<span class="number">0</span>, w - new_w)</span><br><span class="line"></span><br><span class="line">        image = image[top: top + new_h, left: left + new_w]</span><br><span class="line">        landmarks = landmarks - [left, top]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;image&#x27;</span>: image, <span class="string">&#x27;landmarks&#x27;</span>: landmarks&#125;</span><br></pre></td></tr></table></figure>
<p>然后我们对人脸图片进行随机裁剪，裁剪的尺寸大小为 (128, 128)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">crop_transform = RandomCrop((<span class="number">128</span>, <span class="number">128</span>))</span><br><span class="line">crop_sample = crop_transform(sample)</span><br><span class="line"><span class="built_in">print</span>(crop_sample[<span class="string">&#x27;image&#x27;</span>].shape)           <span class="comment"># (128,  128, 3)</span></span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="16%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/DataLoader/RandomCrop.jpg">
</p>

<h3 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor"></a>ToTensor</h3><p>现在需要使用 <code>torch.from_numpy</code> 函数将数据转化成 <code>tensor</code>，在进行这项操作之前，考虑到 torch 的图片输入顺序为 <code>[C, H, W]</code>，而 numpy 的图片顺序为 <code>[H, W, C]</code>，因此需要通过 transpose 转化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToTensor</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Convert ndarrays in sample to Tensors.&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, sample</span>):</span></span><br><span class="line">        image, landmarks = sample[<span class="string">&#x27;image&#x27;</span>], sample[<span class="string">&#x27;landmarks&#x27;</span>]</span><br><span class="line">        <span class="comment"># swap color axis because</span></span><br><span class="line">        <span class="comment"># numpy image: H x W x C</span></span><br><span class="line">        <span class="comment"># torch image: C X H X W</span></span><br><span class="line">        image = image.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;image&#x27;</span>: torch.from_numpy(image),</span><br><span class="line">                <span class="string">&#x27;landmarks&#x27;</span>: torch.from_numpy(landmarks)&#125;</span><br></pre></td></tr></table></figure>
<p>现在可以尝试改变图片的通道顺序，并转化成 tensor</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor_transform = ToTensor()</span><br><span class="line">tensor_sample = tensor_transform(sample)</span><br><span class="line"><span class="built_in">print</span>(tensor_sample[<span class="string">&#x27;image&#x27;</span>].shape)           <span class="comment"># (3,  160, 160)</span></span><br></pre></td></tr></table></figure>

<h2 id="Compose-transforms"><a href="#Compose-transforms" class="headerlink" title="Compose transforms"></a>Compose transforms</h2><p>最后我们可以通过 <code>transforms.Compose</code> 函数将这些操作串联起来, 并将它传递 <code>FaceLandmarksDataset</code> 类的 <code>transform</code> 参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">composed_transform = transforms.Compose([Rescale((<span class="number">256</span>, <span class="number">256</span>)),</span><br><span class="line">                               RandomCrop((<span class="number">224</span>, <span class="number">224</span>)),</span><br><span class="line">                               ToTensor()])</span><br><span class="line">face_dataset = FaceLandmarksDataset(csv_file=<span class="string">&#x27;faces/face_landmarks.csv&#x27;</span>,</span><br><span class="line">                                    root_dir=<span class="string">&#x27;faces/&#x27;</span>,</span><br><span class="line">                                    transform=composed_transform)</span><br></pre></td></tr></table></figure>

<h2 id="Iterating-through-the-dataset"><a href="#Iterating-through-the-dataset" class="headerlink" title="Iterating through the dataset"></a>Iterating through the dataset</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataloader = DataLoader(face_dataset, batch_size=<span class="number">32</span>, </span><br><span class="line">                                         shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> batch_samples <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&gt; &quot;</span>, batch_samples[<span class="string">&quot;image&quot;</span>].shape, batch_samples[<span class="string">&#x27;landmarks&#x27;</span>].shape)</span><br></pre></td></tr></table></figure>
<p>打印出来的结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">=&gt;  torch.Size([32, 3, 224, 224]) torch.Size([32, 68, 2])</span><br><span class="line">=&gt;  torch.Size([32, 3, 224, 224]) torch.Size([32, 68, 2])</span><br><span class="line">=&gt;  torch.Size([5, 3, 224, 224]) torch.Size([5, 68, 2])</span><br></pre></td></tr></table></figure>

<p>一共有 69 张人脸图片，分成了 3 个 <code>batch</code> （32 + 32 +5）进行吞吐。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>基于卷积神经网络的 2D-to-3D 视频转换</title>
    <url>/2019/12/10/Deep3D/</url>
    <content><![CDATA[<p>目前制作 3D 电影的方法有两种：一种是直接用昂贵的立体相机设备进行拍摄，这种制作成本非常庞大。另一种则是通过图像处理技术将 2D 电影转化成 3D 格式，这种转换处理通常依赖于“深度艺术家”，他们手工地为每一帧创造深度图，然后利用标准的基于深度图像的渲染算法将与原始图像相结合，得到一个立体的图像对，这需要大量的人力成本。现在来说，每年只有 20 左右部新的 3D 电影发行。</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Deep3D/Deep3D_01.png" alt="image"></p>
<span id="more"></span>

<p>在这样强烈需求的工业背景下，这篇文章的目的虽然是为了解决如何利用神经网络将 2D 电影转化成具有立体感的 3D 视频的问题，并且不需要人力来标注图片的深度信息。但是它提出的方法太新颖(很多论文都引用了，可见影响力)，所以也把它拎出来讲。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>作者的网络如上图所示：双目图片的左图作为模型的输入，每经过一次卷积和池化层后都会有两个分支：分支 1 会进行下一个卷积池化层进行特征提取，而分支 2 会进入反卷积层进行上采样得到一个与原图分辨率一致的视差图 (disparity map)，如此反复经过 5 层循环，得到 5 个视差图。最终作者会将这 5 个视差图相加，然后再经过一层卷积层并使用 softmax 激活函数，最后会输出一个与原图分辨率一致的视差概率分布 (probalistic disparity map)。</p>
<p>其实该网络的结构设计和其他语义分割类型的网络大同小异，这里没什么讲的。关键是它的损失函数设计以及 image-to-image 的方法，揭开了深度估计无监督学习的序幕。所以让我们直奔主题，给你一张左图和视差图，你如何去重构出右图？</p>
<h2 id="重构右图"><a href="#重构右图" class="headerlink" title="重构右图"></a>重构右图</h2><p>我们的直觉做法是将左视角点向右平移视差 D 个单位，然后便得到了右视角点。由于受极线约束，因此计算复杂度为 o(n)。但是这个方法在神经网络里无法进行反向传播，因为它对视差 D 是不可导的，因此我们无法训练。针对这个问题，作者引入了视差的概率分布对网络进行优化。利用左视角点和视差概率分布对右视角点进行重构的过程如下公式所示：</p>
<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Deep3D/MommyTalk1600758530618.jpg">
</p>

<ul>
<li><code>O_&#123;i, j&#125;</code> 表示在图片坐标 (i, j) 上重构的右视角点</li>
<li><code>I_&#123;i, j&#125;^&#123;d&#125;</code> 表示左视角点 (i, j) 平移 d 个位置后得到的右视角</li>
<li><code>D_&#123;i, j&#125;^&#123;d&#125;</code> 表示左视角点 (i, j) 的视差概率分布在视差 d 时的概率值</li>
</ul>
<h2 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h2><p>上述公式有点晦涩难懂，我琢磨了半天，写了个小程序进行实践：假如我们现在有一对分辨率为 200x200 的双目图片，整张图片上的像素视差都是 20。为了感受视差的偏移性质，我们在图片的中间区域设置了一块 10x10 的白点。可以看到从左图到右图，小白点很明显地移动了一小段距离，这就是视差造成的。</p>
<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Deep3D/lr_img.png">
</p>

<p>我们假定整张图片的最大视差值为 30， 那么就需要划分 0,1,…,30 共 31 个等级。因此图片的视差概率分布的形状为 [200， 200， 31]，由于真实的视差值为 20， 因此该等级属于 onehot 状态，接着左图上每个像素点在每个等级 i 上都会向右平移 i 个单位，这样一来我们便总共得到了 31 张图片， 程序里用 shift_images 表示，最后再将它与视差的概率分布相乘并求和便得到重构的右图。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">l_img = np.zeros([<span class="number">200</span>, <span class="number">200</span>])</span><br><span class="line">l_img[<span class="number">95</span>:<span class="number">105</span>, <span class="number">95</span>:<span class="number">105</span>] = <span class="number">255</span></span><br><span class="line"></span><br><span class="line">r_img = np.zeros([<span class="number">200</span>, <span class="number">200</span>])</span><br><span class="line">r_img[<span class="number">95</span>:<span class="number">105</span>, <span class="number">115</span>:<span class="number">125</span>] = <span class="number">255</span></span><br><span class="line"></span><br><span class="line">cv2.imwrite(<span class="string">&quot;l_img.jpg&quot;</span>, l_img)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;r_img.jpg&quot;</span>, r_img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设整张图片的视差都是20</span></span><br><span class="line">gt_disp = np.ones([<span class="number">200</span>, <span class="number">200</span>]) * <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># construct disparity map</span></span><br><span class="line">max_disp = <span class="number">30</span></span><br><span class="line">prob_disp = np.zeros([<span class="number">200</span>, <span class="number">200</span>, max_disp])</span><br><span class="line">prob_disp[:, :, <span class="number">20</span>] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">shift_images = np.zeros(shape=[<span class="number">200</span>, <span class="number">200</span>, max_disp])</span><br><span class="line"><span class="keyword">for</span> disp <span class="keyword">in</span> <span class="built_in">range</span>(max_disp):</span><br><span class="line">    shift_images[:, :, disp] = np.roll(l_img, disp, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">pred_r_img = np.<span class="built_in">sum</span>(shift_images * prob_disp, axis=<span class="number">2</span>)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;pred_r_img.jpg&quot;</span>, pred_r_img)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;reconstruction loss: &quot;</span>, np.<span class="built_in">sum</span>(pred_r_img - r_img)) <span class="comment"># 0.0</span></span><br></pre></td></tr></table></figure>

<p>由于我们给的是真实的视差概率分布，因此重构损失(reconstruction loss)的值为0. 反过来：如果重构损失不为 0， 那么神经网络将会朝着预测正确的视差概率分布去优化。最后我们将预测出来的右图和真实的右图进行了对比，结果一致。</p>
<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Deep3D/pred_rimg.png">
</p>

<p>总结： 这篇文章的新颖之处在于，通过 image-to-image 训练的方式，打开了深度估计网络通往无监督训练的大门。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Junyuan Xie, Ross Girshick, Ali Farhadi. <a href="https://arxiv.org/abs/1604.03650">Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep Convolutional Neural Networks</a>, CVPR 2016</li>
<li>[2] Deep3D: <a href="https://github.com/piiswrong/deep3d">Automatic 2D-to-3D Video Conversion with CNNs</a></li>
</ul>
]]></content>
      <categories>
        <category>立体视觉</category>
      </categories>
      <tags>
        <tag>深度估计</tag>
      </tags>
  </entry>
  <entry>
    <title>FAST 角点检测</title>
    <url>/2018/07/17/FAST-%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p>FAST 是一种角点，主要检测局部像素灰度变化明显的地方，以速度快著称。<font color=Red>它的思想是: 如果一个像素与它邻域的像素差别较大(过亮或过暗), 那它更可能是角点。相比于其他角点检测算法，FAST 只需比较像素亮度的大小， 十分快捷。</font>它的检测过程如下:</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FAST-角点检测/01.png">
</p>

<span id="more"></span>

<ol>
<li>在图像中选取像素 p，假设它的亮度为 C，并设置一个阈值 T。</li>
<li>以像素 p 为中心, 选取半径为 3 的圆上的<font color=Red> 16 个像素点</font>（如上图所示)。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">circle</span>(<span class="params">row,col</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    对于图片上一像素点位置 (row,col)，获取其邻域圆上 16 个像素点坐标，圆由 16 个像素点组成</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        row：行坐标 注意 row 要大于等于3</span></span><br><span class="line"><span class="string">        col：列坐标 注意 col 要大于等于3       </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    point1  = (row-<span class="number">3</span>, col)</span><br><span class="line">    point2  = (row-<span class="number">3</span>, col+<span class="number">1</span>)</span><br><span class="line">    point3  = (row-<span class="number">2</span>, col+<span class="number">2</span>)</span><br><span class="line">    point4  = (row-<span class="number">1</span>, col+<span class="number">3</span>)</span><br><span class="line">    point5  = (row, col+<span class="number">3</span>)</span><br><span class="line">    point6  = (row+<span class="number">1</span>, col+<span class="number">3</span>)</span><br><span class="line">    point7  = (row+<span class="number">2</span>, col+<span class="number">2</span>)</span><br><span class="line">    point8  = (row+<span class="number">3</span>, col+<span class="number">1</span>)</span><br><span class="line">    point9  = (row+<span class="number">3</span>, col)</span><br><span class="line">    point10 = (row+<span class="number">3</span>, col-<span class="number">1</span>)</span><br><span class="line">    point11 = (row+<span class="number">2</span>, col-<span class="number">2</span>)</span><br><span class="line">    point12 = (row+<span class="number">1</span>, col-<span class="number">3</span>)</span><br><span class="line">    point13 = (row, col-<span class="number">3</span>)</span><br><span class="line">    point14 = (row-<span class="number">1</span>, col-<span class="number">3</span>)</span><br><span class="line">    point15 = (row-<span class="number">2</span>, col-<span class="number">2</span>)</span><br><span class="line">    point16 = (row-<span class="number">3</span>, col-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> [point1, point2,point3,point4,point5,point6,point7,point8,point9,point10,point11,point12, point13,point14,point15,point16]</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><font color=Red>假如这 16 个点中，有连续的 N 个点的亮度大于 C + T 或小于 C − T，那么像素 p 可以被认为是角点。</font></li>
<li>为了排除大量的非角点提出了一种高速测试方法：<font color=Red>直接检测邻域圆上的第  1，5，9，13 个像素的亮度。只有当这四个像素中有三个同时大于 C + T 或小于  C − T 时，当前像素才有可能是一个角点，否则应该直接排除。</font>这样的预测试操作大大加速了角点检测。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_corner</span>(<span class="params">image,row,col,threshold</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    检测图像位置(row,col)处像素点是不是角点</span></span><br><span class="line"><span class="string">    如果圆上有12个连续的点满足阈值条件，那么它就是一个角点</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    方法：</span></span><br><span class="line"><span class="string">        如果位置1和9它的像素值比阈值暗或比阈值亮，则检测位置5和位置15</span></span><br><span class="line"><span class="string">        如果这些像素符合标准，请检查像素5和13是否相符</span></span><br><span class="line"><span class="string">        如果满足有3个位置满足阈值条件，则它是一个角点</span></span><br><span class="line"><span class="string">        重复循环函数返回的每个点如果没有满足阈值，则不是一个角落</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        注意：这里我们简化了论文章中的角点检测过程，会造成一些误差</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    args:</span></span><br><span class="line"><span class="string">        image：输入图片数据,要求为灰度图片</span></span><br><span class="line"><span class="string">        row：行坐标 注意row要大于等于3</span></span><br><span class="line"><span class="string">        col：列坐标 注意col要大于等于3 </span></span><br><span class="line"><span class="string">        threshold：阈值        </span></span><br><span class="line"><span class="string">    return : </span></span><br><span class="line"><span class="string">        返回True或者False</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># 校验</span></span><br><span class="line">    rows,cols = image.shape[:<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">if</span> row &lt; <span class="number">3</span> <span class="keyword">or</span> col &lt; <span class="number">3</span> : <span class="keyword">return</span> <span class="literal">False</span>    </span><br><span class="line">    <span class="keyword">if</span> row &gt;= rows-<span class="number">3</span> <span class="keyword">or</span> col &gt;= cols-<span class="number">3</span>: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">       </span><br><span class="line">    intensity = <span class="built_in">int</span>(image[row][col])</span><br><span class="line">    ROI = circle(row,col)</span><br><span class="line">    <span class="comment"># 获取位置1,9,5,13的像素值</span></span><br><span class="line">    row1, col1   = ROI[<span class="number">0</span>]</span><br><span class="line">    row9, col9   = ROI[<span class="number">8</span>]</span><br><span class="line">    row5, col5   = ROI[<span class="number">4</span>]</span><br><span class="line">    row13, col13 = ROI[<span class="number">12</span>]</span><br><span class="line">    intensity1  = <span class="built_in">int</span>(image[row1][col1])</span><br><span class="line">    intensity9  = <span class="built_in">int</span>(image[row9][col9])</span><br><span class="line">    intensity5  = <span class="built_in">int</span>(image[row5][col5])</span><br><span class="line">    intensity13 = <span class="built_in">int</span>(image[row13][col13])</span><br><span class="line">    <span class="comment"># 统计上面4个位置中满足  像素值  &gt;  intensity + threshold点的个数</span></span><br><span class="line">    countMore = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 统计上面4个位置中满足 像素值  &lt; intensity - threshold点的个数</span></span><br><span class="line">    countLess = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> intensity1 - intensity &gt; threshold:</span><br><span class="line">        countMore += <span class="number">1</span> </span><br><span class="line">    <span class="keyword">elif</span> intensity1 + threshold &lt; intensity:</span><br><span class="line">        countLess += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> intensity9 - intensity &gt; threshold:</span><br><span class="line">        countMore += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> intensity9 + threshold &lt; intensity:</span><br><span class="line">        countLess += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> intensity5 - intensity &gt; threshold:</span><br><span class="line">        countMore += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> intensity5 + threshold &lt; intensity:</span><br><span class="line">        countLess += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> intensity13 - intensity &gt; threshold:</span><br><span class="line">        countMore += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> intensity13 + threshold &lt; intensity:</span><br><span class="line">        countLess += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> countMore &gt;= <span class="number">3</span> <span class="keyword">or</span> countLess&gt;=<span class="number">3</span></span><br></pre></td></tr></table></figure>

<ol start="5">
<li>在第一遍检测后，原始的 FAST 角点经常出现“扎堆”的现象。因此需要使用非极大值抑制，在一定区域内仅保留响应极大值的角点，避免角点集中的问题。</li>
</ol>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FAST-角点检测/02.png">
</p>

<blockquote>
<p>第一张图片使用了非最大值抑制，而第二张没有使用。可以明显看到，第二张图的关键点的位置重复比较严重。</p>
</blockquote>
<p>为了方便，我们可以在 OpenCV 里直接创建 FAST 特征点检测器并使用它:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">&#x27;simple.jpg&#x27;</span>,<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Initiate FAST object with default values</span></span><br><span class="line">fast = cv.FastFeatureDetector_create()</span><br><span class="line"></span><br><span class="line"><span class="comment"># enable nonmaxSuppression</span></span><br><span class="line">fast.setNonmaxSuppression(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># find and draw the keypoints</span></span><br><span class="line">kp = fast.detect(img, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">img = cv.drawKeypoints(img, kp, <span class="literal">None</span>, color=(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<p>参考文献:</p>
<ul>
<li><a href="https://www.cnblogs.com/zyly/p/9542164.html#_label3">[1] 第十四节、FAST角点检测(附源码)</a></li>
<li><a href="https://www.bookstack.cn/read/opencv-doc-zh-4.0/docs-4.0.0-5.6-tutorial_py_fast.md">[2] OpenCV 中文文档 4.0.0 - 角点检测的FAST算法</a></li>
</ul>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>角点检测</tag>
      </tags>
  </entry>
  <entry>
    <title>全卷积神经网络（FCN)</title>
    <url>/2019/07/12/FCN/</url>
    <content><![CDATA[<p>在我还是实习生的时候，我们组的 leader 讲了 FCN 网络。由于当时对图像分割还不是很了解，所以也没太听懂，只记得他当时讲这篇文章拿了 CVPR-2015 的最佳论文奖。现在学习 FCN 就觉得，这应该是图像分割领域里最经典也是最适合入门的网络了吧。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/67369222-33df5d80-f5ab-11e9-95d4-3d7813cfa0a8.png">
</p>

<span id="more"></span>

<h2 id="分割思想"><a href="#分割思想" class="headerlink" title="分割思想"></a>分割思想</h2><p>在我的代码里，使用了 VGG16 作为 backbone 来提取图片特征（其实作者也使用了 VGG19 作为backbone，但是发现效果和 VGG16 差不多)。如果把 FCN 看成是一个黑箱子，那么我们只要关心网络的输入和输出就行了。如果我们使用 VOC 数据集进行训练，输入图片的维度为 [H,W,C]，那么 FCN 输出的 feature map 形状则为 [H, W, 21]。其中，数字 21 代表的 VOC 的 20 个类别还有 1 个背景。</p>
<p>FCN 解决的实际问题就是针对图片里的每个像素进行分类，从而完成精确分割。按照以往 CNN 解决分类问题的思路，一般都会在 feature map 后面接一个全连接层，这个全连接层应该有 21 个神经元，每个神经元输出各个类别的概率。但是由于全连接的特征是一个二维的矩阵，因此我们在全连接层之前会使用 Flatten 层将三维的 feature map 展平。这就带来了2个问题：</p>
<ul>
<li>使用了 Flatten 层抹平了图片的空间信息；</li>
<li>一旦网络训练好，图片的输入尺寸将无法改变。</li>
</ul>
<p>FCN 网络很好地解决了这两个问题，它可以接受任意尺寸的输入图像，并保留了原始输入图像中的空间信息，最后直接在 feature map 上对像素进行分类。</p>
<h2 id="跳跃连接"><a href="#跳跃连接" class="headerlink" title="跳跃连接"></a>跳跃连接</h2><p>在刚开始的时候，作者将输入图片经过卷积和下采样操作一头走到尾，最后宽和高都被缩放了 32 倍。为了将 feature map 上采样到原来的尺寸，因此作者将 vgg16 的输出扩大了 32 倍，并将该模型称为 FCN-32s。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/67386859-53d14a00-f5c8-11e9-9d62-ccb1c2e61a80.jpg">
</p>

<p>但是发现FCN-32s的分割效果并不够好，如下图所示。尽管最后的 feature map 输出经过了 32 倍的上采样操作，但是图片里的边缘细节信息还是被 VGG16 网络里的卷积和下采样操作所模糊掉了。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/67385904-9003ab00-f5c6-11e9-87da-3dbf0dcb079a.png">
</p>

<p>作者把它称作是一个<strong>what</strong>和<strong>where</strong>的问题，请看下面作者的原话：</p>
<blockquote>
<p>Semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where.</p>
</blockquote>
<p>说白了就是<strong>全局信息能够预测这个物体是哪个类别，而局部的细粒度信息能够实现对物体的定位与检测</strong>。为了解决这个问题，作者通过缓慢地（分阶段地）对编码特征进行上采样，从浅层添加了“skip connections(跳跃连接)”，并将这两个特征映射相加，并最终将它上采样 8 或者 16 倍进行输出，分别称为 FCN-8s 和 FCN-16s 模型。</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/67389318-f4c20400-f5cc-11e9-9769-acb912aa8292.png" alt="image"></p>
<p>添加 skip connections 结构后，就能将深层的，粗糙的语义信息与浅层的，精细的表面信息融合起来，从而在一定程度上解决图像边缘分割效果较差的问题。</p>
<blockquote>
<p>We define a skip architecture to take advantage of this feature spectrum that combines deep, coarse, semantic information and shallow, fine, appearance information</p>
</blockquote>
<p><strong>这里需要抛出一个问题，为什么这个 “跳跃连接” 这么牛逼有效?</strong></p>
<p>这还得从感受野(Receptive Field)说起，卷积神经网络中感受野的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。再通俗点的解释是，特征图上的一个点对应输入原来图片上的区域。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/67464190-82592e80-f675-11e9-9ad0-2b4ac870eb52.png">
</p>

<p>前面讲到深层的特征图在空间尺寸上往往会越来越小，这就意味着它的感受野区域会越来越大，从而更富含图片的全局信息，能较好地解决 what 问题；浅层特征图的空间尺寸较大，这就意味着它的感受野会更小，因而容易捕捉到物体的边缘信息和丰富的细粒特征,能较好地解决 where 问题。感受野大的特征，可以很容易的识别出大物体的，但是在实际分割中，<strong>大物体边缘信息和小物体本身是很容易被深层网络一次次的降采样和一次次升采样给弄丢的，这个时候就可能需要感受野小的特征来帮助</strong>。</p>
<blockquote>
<p>在上图中，如果把 conv1 和 conv2 分别比作浅层特征和深层特征的话。那么深层特征里一个数字 “5” 的感受野尺寸就是 3x3，而浅层特征里 4 个 数字 “3” 的感受野也是这个区域，但是平均下来 1 个数字 “3” 的感受野尺寸则 1x1 都不到。</p>
</blockquote>
<p><strong>深层特征的感受野较大，浅层特征的感受野较小，它们分别解决 what 和 where 问题。反正如果将它们联合起来，那就牛逼了！</strong></p>
<h2 id="反卷积层"><a href="#反卷积层" class="headerlink" title="反卷积层"></a>反卷积层</h2><p>FCN的上采样层使用的是反卷积层，反卷积也称为转置卷积操作(Transposed convolution)。要了解反卷积是怎么回事，得先回顾一下正向卷积的实现过程。假设输入的图片 input 尺寸为 4x4，元素矩阵为:</p>
<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757491425.jpg">
</p>

<p>卷积核的尺寸为 3x3，其元素矩阵为：</p>
<p align="center">
    <img width="25%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757544885.jpg">
</p>

<p>正向卷积操作：步长 strides = 1, 填充 padding = 0,输出形状为 2x2，该过程如下图所示：</p>
<p align="center">
    <img width="20%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/no_padding_no_strides.gif">
</p>

<p>在上面这幅图中，底端为输入，上端为输出，卷积核为 3x3。如果我们用矩阵乘法去描述这个过程: 把 input 元素矩阵展开成一个列向量 X</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757584674.jpg">
</p>

<p>把输出图像 output 的元素矩阵展开成一个列向量 Y</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757621835.jpg">
</p>

<p>对于输入元素矩阵 X 和输出元素矩阵 Y ，用矩阵运算描述这个过程:</p>
<p align="center">
    <img width="9%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757652012.jpg">
</p>

<p>通过推导，我们可以获得稀疏矩阵 C</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757682778.jpg">
</p>

<p>稀疏矩阵 C 的形状为 4x16, X 形状为 16x1，Y 的形状为 4x1，将 Y 进行 reshape 后便是我们的期望输出形状 2x2。那么，反卷积的操作就是要对这个矩阵运算过程进行转置，通过输出 Y 得到输入 X：</p>
<p align="center">
    <img width="9%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757652012.jpg">
</p>

<p>从矩阵元素形状的角度出发，可以理解为：16x1=16x4x4x1，下面这个动画比较生动地描述了反卷积过程:</p>
<p align="center">
    <img width="10%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/FCN/MommyTalk1600757723514.jpg">
</p>

<p>值得注意的是，反卷积操作并不是卷积操作的可逆过程，也就是说图像经过卷积操作后是不能通过反卷积操作恢复原来的样子。这是因为反卷积只是转置运算，并非可逆运算。</p>
<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>在 PASCAL VOC 数据集中，每个类别对应一个色彩【RGB】, 因此我们需要对SegmentationClass文件夹里的每张 mask 图片根据像素的色彩来标定其类别，在代码 parser_voc.py是这样进行处理的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(H):</span><br><span class="line">   write_line = []</span><br><span class="line">   <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(W):</span><br><span class="line">   	pixel_color = label_image[i, j].tolist() <span class="comment"># 得到该像素点的 RGB 值</span></span><br><span class="line">        <span class="keyword">if</span> pixel_color <span class="keyword">in</span> colormap:</span><br><span class="line">       	    cls_idx = colormap.index(pixel_color) <span class="comment"># 索引该 RGB 值的类别</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cls_idx = <span class="number">0</span></span><br></pre></td></tr></table></figure>


<blockquote>
<p>考虑到在批量训练图片时的 batch_size &gt;= 1，因此必须将图片 resize 成相同的尺寸，这里采用的是最近邻插值法，从而保证新插值的像素分类问题。</p>
</blockquote>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>如果你要训练 FCN-8s 的话，还是推荐你加载 VGG16 模型的，否则会变得非常耗时。还有一点的就是，其实训练图片里的像素类别是非常不均衡的。例如 75% 的图片像素都属于背景（见上图），因此你会发现在训练时其精度很快就达到了80%，但此时的预测结果却是一片黑，即预测的类别都为背景。</p>
<p>一般对于语义分割的训练，学术界有两种办法： Patchwise training 和类别损失加权的方法来进行训练。</p>
<ul>
<li>Patchwise training: 补丁式训练方法，它旨在避免全图像训练的冗余。在语义分割中，由于要对图像中的每个像素进行分类，如果输入整个图像可能会有大量的冗余。因此在训练分割网络时，避免这种情况的一种标准方法是从训练集而不是完整图像中给网络提供成批的随机补丁（感兴趣对象周围的小图像区域）。从另一种角度出发，我们也可以使得这些补丁区域尽量减少背景信息，从而缓解类别不均衡问题。</li>
<li>类别损失加权: 根据类别数量的分布比例对各自的损失函数进行加权，比如有些样本的数量较少，我就给它的损失函数比重增大一些。</li>
</ul>
<p>对此，作者根据实验结果非常霸气地放话了：</p>
<blockquote>
<p> explore training with sampling in Section 4.3, and do not find that it yields faster or better convergence for dense prediction. Whole image training is effective and efficient.</p>
</blockquote>
<p>补丁式训练完全没有必要，训练 FCN 还是输入整张图片比较好。并且解决这种类别不均衡的问题，只需要给损失函数按比例加权重就行。最后作者还对此进行了学术上的解释，我这里就不讲了，话讲多了你们会觉得我在胡言乱语…</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Jonathan Long, Evan Shelhamer, Trevor Darrell. <a href="https://arxiv.org/abs/1411.4038">Fully Convolutional Networks for Semantic Segmentation</a>. CVPR 2015</li>
<li>[2] TensorFlow2.0-Example code: <a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/tree/master/5-Image_Segmentation/FCN">FCN</a></li>
</ul>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>全卷积网络</tag>
        <tag>Skip Connection</tag>
      </tags>
  </entry>
  <entry>
    <title>Faster-rcnn 里的区域生成网络（RPN）</title>
    <url>/2019/09/27/RPN/</url>
    <content><![CDATA[<p>我觉得 RPN 是目标检测领域里最经典也是最容易入门的网络了。如果你想学好目标检测，那一定不能不知道它！今天讲的 RPN 是来一篇来自 CVPR 2017 的论文 Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters， 作者在 Faster-rcnn 的 RPN 基础上进行了改进，用于行人检测。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p align="center">
    <img width="65%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/RPN.png">
</p>


<p>上面是 RPN 的网络结构，它采用了 VGG16 网络进行特征提取。从 VGG16 的整体架构来看，作者为了提高 RPN 在不同分辨率图片下的检测率，分别将 Pool3 层、Pool4 层和 Pool5 层的输出进行卷积和融合得到了一个 45 x 60 x 1280 尺寸的 feature map。最后将这个 feature map 分别输入两个卷积层中得到 softmax 分类层与 bboxes 回归层。</p>
<span id="more"></span>

<h2 id="Anchor-机制"><a href="#Anchor-机制" class="headerlink" title="Anchor 机制"></a>Anchor 机制</h2><p>目标检测其实是生产很多框，然后在消灭无效框的过程。生产很多框的过程利用的是 Anchor 机制，消灭无效框则采用非极大值抑制过程进行处理。RPN 网络输入的图片为 720 x 960，输出的 feature map 尺寸为 45 x 60，由于它们每个点上会产生 9 个 anchor boxes，因此最终一共会得到 45 x 60 x 9 个 anchor boxes。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/faster-rcnn.png">
</p>

<p>直接利用这些 anchor boxes 对真实框进行预测会有些困难，因此作者采用了 <strong><font color=red>anchor boxes 与 ground-truth boxes 的偏移量机制</font></strong>进行回归预测。</p>
<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/MommyTalk1600756419208.jpg">
</p>

<p><code>x, y, w, h</code> 分别表示 boxes 的中心坐标和宽高；变量 <code>x, x_&#123;a&#125;, x^&#123;*&#125;</code> 则分别代表预测框，anchor 框和 ground-truth 框的中心坐标 <code>x</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_regression</span>(<span class="params">box1, box2</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    box1: predicted boxes</span></span><br><span class="line"><span class="string">    box2: anchor boxes</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    target_reg = np.zeros(shape=[<span class="number">4</span>,])</span><br><span class="line">    w1 = box1[<span class="number">2</span>] - box1[<span class="number">0</span>]</span><br><span class="line">    h1 = box1[<span class="number">3</span>] - box1[<span class="number">1</span>]</span><br><span class="line">    w2 = box2[<span class="number">2</span>] - box2[<span class="number">0</span>]</span><br><span class="line">    h2 = box2[<span class="number">3</span>] - box2[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    target_reg[<span class="number">0</span>] = (box1[<span class="number">0</span>] - box2[<span class="number">0</span>]) / w2</span><br><span class="line">    target_reg[<span class="number">1</span>] = (box1[<span class="number">1</span>] - box2[<span class="number">1</span>]) / h2</span><br><span class="line">    target_reg[<span class="number">2</span>] = np.log(w1 / w2)</span><br><span class="line">    target_reg[<span class="number">3</span>] = np.log(h1 / h2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> target_reg</span><br></pre></td></tr></table></figure>

<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>RPN 的损失函数和 YOLO 非常像，不过从发表论文时间顺序来看，应该是 YOLO 借鉴了 RPN 。在 Faster-rcnn 论文里，RPN 的损失函数是这样的:</p>
<ul>
<li>为了训练 RPN， 我们首先给每个 anchor boxes 设置了两个标签，分别为 0: 背景, 1: 前景；</li>
<li>与 ground-truth boxes 重合度 (iou) 最高的那个 anchor boxes 设置为正样本;</li>
<li>只要这个 anchor boxes 与任何一个 ground-truth boxes 的 iou 大于 0.7，那么它也是一个正样本；</li>
<li>如果 anchor boxes 与所有的 ground-truth boxes 的 iou 都小于 0.3， 那么它就是一个负样本，表示不包含物体；</li>
<li>在前面这几种情况下，已经能够产生足够多的正、负样本了，剩下的则既不是正样本，也不是负样本，它们不会参与到 RPN 的 loss 的计算中去。</li>
</ul>
<p>在我的代码 <a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/blob/master/4-Object_Detection/RPN/demo.py">demo.py</a> 里将正负样本都可视化出来了，大家只要配置好 image 和 label 的路径然后直接执行 python demo.py 就可以看到以下图片。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/01.png">
</p>

<p>在上图中，蓝色框为 anchor boxes，它们就是正样本，红点为这些正样本 anchor boxes 的中心位置，黑点表示的是负样本 anchor boxes 的中心位置。从图中可以看出：在有人的区域，正样本框的分布比较密集，并且红点都在人体中心区域；而在没有人的区域则布满了黑点,它们表示的是负样本，都属于背景。</p>
<p>在前面讲到，RPN 网络预测的是 <strong><font color=red>anchor boxes 与 ground-truth boxes 的偏移量</font></strong>，那如果我们将这些正样本 anchor boxes 的偏移量映射回去的话：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">=&gt; Decoding positive sample: 20, 20, 0</span><br><span class="line">=&gt; Decoding positive sample: 20, 20, 7</span><br><span class="line">...</span><br><span class="line">=&gt; Decoding positive sample: 36, 31, 1</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/02.png">
</p>

<p>你会发现，这就是 ground-truth boxes 框（绿色框）和物体中心点（红色点）的位置。事实上，RPN 的损失是一个多任务的 loss function，集合了分类损失与回归框损失，它们两者之间的优化可以通过 λ 系数去实现平衡。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/MommyTalk1600756499331.jpg">
</p>

<p>初次看这个损失函数有点迷，<strong><font color=red>它其实是一个 smooth-L1 损失函数, 它的优点在于解决了 L1 损失函数在 0 点附近的不可导问题，而且相比于 L2 损失函数而言，它在训练初始阶段的梯度回传会更加稳定</font></strong>。如下图所示，正负样本都会参与到分类损失的反向传播中去（因为你需要告诉网络什么是正样本和负样本），而回归框的损失只有正样本参与计算（只有正样本才有回归框损失，负样本作为背景是没有回归框损失的)。</p>
<p align="center">
    <img width="47%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/MommyTalk1600756544371.jpg">
</p>

<p>其中：</p>
<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/MommyTalk1600756586214.jpg">
</p>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">target_scores, target_bboxes, target_masks, pred_scores, pred_bboxes</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    target_scores shape: [1, 45, 60, 9, 2],  pred_scores shape: [1, 45, 60, 9, 2]</span></span><br><span class="line"><span class="string">    target_bboxes shape: [1, 45, 60, 9, 4],  pred_bboxes shape: [1, 45, 60, 9, 4]</span></span><br><span class="line"><span class="string">    target_masks  shape: [1, 45, 60, 9]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    score_loss = tf.nn.softmax_cross_entropy_with_logits(labels=target_scores, logits=pred_scores)</span><br><span class="line">    foreground_background_mask = (np.<span class="built_in">abs</span>(target_masks) == <span class="number">1</span>).astype(np.<span class="built_in">int</span>)</span><br><span class="line">    score_loss = tf.reduce_sum(score_loss * foreground_background_mask, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) / np.<span class="built_in">sum</span>(foreground_background_mask)</span><br><span class="line">    score_loss = tf.reduce_mean(score_loss)</span><br><span class="line"></span><br><span class="line">    boxes_loss = tf.<span class="built_in">abs</span>(target_bboxes - pred_bboxes)</span><br><span class="line">    boxes_loss = <span class="number">0.5</span> * tf.<span class="built_in">pow</span>(boxes_loss, <span class="number">2</span>) * tf.cast(boxes_loss&lt;<span class="number">1</span>, tf.float32) + (boxes_loss - <span class="number">0.5</span>) * tf.cast(boxes_loss &gt;=<span class="number">1</span>, tf.float32)</span><br><span class="line">    boxes_loss = tf.reduce_sum(boxes_loss, axis=-<span class="number">1</span>)</span><br><span class="line">    foreground_mask = (target_masks &gt; <span class="number">0</span>).astype(np.float32)</span><br><span class="line">    boxes_loss = tf.reduce_sum(boxes_loss * foreground_mask, axis=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) / np.<span class="built_in">sum</span>(foreground_mask)</span><br><span class="line">    boxes_loss = tf.reduce_mean(boxes_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> score_loss, boxes_loss</span><br></pre></td></tr></table></figure>

<h2 id="k-means-造框"><a href="#k-means-造框" class="headerlink" title="k-means 造框"></a>k-means 造框</h2><p>如果 Anchor boxes 的尺寸选得好，那么就使得网络更容易去学习。刚开始我以为反正网络预测的都是 Bounding Boxes 的偏移量，那么 Anchor boxes 尺寸就没那么重要了。但我在复现算法和写代码的过程中发现，看来我还是太年轻了。我使用的是 synthetic_dataset 数据集进行训练，该数据集里所有检测的目标都为 “person”，假如我直接用作者论文里的原始 anchor，那么得到的正样本为如下左图；而如果我使用 k-means算法对该数据集所有的 ground-truth boxes 进行聚类得到的 anchor，那么效果就如下右图所示，显然后者的效果比前者好得多。</p>
<table>
<thead>
<tr>
<th align="center">论文原始 anchor</th>
<th align="center">k-means 的 anchor</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/03.png" alt="论文原始 anchor"></td>
<td align="center"><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/04.png" alt="k-means 的 anchor"></td>
</tr>
</tbody></table>
<p>不仅如此，事实上一些其他超参数也会影响正负样本的分布情况，从而直接影响到网络的学习过程。所有这些事实都告诉我们，学习神经网络不能靠从网上看一些浅显的教程就够了的，关键还得自己去多多看源码并实践，才能成为一名合格的深度学习炼丹师。</p>
<table>
<thead>
<tr>
<th align="center">pos_thresh=0.2, neg_thresh=0.1</th>
<th align="center">pos_thresh=0.7, neg_thresh=0.2</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/05.png" alt="pos_thresh=0.2, neg_thresh=0.1"></td>
<td align="center"><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/RPN/06.png" alt="pos_thresh=0.7, neg_thresh=0.2"></td>
</tr>
</tbody></table>
<p>最后在测试集上的效果，还是非常赞的! 训练的 score loss基本降到了零，boxes loss 也是非常非常低。但是由于是 RPN 网络，所以我们又不能对它抱太大期望，不然 Faster-RCNN 后面更精确的回归层和分类层意义就不大了。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. <a href="https://arxiv.org/abs/1506.01497">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>, CVPR 2016</li>
<li>[2] Shiyu Huang. <a href="https://arxiv.org/pdf/1703.06283.pdf">Expecting the Unexpected:Training Detectors for Unusual Pedestrians with Adversarial Imposters</a>, CVPR 2017</li>
</ul>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>Faster-rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title>能不能用梯度下降法求解平方根 ？</title>
    <url>/2020/01/21/SGD/</url>
    <content><![CDATA[<p>2020 年春节将至，大部分同事已经回家。回顾下自己的 2019，似乎收获颇丰：不仅顺利毕业，还找了份谋生的工作。这期间看了很多复杂的算法，有监督 or 无监督，目标检测 or 深度估计。而人一旦徜徉在其中，就会渐渐忘记一些基础的东西。是时候回顾一下梯度下降法了….</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGD/timg.jpg">
</p>

<p>问题：请尝试使用梯度下降法求解 <code>sqrt&#123;2020&#125;</code> 的值，并精确到小数点后 4 位。</p>
<span id="more"></span>

<p>思路：该问题等价于求函数 <code>f(x) = x^&#123;2&#125; - 2020</code> 的根，也就等价于求 <code>g(x) = (x^&#123;2&#125; - 2020)^2</code> 的最小值。所以，我们可以建立损失函数：</p>
<p align="center">
    <img width="20%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGD/MommyTalk1600755062209.jpg">
</p>

<table><tr><td bgcolor= LightSalmon><strong>梯度下降法:

<p align="center">
    <img width="20%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGD/MommyTalk1600755121091.jpg">
</p>

<p></strong></td></tr></table></p>
<p>其中 <code>a</code> 为学习率，整个过程的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">lr = <span class="number">1e-5</span></span><br><span class="line">x  = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">grad = <span class="keyword">lambda</span> x: <span class="number">4</span> * x * (np.power(x, <span class="number">2</span>) - <span class="number">2020</span>)</span><br><span class="line">loss = <span class="keyword">lambda</span> x: (np.power(x, <span class="number">2</span>) - <span class="number">2020</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    x = x - lr * grad(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&gt; epoch %2d, x=%.4f, loss = %.4f&quot;</span> %(epoch, x, loss(x)))</span><br></pre></td></tr></table></figure>

<p><strong><font color=Red>在整个过程中，我们只需要不断利用梯度下降法更新参数就可以了</font></strong>。最后训练损失曲线逐渐下降至 0， 此时得到的 $x$ 已经收敛至 44.9444，满足要求。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGD/loss.png">
</p>

<p>但事情并不总是一帆风顺，我也尝试了一些失败的案例：</p>
<ul>
<li><p>学习率过大的情况，当 <code>x=10</code> 而且 <code>lr=1e-3</code> 时</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">=&gt; epoch  0, x=86.8000, loss = 30406842.7776</span><br><span class="line">=&gt; epoch  1, x=-1827.7441, loss = 11146440911634.0312</span><br><span class="line">...</span><br><span class="line">=&gt; epoch 99, x=nan, loss = nan</span><br></pre></td></tr></table></figure></li>
<li><p>x 的初始值过大的情况，当 <code>x=2020</code> 而且 <code>lr=1e-5</code> 时</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">=&gt; epoch  0, x=-327513.1040, loss = 11505744027749453922304.0000</span><br><span class="line">=&gt; epoch  1, x=1405225186080.3201, loss = 3899273520282849001736422898828492926803876249600.0000</span><br><span class="line">...</span><br><span class="line">=&gt; epoch 99, x=nan, loss = nan</span><br></pre></td></tr></table></figure></li>
</ul>
<p>希望能给大家调参带来一些启示。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title>手写双目立体匹配 SGM 算法（上)</title>
    <url>/2020/01/17/SGM_01/</url>
    <content><![CDATA[<p><strong>SGM（Semi-Global Matching）</strong>是一个基于双目视觉的半全局立体匹配算法，专门用于计算图像的视差。在 SGM 算法中，匹配代价计算是双目立体匹配的第一步，本文将使用 <strong>Census Transform</strong> 方法对此进行介绍。</p>
<h2 id="读取图片"><a href="#读取图片" class="headerlink" title="读取图片"></a>读取图片</h2><p>首先使用 OpenCV 将左图和右图读取进来，并需要将它们转成单通道的灰度图输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">left_image  = cv2.imread(<span class="string">&quot;./left.png&quot;</span>,  <span class="number">0</span>)</span><br><span class="line">right_image = cv2.imread(<span class="string">&quot;./right.png&quot;</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/left_gray.jpg">
</p>

<span id="more"></span>

<blockquote>
<p>left.png 和 right.png 可以从<font color=Red><a href="https://github.com/YunYang1994/YunYang1994.github.io/tree/master/images/SGM">这里</a></font>进行下载。</p>
</blockquote>
<h2 id="高斯平滑"><a href="#高斯平滑" class="headerlink" title="高斯平滑"></a>高斯平滑</h2><p>为了减小双目图片的噪声和细节的层次感，有必要使用高斯平滑算法对它们进行预处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">left_image  = cv2.GaussianBlur(left_image,  (<span class="number">3</span>,<span class="number">3</span>), <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">right_image = cv2.GaussianBlur(right_image, (<span class="number">3</span>,<span class="number">3</span>), <span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Census-变换"><a href="#Census-变换" class="headerlink" title="Census 变换"></a>Census 变换</h2><p>最早的匹配测度算法使用的是互信息法：对于两幅配准好的影像来说，它们的联合熵是很小的，因为其中一张影像可以通过另外一张影像预测，这表示两者之间的相关性最大，从而互信息也最大。但是它的数学原理非常复杂，且计算需要迭代，计算效率不高。在实际应用中，有一种更简单高效的方法叫 Census 变换更容易收到青睐（OpenCV 里用的就是这种方法）。</p>
<p>Census 变换的基本原理：在图像区域定义一个矩形窗口，用这个矩形窗口遍历整幅图像。选取中心像素作为参考像素，将矩形窗口中每个像素的灰度值与参考像素的灰度值进行比较，灰度值小于参考值的像素标记为 0，大于或等于参考值的像素标记为 1，最后再将它们按位连接，得到变换后的结果，变换后的结果是由 0 和 1 组成的二进制码流。<strong>Census 变换的实质是将邻域像素灰度值相对于中心像素灰度值的差异编码成二进制码流。</strong></p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/Census.png">
</p>

<p>我们不妨首先定义矩形窗口的大小为 7x7，由于不会对图像边界进行填充，因此计算图像的补偿尺寸：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image_h, image_w = left_image.shape[:<span class="number">2</span>]      <span class="comment"># 获取图片尺寸</span></span><br><span class="line">census_ksize = <span class="number">7</span></span><br><span class="line">x_offset = y_offset = census_ksize // <span class="number">2</span>      <span class="comment"># 补偿尺寸 = 3</span></span><br></pre></td></tr></table></figure>
<p>计算好这些必要的参数后，根据 Census 的变换原理可以获得它们的二进制编码，并将二进制编码存储为十进制数字。整个过程如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CensusTransform</span>(<span class="params">image</span>):</span></span><br><span class="line">    <span class="keyword">global</span> image_h, image_w, census_ksize, x_offset, y_offset</span><br><span class="line">    census  = np.zeros(shape=(image_h, image_w), dtype=np.uint64)</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(y_offset, image_h-y_offset):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(x_offset, image_w-x_offset):</span><br><span class="line">            center_pixel = image[y, x]</span><br><span class="line">            reference = np.full(shape=(census_ksize, census_ksize), fill_value=center_pixel, dtype=np.int64)</span><br><span class="line">            <span class="comment"># 定义二进制编码流</span></span><br><span class="line">            binary_pattern = []</span><br><span class="line">            <span class="comment"># 定义矩形窗口</span></span><br><span class="line">            window_image = image[(y - y_offset):(y + y_offset + <span class="number">1</span>), (x - x_offset):(x + x_offset + <span class="number">1</span>)]</span><br><span class="line">            <span class="comment"># 比较矩形窗口其他像素和中心像素的大小</span></span><br><span class="line">            comparison = window_image - reference</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(comparison.shape[<span class="number">0</span>]):</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(comparison.shape[<span class="number">1</span>]):</span><br><span class="line">                    <span class="keyword">if</span> (i, j) != (y_offset, x_offset):</span><br><span class="line">                        <span class="comment"># 如果比中心像素小则编码为 1， 否则为 0</span></span><br><span class="line">                        <span class="keyword">if</span> comparison[j, i] &lt; <span class="number">0</span>:</span><br><span class="line">                            bit = <span class="number">1</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            bit = <span class="number">0</span></span><br><span class="line">                        binary_pattern.append(<span class="built_in">str</span>(bit))</span><br><span class="line"></span><br><span class="line">            binary_pattern = <span class="string">&quot;&quot;</span>.join(binary_pattern)</span><br><span class="line">            <span class="comment"># 将二进制编码存储为十进制数字</span></span><br><span class="line">            decimal_number = <span class="built_in">int</span>(binary_pattern, base=<span class="number">2</span>)</span><br><span class="line">            census[y, x] = decimal_number</span><br><span class="line">    <span class="keyword">return</span> census</span><br></pre></td></tr></table></figure>

<p>现在利用 CensusTransform 函数对左右图进行变换得到它们的 census 特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">left_census  = CensusTransform(left_image)</span><br><span class="line">right_census = CensusTransform(right_image)</span><br></pre></td></tr></table></figure>

<p>我们可以对左图的 Census 特征进行可视化，得到下图所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv2.imwrite(<span class="string">&quot;left_census.png&quot;</span>, left_census.astype(np.uint8))</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/left_census.png">
</p>

<h2 id="Cost-Volume"><a href="#Cost-Volume" class="headerlink" title="Cost Volume"></a>Cost Volume</h2><p>经过census变换后的图像可以使用汉明距离来计算左右两个匹配点之间相似度，这里并没有使用它们的灰度值，而是它们的 census 序列。这是因为单个像素点的灰度差异进行比较没有多大意义，而用该像素点领域范围内的纹理特征（census 序列）进行比较更具有代表性。</p>
<h3 id="汉明距离"><a href="#汉明距离" class="headerlink" title="汉明距离"></a>汉明距离</h3><p>两个 census 序列之间的相似度比较使用的是 Hamming 距离，它的度量方式为：<strong>两个字符串对应位置的不同字符的个数</strong>，它本身是一个异或问题，可以使用 <code>numpy.bitwise_xor</code> 进行求解。</p>
<table><tr><td bgcolor=Bisque>异或（xor）问题：如果 a、b 两个值不相同，则异或结果为 1。如果 a、b 两个值相同，异或结果为 0。</td></tr></table>

<p>Examples：数字 13 的二进制编码为 <code>00001101</code>， 17 则为 <code>00010001</code>，那么它们之间的 Hamming 距离为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.bitwise_xor(<span class="number">13</span>, <span class="number">17</span>)</span><br><span class="line"><span class="number">28</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>xor = np.binary_repr(<span class="number">28</span>)</span><br><span class="line"><span class="string">&#x27;11100&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>xor.count(<span class="string">&#x27;1&#x27;</span>)</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>13 和 17 的二进制编码有 3 个字符不同，所以它们的 Hamming 距离为 3。综上来说，Hamming 距离的计算代码如下：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HanMingDistance</span>(<span class="params">a, b</span>):</span></span><br><span class="line">    xor = np.int64(np.bitwise_xor(a, b))</span><br><span class="line">    xor = np.binary_repr(xor)</span><br><span class="line">    distance = xor.count(<span class="string">&#x27;1&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> distance</span><br></pre></td></tr></table></figure>

<h3 id="代价计算"><a href="#代价计算" class="headerlink" title="代价计算"></a>代价计算</h3><p>在极线约束下，我们会对右图从左至右进行扫描: 在右图 u 的位置得到该像素的 census 序列，然后与左图 u+d 位置处进行比较。由于我们事先不知道该处的视差到底有多大，因此我们会假设一个最大视差值 <code>max_disparity</code>，并计算 <code>0, 1, 2, ..., max_disparity</code> 处所有的 Hamming 距离。这个过程称为代价计算，如下图所示：</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/Hamming.jpg" alt="image"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_disparity = <span class="number">64</span></span><br><span class="line">cost_volume = np.zeros(shape=(image_h, image_w, max_disparity), dtype=np.uint32)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, max_disparity):</span><br><span class="line">    shift_census = np.zeros(shape=(image_h, image_w), dtype=np.int64)</span><br><span class="line">    shift_census[:, x_offset:(image_w - x_offset - d)] = left_census[:, (x_offset + d):(image_w - x_offset)]</span><br><span class="line">    shift_census[:, (image_w - x_offset - d):(image_w - x_offset)] = \</span><br><span class="line">            left_census[:, (image_w - x_offset - max_disparity):(image_w - x_offset - max_disparity + d)]</span><br><span class="line"></span><br><span class="line">    f = np.frompyfunc(HanMingDistance, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    distance = f(shift_census, right_census)</span><br><span class="line">    cost_volume[:, :, d] = distance</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="35%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/cost_volume.png">
</p>

<p>既然现在已经计算出了每个像素在不同视差 d 时的汉明距离，那么其最小值对应的视差理应最接近该像素的真实视差，从而我们可以得到它的视差图并将其进行归一化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">disparity_map, max_disparity=<span class="number">64</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    transforms values from the range (0, 64) to (0, 255).</span></span><br><span class="line"><span class="string">    :param volume: n dimension array to normalize.</span></span><br><span class="line"><span class="string">    :param max_disparity: maximuim value of disparity</span></span><br><span class="line"><span class="string">    :return: normalized array.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">255.0</span> * disparity_map / max_disparity</span><br><span class="line">    </span><br><span class="line">disp = np.argmin(cost_volume, -<span class="number">1</span>).astype(np.uint8)</span><br><span class="line">disp = normalize(disp)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;cost_volume_disp.png&quot;</span>, disp)</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/cost_volume_disp.png">
</p>

<p>我们可以发现视差图中出现了很多椒盐噪声，因此可以考虑使用中值滤波算法进行去燥，得到下图：</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/cost_volume_refine_disp.png">
</p>

<p>图中的一些连续平面区域依然出现了很多噪声，而且对于视差不连续的区域其效果特别差。因此我们还需要在此基础上加入一些平滑处理，并构造出了一个能量方程。从而使得立体匹配问题可以转换成寻找最优视差图 <code>D</code>，让能量方程 <code>E(D)</code> 取得最小值。</p>
<p align="center">
    <img width="80%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/MommyTalk1600751560479.jpg">
</p>

<p>该能量方程由两部分组成：等式右边第一项表示像素点 <code>p</code> 在视差范围内所以匹配代价之和; 第二项和第三项是指当前像素 <code>p</code> 和其邻域内所有像素 <code>q</code> 之间的平滑性约束, 它是 SGM 算法的核心，将在下节对此进行讲述。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Heiko Hirschmuller. <a href="https://core.ac.uk/download/pdf/11134866.pdf">stereo Processing by Semi-Global Matching and Mutual Information</a>. CVPR 2005</li>
<li>[2] <a href="http://lunokhod.org/?p=1356">LUNOKHOD SGM Blog Post</a></li>
</ul>
]]></content>
      <categories>
        <category>立体视觉</category>
      </categories>
      <tags>
        <tag>视差估计</tag>
        <tag>立体匹配</tag>
        <tag>汉明距离</tag>
      </tags>
  </entry>
  <entry>
    <title>手写双目立体匹配 SGM 算法（下)</title>
    <url>/2020/01/20/SGM_02/</url>
    <content><![CDATA[<p>上节的内容主要对 SGM 算法的匹配代价体 (Cost Volume) 进行了详细介绍，发现如果只寻找逐个像素匹配代价的最优解会使得视差图对噪声特别敏感。因此在能量方程中考虑使用该点像素的邻域视差数据来构造惩罚函数以增加平滑性约束, 这个求解过程也称为<strong>代价聚合 (Cost Aggregation) </strong>的过程。 </p>
<h2 id="能量方程"><a href="#能量方程" class="headerlink" title="能量方程"></a>能量方程</h2><p>SGM 算法建立了能量方程，并引入了视差约束条件，以对此进行优化：</p>
<p align="center">
    <img width="80%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/MommyTalk1600751174763.jpg">
</p>

<span id="more"></span>

<p>等式右边第一项表示像素点 <code>p</code> 在视差范围内所以匹配代价之和; 第二项和第三项是指当前像素 <code>p</code> 和其邻域内所有像素 <code>q</code> 之间的平滑性约束，增加了惩罚因子 <code>P1</code> 和 <code>P2</code>: 若 <code>p</code> 和 <code>q</code> 的视差的差值等于 1，则惩罚因子 <code>P1</code>，若差值大于 1，则惩罚因子为 <code>P2</code>。 </p>
<p>为了高效地求解它，SGM 提出一种路径代价聚合的思路，即将像素所有视差下的匹配代价进行像素周围所有路径上的一维聚合得到路径下的路径代价值，然后将所有路径代价值相加得到该像素聚合后的匹配代价值。</p>
<h2 id="路径代价"><a href="#路径代价" class="headerlink" title="路径代价"></a>路径代价</h2><p>设 <code>L_&#123;r&#125;</code> 表示穿过 <code>r</code> 方向的扫描路径代价，其计算方式如下所示：</p>
<p align="center">
    <img width="80%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/MommyTalk1600751250322.jpg">
</p>

<p>等号右边第一项表示像素点 <code>p</code> 的初始匹配代价；第二项表示 <code>p</code> 的前一个像素点 <code>p − r</code> 的最小匹配代价：若和 <code>p</code> 的视差差值为 0，无需加任何惩罚因子，差值为 1，加惩罚因子 <code>P1</code> ，若差值大于 1，则惩罚因子为 <code>P2</code>；第三项表示前一个像素点 <code>p − r</code> 沿 <code>r</code> 路径上的最小匹配代价，加入该项的目的是抑制 <code>L_&#123;r&#125;( p, d )</code> 的数值过大，并不会影响视差空间。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_path_cost</span>(<span class="params"><span class="built_in">slice</span>, offset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    part of the aggregation step, finds the minimum costs in a D x M slice (where M = the number of pixels in the</span></span><br><span class="line"><span class="string">    given direction)</span></span><br><span class="line"><span class="string">    :param slice: M x D array from the cost volume.</span></span><br><span class="line"><span class="string">    :param offset: ignore the pixels on the border.</span></span><br><span class="line"><span class="string">    :return: M x D array of the minimum costs for a given slice in a given direction.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    P1 = <span class="number">5</span></span><br><span class="line">    P2 = <span class="number">70</span></span><br><span class="line"></span><br><span class="line">    slice_dim, disparity_dim = <span class="built_in">slice</span>.shape</span><br><span class="line">    disparities = [d <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(disparity_dim)] * disparity_dim</span><br><span class="line">    disparities = np.array(disparities).reshape(disparity_dim, disparity_dim)</span><br><span class="line">    penalties = np.zeros(shape=(disparity_dim, disparity_dim), dtype=np.uint32)</span><br><span class="line"></span><br><span class="line">    penalties[np.<span class="built_in">abs</span>(disparities - disparities.T) == <span class="number">1</span>] = P1</span><br><span class="line">    penalties[np.<span class="built_in">abs</span>(disparities - disparities.T)  &gt; <span class="number">1</span>] = P2</span><br><span class="line"></span><br><span class="line">    minimum_cost_path = np.zeros(shape=(other_dim, disparity_dim), dtype=np.uint32)</span><br><span class="line">    minimum_cost_path[offset - <span class="number">1</span>, :] = <span class="built_in">slice</span>[offset - <span class="number">1</span>, :]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(offset, other_dim):</span><br><span class="line">        previous_cost = minimum_cost_path[i - <span class="number">1</span>, :]</span><br><span class="line">        current_cost = <span class="built_in">slice</span>[i, :]</span><br><span class="line">        costs = np.repeat(previous_cost, repeats=disparity_dim, axis=<span class="number">0</span>).reshape(disparity_dim, disparity_dim)</span><br><span class="line">        costs = np.amin(costs + penalties, axis=<span class="number">0</span>)</span><br><span class="line">        minimum_cost_path[i, :] = current_cost + costs - np.amin(previous_cost)</span><br><span class="line">    <span class="keyword">return</span> minimum_cost_path</span><br></pre></td></tr></table></figure>

<h2 id="代价聚合"><a href="#代价聚合" class="headerlink" title="代价聚合"></a>代价聚合</h2><h3 id="单路聚合"><a href="#单路聚合" class="headerlink" title="单路聚合"></a>单路聚合</h3><p>如果我们只考虑单路扫描会是什么样的结果呢？假设代价聚合路线为从上至下，即 south 方向（2 号路线）。如下图所示，那么</p>
<p align="center">
    <img width="25%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/south.png">
</p>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">main_aggregation   = np.zeros(shape=(image_h, image_w, max_disparity), dtype=np.uint32)</span><br><span class="line">aggregation_volume = np.zeros(shape=(image_h, image_w, max_disparity, <span class="number">1</span>), dtype=np.uint32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, image_w):</span><br><span class="line">    south = cost_volume[<span class="number">0</span>:image_h, x, :]</span><br><span class="line">    main_aggregation[:, x, :] = get_path_cost(south, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">aggregation_volume[:, :, :, <span class="number">0</span>] = main_aggregation</span><br></pre></td></tr></table></figure>

<p>便得到了<strong>代价聚合体（aggregation_volume）</strong>进行视差计算，找到该路径下的聚合最小匹配代价值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_disparity</span>(<span class="params">aggregation_volume</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    last step of the sgm algorithm, corresponding to equation 14 followed by winner-takes-all approach.</span></span><br><span class="line"><span class="string">    :param aggregation_volume: H x W x D x N array of matching cost for all defined directions.</span></span><br><span class="line"><span class="string">    :return: disparity image.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    volume = np.<span class="built_in">sum</span>(aggregation_volume, axis=<span class="number">3</span>)</span><br><span class="line">    disparity_map = np.argmin(volume, axis=<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> disparity_map</span><br></pre></td></tr></table></figure>
<p>最后通过最小化该路径在一维聚合下的匹配代价，得到了视差图如下图所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">disparity_map = select_disparity(aggregation_volume)</span><br><span class="line">disparity_map = normalize(disparity_map)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;scan_south_disp.png&quot;</span>, disparity_map)</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/scan_south_disp.png">
</p>

<h3 id="多路聚合"><a href="#多路聚合" class="headerlink" title="多路聚合"></a>多路聚合</h3><p>从单路扫描的结果可以看出，一维扫描线优化仅仅只受一个方向约束，容易造成“条纹效应”，所以在 SGM 算法中，聚合了多个扫描路径上的匹配代价。</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/scanline.png">
</p>

<p>一共有八个方向：south 和 north，east 和 west，south_east 和 north_west， south_west 和 north_east。刚刚实现了 south 方向单路扫描聚合代价的最优求解，而其他路扫描聚合代价的过程和它是类似的。限于篇幅，在这里就不补充了，最后得到的视差图如下所示：</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/SGM/disparity_map.png">
</p>


]]></content>
      <categories>
        <category>立体视觉</category>
      </categories>
      <tags>
        <tag>视差估计</tag>
      </tags>
  </entry>
  <entry>
    <title>双目测距和三维重建</title>
    <url>/2019/12/29/StereoVision/</url>
    <content><![CDATA[<p>双目相机通过同步采集左右相机的图像，计算图像间视差，来估计每一个像素的深度。一旦我们获取了物体在图像上的每个像素深度，我们便能重构出一些它的三维信息。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/StereoVision/result.gif">
</p>

<p>双目相机一般由左眼和右眼两个水平放置的相机组成，其距离称为双目相机的基线(Baseline, 记作 b)，是双目的重要参数。由于左右两个相机之间有一定距离，因此同一个物体在左右图上的横坐标会有一些差异，称为<font color=red><strong>视差(Disparity)</strong></font>。</p>
<span id="more"></span>

<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/StereoVision/disparity.png">
</p>


<p>根据视差，我们可以估计一个像素离相机的距离。</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/StereoVision/triangle_measurement.png">
</p>

<p align="center">
    <img width="28%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/StereoVision/MommyTalk1600749965692.jpg">
</p>

<p>根据相机坐标系点 <code>P</code> 坐标为 <code>(X, Y, Z)</code> 到图像坐标系 <code>(u, v)</code> 之间的投影关系：</p>
<p align="center">
    <img width="20%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/StereoVision/MommyTalk1600750017380.jpg">
</p>

<p>从而反推得到点 <code>P</code> 的坐标信息</p>
<p align="center">
    <img width="17%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/StereoVision/MommyTalk1600750052615.jpg">
</p>

<p>例如，以下图片为例：</p>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center"></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/StereoVision/000004_10.jpg" alt="原图"></td>
<td align="center"><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/StereoVision/000004_10_disp.png" alt="视差图"></td>
</tr>
</tbody></table>
<blockquote>
<p>视差图采用十六位 (uint16) 整数来存取，并将视差值扩大了 256 倍，所以在读取时需要除以256。</p>
</blockquote>
<p>鼠标右键将原图和视差图下载下来，然后安装好第三方库 PyOpenGL==3.1.0 和 <a href="https://github.com/uoip/pangolin">pangolin</a> 即可执行以下程序便得到了动图的结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> pangolin</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> OpenGL.GL <span class="keyword">as</span> gl</span><br><span class="line"></span><br><span class="line">fx = <span class="number">721.5377</span></span><br><span class="line">fy = <span class="number">721.5377</span></span><br><span class="line">cx = <span class="number">607.1928</span></span><br><span class="line">cy = <span class="number">185.2157</span></span><br><span class="line">B  = <span class="number">0.54</span></span><br><span class="line"></span><br><span class="line">img_disp = cv2.imread(<span class="string">&#x27;000004_10_disp.png&#x27;</span>, -<span class="number">1</span>) / <span class="number">256.</span></span><br><span class="line">imgL = cv2.imread(<span class="string">&#x27;000004_10.jpg&#x27;</span>)</span><br><span class="line">imgL = cv2.cvtColor(imgL, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line">h, w = imgL.shape[:<span class="number">2</span>]</span><br><span class="line">f = <span class="number">0.5</span> * w</span><br><span class="line">points, colors = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">range</span>(h):</span><br><span class="line">    <span class="keyword">for</span> u <span class="keyword">in</span> <span class="built_in">range</span>(w):</span><br><span class="line">        disp = img_disp[v, u]</span><br><span class="line">        <span class="keyword">if</span> disp &gt; <span class="number">0.</span>:</span><br><span class="line">            depth = B * fx / disp</span><br><span class="line">            z_w = depth</span><br><span class="line">            x_w = (u - cx) * z_w / fx</span><br><span class="line">            y_w = (v - cy) * z_w / fy</span><br><span class="line">            points.append([x_w, y_w, z_w])</span><br><span class="line">            colors.append(imgL[v, u])</span><br><span class="line">points = np.array(points)</span><br><span class="line">colors = np.array(colors) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">pangolin.CreateWindowAndBind(<span class="string">&#x27;Main&#x27;</span>, <span class="number">640</span>, <span class="number">480</span>)</span><br><span class="line">gl.glEnable(gl.GL_DEPTH_TEST)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define Projection and initial ModelView matrix</span></span><br><span class="line">scam = pangolin.OpenGlRenderState(</span><br><span class="line">    pangolin.ProjectionMatrix(<span class="number">640</span>, <span class="number">480</span>, <span class="number">2000</span>, <span class="number">2000</span>, <span class="number">320</span>, <span class="number">240</span>, <span class="number">0.1</span>, <span class="number">1000</span>),</span><br><span class="line">    pangolin.ModelViewLookAt(<span class="number">0</span>, <span class="number">0</span>, -<span class="number">20</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">handler = pangolin.Handler3D(scam)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Interactive View in window</span></span><br><span class="line">dcam = pangolin.CreateDisplay()</span><br><span class="line">dcam.SetBounds(<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>, -<span class="number">640.0</span>/<span class="number">480.0</span>)</span><br><span class="line">dcam.SetHandler(handler)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> pangolin.ShouldQuit():</span><br><span class="line">    gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)</span><br><span class="line">    gl.glClearColor(<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>)</span><br><span class="line">    dcam.Activate(scam)</span><br><span class="line">    gl.glPointSize(<span class="number">3</span>)</span><br><span class="line">    pangolin.DrawPoints(points, colors)</span><br><span class="line">    pangolin.FinishFrame()</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>立体视觉</category>
      </categories>
      <tags>
        <tag>三维重建</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 的多卡 GPU 训练机制</title>
    <url>/2020/02/07/TensorFlow-%E7%9A%84%E5%A4%9A%E5%8D%A1-GPU-%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<p>武汉疫情还没过去，这几天窝在家里琢磨了下 TensorFlow 的多卡 GPU 分布式训练的机制。本文将使用流行的 MNIST 数据集上训练一个 MobileNetV2 模型，并利用 <code>tf.distribute.Strategy</code> 函数实现多卡 GPU 对训练方式。 详细代码见 <a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/blob/master/7-Utils/multi_gpu_train.py"><font color=Red>TensorFlow2.0-Example</font></a></p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/TensorFlow-的多卡-GPU-训练机制/th.jpg">
</p>

<span id="more"></span>

<h2 id="下载-MNIST-数据集"><a href="#下载-MNIST-数据集" class="headerlink" title="下载 MNIST 数据集"></a>下载 MNIST 数据集</h2><p>点击<a href="https://github.com/YunYang1994/yymnist/releases/download/v1.0/mnist.zip"><font color=Red>这里</font></a>可以下载到 mnist.zip，将它们解压得到以下目录结构：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── test</span><br><span class="line">│   ├── 0</span><br><span class="line">│   ├── 1</span><br><span class="line">│   ├── 2</span><br><span class="line">│   ├── 3</span><br><span class="line">│   ├── 4</span><br><span class="line">│   ├── 5</span><br><span class="line">│   ├── 6</span><br><span class="line">│   ├── 7</span><br><span class="line">│   ├── 8</span><br><span class="line">│   └── 9</span><br><span class="line">└── train</span><br><span class="line">    ├── 0</span><br><span class="line">    ├── 1</span><br><span class="line">    ├── 2</span><br><span class="line">    ├── 3</span><br><span class="line">    ├── 4</span><br><span class="line">    ├── 5</span><br><span class="line">    ├── 6</span><br><span class="line">    ├── 7</span><br><span class="line">    ├── 8</span><br><span class="line">    └── 9</span><br><span class="line"></span><br><span class="line">22 directories, 0 files</span><br></pre></td></tr></table></figure>

<h2 id="创建一个分发变量和图的策略"><a href="#创建一个分发变量和图的策略" class="headerlink" title="创建一个分发变量和图的策略"></a>创建一个分发变量和图的策略</h2><p>接下来将会使用到 <code>tf.distribute.MirroredStrategy</code> ，它是如何运作的？</p>
<ul>
<li>所有变量和模型图都复制在副本上；</li>
<li>输入都均匀分布在副本中；</li>
<li>每个副本在收到输入后计算输入的损失和梯度；</li>
<li>通过求和，每一个副本上的梯度都能同步；</li>
<li>同步后，每个副本上的复制的变量都可以同样更新。</li>
</ul>
<p>你可以这样创建一个策略：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">strategy = tf.distribute.MirroredStrategy()</span><br></pre></td></tr></table></figure>
<p>或者指定使用特定的 GPU</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">strategy = tf.distribute.MirroredStrategy(devices=[<span class="string">&quot;/gpu:0&quot;</span>, <span class="string">&quot;/gpu:2&quot;</span>, <span class="string">&quot;/gpu:3&quot;</span>])</span><br></pre></td></tr></table></figure>

<h2 id="构建-MobileNetV2"><a href="#构建-MobileNetV2" class="headerlink" title="构建 MobileNetV2"></a>构建 MobileNetV2</h2><p>使用 <code>tf.keras.applications.mobilenet_v2.MobileNetV2</code> 创建一个模型。你也可以使用模型子类化 API 来完成这个。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Defining Model</span></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    model = applications.mobilenet_v2.MobileNetV2(include_top=<span class="literal">False</span>, weights=<span class="literal">None</span>,</span><br><span class="line">                                                  input_shape=(IMG_SIZE,IMG_SIZE,<span class="number">3</span>))</span><br><span class="line">    x = tf.keras.layers.Input(shape=(IMG_SIZE,IMG_SIZE,<span class="number">3</span>))</span><br><span class="line">    y = model(x)</span><br><span class="line">    y = tf.keras.layers.AveragePooling2D()(y)</span><br><span class="line">    y = tf.keras.layers.Flatten()(y)</span><br><span class="line">    y = tf.keras.layers.Dense(<span class="number">512</span>,  activation=<span class="literal">None</span>)(y)</span><br><span class="line">    y = tf.keras.layers.Dense(<span class="number">10</span>,   activation=<span class="string">&#x27;softmax&#x27;</span>)(y)</span><br><span class="line">    model = tf.keras.models.Model(inputs=x, outputs=y)</span><br><span class="line">    optimizer = tf.keras.optimizers.Adam(<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure>

<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><p>在多卡 GPU 的训练方式中，<code>tf.distribute.Strategy</code> 是如何计算损失的呢？</p>
<ul>
<li>举一个例子，假设您有 4 个 GPU，批量大小为 64. 输入的一个批次分布在各个副本（ 4个 GPU）上，每个副本获得的输入大小为 16。</li>
<li>每个副本上的模型使用其各自的输入执行正向传递并计算损失, 使用 <code>tf.nn.compute_average_loss</code> 来获取每张 GPU 卡的训练损失，并通过 <code>global_batch_size</code> 返回缩放损失。（相当于<code>scale_loss = tf.reduce_sum(loss) * (1. / GLOBAL_BATCH_SIZE)</code>）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Defining Loss and Metrics</span></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">    loss_object = tf.keras.losses.CategoricalCrossentropy(</span><br><span class="line">        reduction=tf.keras.losses.Reduction.NONE</span><br><span class="line">    )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span>(<span class="params">labels, predictions</span>):</span></span><br><span class="line">        per_example_loss = loss_object(labels, predictions)</span><br><span class="line">        <span class="keyword">return</span> tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZE)</span><br><span class="line"></span><br><span class="line">    train_accuracy = tf.keras.metrics.CategoricalAccuracy(</span><br><span class="line">        name=<span class="string">&#x27;train_accuracy&#x27;</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<h2 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h2><ul>
<li>我们使用 <code>for x in ...</code> 迭代构造 train_dataset ；</li>
<li>缩放损失是 <code>distributed_train_step</code> 的返回值。 这个值会在各个副本使用<code>tf.distribute.Strategy.reduce</code> 的时候合并，然后通过 <code>tf.distribute.Strategy.reduce</code> 叠加各个返回值来跨批次。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Defining Training Loops</span></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">distributed_train_step</span>(<span class="params">dataset_inputs</span>):</span></span><br><span class="line">        per_replica_losses = strategy.experimental_run_v2(train_step,</span><br><span class="line">                                                          args=(dataset_inputs,))</span><br><span class="line">        <span class="keyword">return</span> strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,</span><br><span class="line">                               axis=<span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">        batchs_per_epoch = <span class="built_in">len</span>(train_generator)</span><br><span class="line">        train_dataset    = <span class="built_in">iter</span>(train_generator)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tqdm(total=batchs_per_epoch,</span><br><span class="line">                  desc=<span class="string">&quot;Epoch %2d/%2d&quot;</span> %(epoch+<span class="number">1</span>, EPOCHS)) <span class="keyword">as</span> pbar:</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(batchs_per_epoch):</span><br><span class="line">                batch_loss = distributed_train_step(<span class="built_in">next</span>(train_dataset))</span><br><span class="line">                batch_acc  = train_accuracy.result()</span><br><span class="line">                pbar.set_postfix(&#123;<span class="string">&#x27;loss&#x27;</span> : <span class="string">&#x27;%.4f&#x27;</span> %batch_loss,</span><br><span class="line">                                  <span class="string">&#x27;accuracy&#x27;</span> : <span class="string">&#x27;%.6f&#x27;</span> %batch_acc&#125;)</span><br><span class="line">                train_accuracy.reset_states()</span><br><span class="line">                pbar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>多卡GPU训练</tag>
      </tags>
  </entry>
  <entry>
    <title>医学图片分割网络 —— Unet</title>
    <url>/2018/11/12/Unet/</url>
    <content><![CDATA[<p>Unet 是 Kaggle 语义分割挑战赛上的常客。因为它简单，高效，易懂，容易定制，最主要的是它可以从相对较小的数据集中学习。在医学图像处理领域，各路高手更是拿着 Unet 各种魔改。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>U-Net 与 FCN 非常的相似（比如都没有使用全连接层），U-Net 比 FCN 稍晚提出来，但都发表在 2015 年，和 FCN 相比，U-Net 的第一个特点是完全对称，也就是左边和右边是很类似的。当我第一次看到该网络的拓扑结构时，顿时惊为天人，卧槽，简直是一个大写的 U。</p>
<p align="center">
    <img width="65%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Unet/Unet.png">
</p>

<span id="more"></span>

<p>其次，Unet 与 FCN 第二个不同点就是 skip-connections（跳跃连接）的操作不一样：FCN 采用的是 tf.add，而 Unet 则使用的 tf.concat 操作，它们之间的区别在于前者在 pixel-to-pixel 上直接相加，而后者是相叠加而改变了通道数目。Unet 的网络结构主要包括三点：</p>
<ul>
<li>下采样路径, 论文里称为 The contracting path；</li>
<li>Bottleneck 结构；</li>
<li>上采样路径，论文里称为 The expanding path；</li>
</ul>
<h2 id="下采样路径"><a href="#下采样路径" class="headerlink" title="下采样路径"></a>下采样路径</h2><p>下采样路径一共由4个模块组成，每个模块的结构为：</p>
<ul>
<li>3x3 的卷积操作 + relu 激活函数；</li>
<li>3x3 的卷积操作 + relu 激活函数；</li>
<li>2x2 的 Pooling 操作。</li>
</ul>
<blockquote>
<p>It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.</p>
</blockquote>
<p>但是值得一提的是每次在 Pooling 结构后，feature map 的通道数目就会加倍，最终 feature map 的空间尺寸越来越小，而通道数目越来越多。这样做的目的是为了捕获输入图像的上下文信息，以便能够进行分割。随后，这些粗略的上下文信息随后将通过跳跃连接传输到上采样路径。</p>
<h2 id="Bottleneck-结构"><a href="#Bottleneck-结构" class="headerlink" title="Bottleneck 结构"></a>Bottleneck 结构</h2><p>瓶颈结构，顾名思义，就是在下采样和上采样之间的结构。它由两个卷积层组成，并且后面接有 dropout。在代码里是这样实现的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conv5 = Conv2D(<span class="number">1024</span>, <span class="number">3</span>, activation = <span class="string">&#x27;relu&#x27;</span>, padding = <span class="string">&#x27;same&#x27;</span>)(pool4)</span><br><span class="line">conv5 = Conv2D(<span class="number">1024</span>, <span class="number">3</span>, activation = <span class="string">&#x27;relu&#x27;</span>, padding = <span class="string">&#x27;same&#x27;</span>)(conv5)</span><br><span class="line">drop5 = Dropout(<span class="number">0.5</span>)(conv5)</span><br></pre></td></tr></table></figure>

<h2 id="上采样路径"><a href="#上采样路径" class="headerlink" title="上采样路径"></a>上采样路径</h2><p>跟下采样结构一样，上采样结构也是由 4 个模块组成，这样才能对称，每个模块都结构为:</p>
<ul>
<li>上采样层；</li>
<li>3x3 卷积操作 + relu 激活函数；</li>
</ul>
<p>不过这里面多了一些骚操作，在对称的地方与来自下采样路径的跳跃连接进行了 Concate 操作，从而融合网络的浅层特征。</p>
<h2 id="个人思考"><a href="#个人思考" class="headerlink" title="个人思考"></a>个人思考</h2><p>从 Unet 的主体结构设计来看，其实是借鉴了 Hinton 祖师爷的自编码网络。仔细看 Unet 会发现：它的下采样结构其实是一个编码过程，所谓的编码就是尽可能地压缩图像的空间信息，而保留最本质的特征；上采样结构则是一个解码过程，尽可能地还原到原来的图像。在 FCN 的 skip-connection 提出后，这几乎成了语义分割和目标检测领域的标配，因此 Unet 网络避免不了得用它。所以 Unet 更像是 AutoEncoder 与 FCN 的结合版。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Olaf Ronneberger, Philipp Fischer, Thomas Brox. <a href="https://arxiv.org/abs/1505.04597">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>, accepted at MICCAI 2015</li>
</ul>
]]></content>
      <categories>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>Skip Connection</tag>
        <tag>Unet 网络结构</tag>
      </tags>
  </entry>
  <entry>
    <title>TensorFlow 模型转化 tflite</title>
    <url>/2019/05/16/Tensorflow-%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8C%96-tflite/</url>
    <content><![CDATA[<p>自从有了TensorFlow Lite，应用开发者可以在移动设备上很轻松地部署神经网络。</p>
<p align="center">
    <img width="90%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/Tensorflow-模型转化-tflite/tflite.jpg">
</p>

<span id="more"></span>

<p>Tensorflow Lite 转化器可以将我们的训练模型转化成 <code>.tflite</code> 文件，它分别支持 <a href="https://tensorflow.google.cn/guide/saved_model"><font color=Red>SavedModel directories</font></a>, <a href="https://tensorflow.org/guide/concrete_function"><font color=Red>concrete functions</font></a> 和 <a href="https://tensorflow.google.cn/guide/keras/overview"><font color=Red>tf.keras models</font></a>三种结构。由于我经常使用的是 <code>tf.keras.model</code> 结构，因此只对它进行详细介绍。</p>
<p>以 mtcnn 网络的 rnet 模型为例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNet</span>(<span class="params">tf.keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = tf.keras.layers.Conv2D(<span class="number">28</span>, <span class="number">3</span>, <span class="number">1</span>, name=<span class="string">&#x27;conv1&#x27;</span>)</span><br><span class="line">        self.prelu1 = tf.keras.layers.PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>], name=<span class="string">&quot;prelu1&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.conv2 = tf.keras.layers.Conv2D(<span class="number">48</span>, <span class="number">3</span>, <span class="number">1</span>, name=<span class="string">&#x27;conv2&#x27;</span>)</span><br><span class="line">        self.prelu2 = tf.keras.layers.PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>], name=<span class="string">&quot;prelu2&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.conv3 = tf.keras.layers.Conv2D(<span class="number">64</span>, <span class="number">2</span>, <span class="number">1</span>, name=<span class="string">&#x27;conv3&#x27;</span>)</span><br><span class="line">        self.prelu3 = tf.keras.layers.PReLU(shared_axes=[<span class="number">1</span>,<span class="number">2</span>], name=<span class="string">&quot;prelu3&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.dense4 = tf.keras.layers.Dense(<span class="number">128</span>, name=<span class="string">&#x27;conv4&#x27;</span>)</span><br><span class="line">        self.prelu4 = tf.keras.layers.PReLU(shared_axes=<span class="literal">None</span>, name=<span class="string">&quot;prelu4&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.dense5_1 = tf.keras.layers.Dense(<span class="number">2</span>, name=<span class="string">&quot;conv5-1&quot;</span>)</span><br><span class="line">        self.dense5_2 = tf.keras.layers.Dense(<span class="number">4</span>, name=<span class="string">&quot;conv5-2&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.flatten = tf.keras.layers.Flatten()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self, x, training=<span class="literal">False</span></span>):</span></span><br><span class="line">        out = self.prelu1(self.conv1(x))</span><br><span class="line">        out = tf.nn.max_pool2d(out, <span class="number">3</span>, <span class="number">2</span>, padding=<span class="string">&quot;SAME&quot;</span>)</span><br><span class="line">        out = self.prelu2(self.conv2(out))</span><br><span class="line">        out = tf.nn.max_pool2d(out, <span class="number">3</span>, <span class="number">2</span>, padding=<span class="string">&quot;VALID&quot;</span>)</span><br><span class="line">        out = self.prelu3(self.conv3(out))</span><br><span class="line">        out = self.flatten(out)</span><br><span class="line">        out = self.prelu4(self.dense4(out))</span><br><span class="line">        score = tf.nn.softmax(self.dense5_1(out), -<span class="number">1</span>)</span><br><span class="line">        boxes = self.dense5_2(out)</span><br><span class="line">        <span class="keyword">return</span> boxes, score</span><br></pre></td></tr></table></figure>

<p>接下来就是对模型进行转化和量化了，转换器可以配置为应用各种优化措施（optimizations），这些优化措施可以提高性能，减少文件大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rnet.predict(tf.ones(shape=[<span class="number">1</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">3</span>]))</span><br><span class="line">rnet_converter = tf.lite.TFLiteConverter.from_keras_model(rnet)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 量化（quantization）可以减小模型的大小和推理所需的时间</span></span><br><span class="line">rnet_converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存 tflite 模型</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;rnet.tflite&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    rnet_tflite_model = rnet_converter.convert()</span><br><span class="line">    f.write(rnet_tflite_model)</span><br></pre></td></tr></table></figure>

<p>参考文献:</p>
<ul>
<li>[1] <a href="https://tensorflow.google.cn/lite/guide/get_started#4_optimize_your_model_optional">TensorFlow 中文开发指南</a></li>
<li>[2] <a href="https://github.com/YunYang1994/TensorFlow2.0-Examples/blob/master/4-Object_Detection/MTCNN/mtcnn.py">TensorFlow2.0-Example</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>移动端部署</tag>
      </tags>
  </entry>
  <entry>
    <title>人脸识别之人脸矫正</title>
    <url>/2020/02/16/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B9%8B%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/</url>
    <content><![CDATA[<p>一般来说，使用 <code>mtcnn</code> 网络检测到人脸后，都需要进行矫正。而对于人脸矫正，最简单的可以通过使用仿射变换来实现。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/人脸识别之人脸矫正/face_align.jpg">
</p>

<span id="more"></span>

<p>思路：</p>
<ul>
<li>通过 mtcnn 模型检测到人脸的 5 个特征点：左眼，右眼，鼻子，左嘴角，右嘴角；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bounding_boxes = &#123;</span><br><span class="line">        <span class="string">&#x27;box&#x27;</span>: [<span class="built_in">int</span>(bounding_box[<span class="number">0</span>]), <span class="built_in">int</span>(bounding_box[<span class="number">1</span>]),</span><br><span class="line">                <span class="built_in">int</span>(bounding_box[<span class="number">2</span>]), <span class="built_in">int</span>(bounding_box[<span class="number">3</span>])],</span><br><span class="line">        <span class="string">&#x27;confidence&#x27;</span>: bounding_box[-<span class="number">1</span>],</span><br><span class="line">        <span class="string">&#x27;keypoints&#x27;</span>: &#123;</span><br><span class="line">            <span class="string">&#x27;left_eye&#x27;</span>: (<span class="built_in">int</span>(keypoints[<span class="number">0</span>]), <span class="built_in">int</span>(keypoints[<span class="number">5</span>])),</span><br><span class="line">            <span class="string">&#x27;right_eye&#x27;</span>: (<span class="built_in">int</span>(keypoints[<span class="number">1</span>]), <span class="built_in">int</span>(keypoints[<span class="number">6</span>])),</span><br><span class="line">            <span class="string">&#x27;nose&#x27;</span>: (<span class="built_in">int</span>(keypoints[<span class="number">2</span>]), <span class="built_in">int</span>(keypoints[<span class="number">7</span>])),</span><br><span class="line">            <span class="string">&#x27;mouth_left&#x27;</span>: (<span class="built_in">int</span>(keypoints[<span class="number">3</span>]), <span class="built_in">int</span>(keypoints[<span class="number">8</span>])),</span><br><span class="line">            <span class="string">&#x27;mouth_right&#x27;</span>: (<span class="built_in">int</span>(keypoints[<span class="number">4</span>]), <span class="built_in">int</span>(keypoints[<span class="number">9</span>])),</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">bounding_box = bounding_boxes[<span class="string">&#x27;box&#x27;</span>]</span><br><span class="line">keypoints = bounding_boxes[<span class="string">&#x27;keypoints&#x27;</span>]</span><br></pre></td></tr></table></figure>

<ul>
<li>计算双眼中心点的位置 <code>eye_center</code> 和双眼连线的倾斜角度 <code>angle</code>；</li>
<li>通过 <code>eye_center</code> 和 <code>angle</code> 得到仿射变换矩阵 <code>rot_matrix</code>；</li>
<li>将整张图片进行<a href="https://yunyang1994.github.io/posts/AffineTransformation/"><font color=red>仿射变换</font></a>得到 <code>align_image</code>；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">align_face</span>(<span class="params">image, keypoints, scale=<span class="number">1.0</span></span>):</span></span><br><span class="line">    eye_center = (</span><br><span class="line">            (keypoints[<span class="string">&#x27;left_eye&#x27;</span>][<span class="number">0</span>] + keypoints[<span class="string">&#x27;right_eye&#x27;</span>][<span class="number">0</span>]) * <span class="number">0.5</span>,</span><br><span class="line">            (keypoints[<span class="string">&#x27;left_eye&#x27;</span>][<span class="number">1</span>] + keypoints[<span class="string">&#x27;right_eye&#x27;</span>][<span class="number">1</span>]) * <span class="number">0.5</span>,</span><br><span class="line">            )</span><br><span class="line">    dx = keypoints[<span class="string">&#x27;right_eye&#x27;</span>][<span class="number">0</span>] - keypoints[<span class="string">&#x27;left_eye&#x27;</span>][<span class="number">0</span>]</span><br><span class="line">    dy = keypoints[<span class="string">&#x27;right_eye&#x27;</span>][<span class="number">1</span>] - keypoints[<span class="string">&#x27;left_eye&#x27;</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    angle = cv2.fastAtan2(dy, dx)</span><br><span class="line">    rot_matrix = cv2.getRotationMatrix2D(eye_center, angle, scale=scale)</span><br><span class="line">    rot_image = cv2.warpAffine(image, rot_matrix, dsize=(image.shape[<span class="number">1</span>], image.shape[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">return</span> rot_image</span><br><span class="line"></span><br><span class="line">align_image = align_face(image, keypoints)</span><br></pre></td></tr></table></figure>

<ul>
<li>根据从 <font color=red>原图检测的 2D 框</font>从 <code>align_image</code> 图片中抠出人脸并保存。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xmin = bounding_box[<span class="number">0</span>]</span><br><span class="line">ymin = bounding_box[<span class="number">1</span>]</span><br><span class="line">xmax = bounding_box[<span class="number">2</span>]</span><br><span class="line">ymax = bounding_box[<span class="number">3</span>]</span><br><span class="line">crop_image = align_image[ymin:ymax, xmin:xmax, :]</span><br><span class="line">cv2.imwrite(<span class="string">&quot;align_face.jpg&quot;</span>, crop_image)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>仿射变换</tag>
        <tag>人脸矫正</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Pytorch 对 mnist 数字进行分类</title>
    <url>/2018/09/09/%E4%BD%BF%E7%94%A8-Pytorch-%E5%AF%B9-mnist-%E6%95%B0%E5%AD%97%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<p>一直以来就非常喜欢 Pytorch，今天就小试牛刀一下，用它实现对 mnist 数字进行分类。</p>
<p align="center">
    <img width="80%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/使用-Pytorch-对-mnist-数字进行分类/Pytorch.jpg">
</p>

<span id="more"></span>

<p>首先可以从<a href="https://github.com/YunYang1994/yymnist/releases/download/v1.0/mnist.zip"><font color=Red>这里</font></a>下载 mnist 数据集并解压，然后代码的开始部分如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, utils</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Device configuration</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>之后，我们会想办法构造一个手写数字数据集:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MnistDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;custom mnist dataset.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root_dir, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            root_dir (string): Directory with all the images.</span></span><br><span class="line"><span class="string">            transform (callable, optional): Optional transform to be applied</span></span><br><span class="line"><span class="string">                on a sample.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.data_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">            img_paths = glob.glob(root_dir + <span class="string">&quot;%d/*.jpg&quot;</span> %i)</span><br><span class="line">            <span class="keyword">for</span> img_path <span class="keyword">in</span> img_paths:</span><br><span class="line">                img_label = &#123;<span class="string">&#x27;img_path&#x27;</span>:img_path, <span class="string">&#x27;label&#x27;</span>:i &#125;</span><br><span class="line">                self.data_list.append(img_label)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line">        img_path = self.data_list[idx][<span class="string">&#x27;img_path&#x27;</span>]</span><br><span class="line">        image = transforms.ToTensor()(Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&#x27;RGB&#x27;</span>))</span><br><span class="line">        label = self.data_list[idx][<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">        <span class="keyword">return</span> image, label</span><br></pre></td></tr></table></figure>

<p>对数据集的装载使用的是 <code>torch.utils.data.DataLoader</code> 类，类中的 <code>dataset</code> 参数用于指定我们载入的数据集名称，<code>batch_size</code> 设置了每个 <code>batch</code> 的样本数量，<code>shuffle=True</code> 会在装载过程将数据的顺序打乱然后打包。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_dataset = MnistDataset(<span class="string">&quot;./mnist/train/&quot;</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset=train_dataset,</span><br><span class="line">    						batch_size=<span class="number">32</span>,</span><br><span class="line">    						shuffle=<span class="literal">True</span>,</span><br><span class="line">    						num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>在顺利完成数据的加载后，我们就可以搭建一个简单的 CNN 模型，如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Convolutional neural network (two convolutional layers) &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ConvNet, self).__init__()</span><br><span class="line">        self.layer1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">        self.layer2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">        self.fc = nn.Linear(<span class="number">7</span>*<span class="number">7</span>*<span class="number">32</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.layer1(x)</span><br><span class="line">        out = self.layer2(out)</span><br><span class="line">        out = out.reshape(out.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">model = ConvNet(num_classes=<span class="number">10</span>).to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p>在编写完搭建卷积神经网络的模型代码后，我们就可以开始对模型进行训练和对参数进行优化了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define training loops</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    model.train()</span><br><span class="line">    loss_value = <span class="number">0.</span></span><br><span class="line">    acc_value  = <span class="number">0.</span></span><br><span class="line">    num_batch  = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> tqdm(total=<span class="built_in">len</span>(train_loader),</span><br><span class="line">                desc=<span class="string">&quot;Epoch %2d/20&quot;</span> %(epoch+<span class="number">1</span>)) <span class="keyword">as</span> pbar:</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> train_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            output = model(data)</span><br><span class="line">            loss = F.nll_loss(output, target)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            num_batch  += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            loss_value += loss.item()</span><br><span class="line">            <span class="comment"># get the index of the max log-probability</span></span><br><span class="line">            pred = output.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">            correct = pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()</span><br><span class="line">            acc_value += correct / <span class="built_in">len</span>(target)</span><br><span class="line"></span><br><span class="line">            pbar.set_postfix(&#123;<span class="string">&#x27;loss&#x27;</span> :<span class="string">&#x27;%.4f&#x27;</span> %(loss_value / num_batch),</span><br><span class="line">                              <span class="string">&#x27;acc&#x27;</span>  :<span class="string">&#x27;%.4f&#x27;</span> %(acc_value  / num_batch)&#125;)</span><br><span class="line">            pbar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>在测试阶段，代码也非常简洁:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Define testing step</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    model(data)</span><br></pre></td></tr></table></figure>

<p>接着我们可以保存模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>然后下次便可以重新加载模型:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = ConvNet(num_classes=<span class="number">10</span>).to(device)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&quot;model.pth&quot;</span>))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>mnist 分类</tag>
      </tags>
  </entry>
  <entry>
    <title>修改 YOLOv5 源码在 DOTAv1.5 遥感数据集上进行旋转目标检测</title>
    <url>/2021/04/20/%E4%BF%AE%E6%94%B9-YOLOv5-%E6%BA%90%E7%A0%81%E5%9C%A8-DOTAv1.5-%E9%81%A5%E6%84%9F%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E6%97%8B%E8%BD%AC%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p><a href="https://github.com/ultralytics/yolov5">YOLOv5</a> 发布已经有一段时间了，但是我一直还没有怎么去用过它。机会终于来了，最近需要做一个「旋转目标检测」的项目。于是我想到用它来进行魔改，使其能输出目标的 <code>rotated bounding boxes</code>。</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/修改-YOLOv5-源码在-DOTAv1.5-遥感数据集上进行旋转目标检测/P0872.jpg">
</p>

<span id="more"></span>

<h2 id="DOTA-v1-5-遥感数据集"><a href="#DOTA-v1-5-遥感数据集" class="headerlink" title="DOTA-v1.5 遥感数据集"></a>DOTA-v1.5 遥感数据集</h2><p><a href="https://captain-whu.github.io/DOTA/index.html">DOTA</a> 是武汉大学制作的一个关于航拍遥感数据集，里面的每个目标都由一个任意的四边形边界框标注。这个数据集目前一共有 3 个版本，这里我们只使用和介绍其中的 DOTA-v1.5 版本。</p>
<h3 id="图片类别"><a href="#图片类别" class="headerlink" title="图片类别"></a>图片类别</h3><p>一共有 2806 张图片，40 万个实例，分为 16 个类别：<em><strong>飞机，轮船，储罐，棒球场，网球场，篮球场，地面跑道，港口，桥梁，大型车辆，小型车辆，直升机，环形交叉路口，足球场，游泳池，起重机。</strong></em></p>
<h3 id="标注方式"><a href="#标注方式" class="headerlink" title="标注方式"></a>标注方式</h3><p>每个目标都被一个四边框 <strong>oriented bounding box (OBB)</strong> 标注，其 4 个顶点的坐标表示为（x1, y1, x2, y2, x3, y3, x4, y4）。标注框的起始顶点为黄色，其余 3 个顶点则按照顺时针顺序排列。</p>
<p align="center">
    <img width="35%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/修改-YOLOv5-源码在-DOTAv1.5-遥感数据集上进行旋转目标检测/vertice.jpg">
</p>

<p>每张图片的标注内容为：在第一行 <code>imagesource</code> 表示图片的来源，<code>GoogleEarth</code> 或者 <code>GF-2</code>；第二行 <code>gsd</code> 表示的是<a href="https://zh.wikipedia.org/wiki/%E5%9C%B0%E9%9D%A2%E9%87%87%E6%A0%B7%E8%B7%9D%E7%A6%BB">地面采样距离（Ground Sample Distance，简称 GSD）</a>，如果缺失，则为 <code>null</code>；第三行以后则标注的是每个实例的四边框、类别和识别难易程度。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;imagesource&#x27;:imagesource</span><br><span class="line">&#x27;gsd&#x27;:gsd</span><br><span class="line">x1, y1, x2, y2, x3, y3, x4, y4, category, difficult</span><br><span class="line">x1, y1, x2, y2, x3, y3, x4, y4, category, difficult</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="数据下载"><a href="#数据下载" class="headerlink" title="数据下载"></a>数据下载</h3><ul>
<li>百度云盘：<a href="https://pan.baidu.com/s/1kWyRGaz">Training set</a>，<a href="https://pan.baidu.com/s/1qZCoF72">Validation set</a>，<a href="https://pan.baidu.com/s/1i6ly9Id">Testing images</a></li>
<li>谷歌云盘：<a href="https://drive.google.com/drive/folders/1gmeE3D7R62UAtuIFOB9j2M5cUPTwtsxK?usp=sharing">Training set</a>，<a href="https://drive.google.com/drive/folders/1n5w45suVOyaqY84hltJhIZdtVFD9B224?usp=sharing">Validation set</a>，<a href="https://drive.google.com/drive/folders/1mYOf5USMGNcJRPcvRVJVV1uHEalG5RPl?usp=sharing">Testing images</a></li>
</ul>
<p>建议通过百度云盘下载，得到的数据容量为：<code>train</code>，9.51G；<code>val</code>，3.11G。</p>
<h2 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h2><h3 id="加载显示图片"><a href="#加载显示图片" class="headerlink" title="加载显示图片"></a>加载显示图片</h3><p>我们需要用到官方发布的 <a href="https://github.com/CAPTAIN-WHU/DOTA_devkit">DOTA_devkit</a> 来对数据集进行预处理，不妨根据 <code>README</code> 先下载安装该工具。然后我们可以使用 <a href="https://github.com/CAPTAIN-WHU/DOTA_devkit/blob/master/DOTA.py"><code>DOTA.py</code></a> 来加载指定的图片并显示目标的 obb 框。如果你还想将数据格式转换成 COCO 格式，那么就可以使用 <a href="https://github.com/CAPTAIN-WHU/DOTA_devkit/blob/master/DOTA2COCO.py">DOTA2COCO.py</a>。</p>
<h3 id="分割数据集"><a href="#分割数据集" class="headerlink" title="分割数据集"></a>分割数据集</h3><p>数据集里的图片分辨率都很高（最高达到了 <code>20000x20000</code>），显然我们的 GPU 不能满足这样的运算要求。如果我们 resize 图片，则会损失图片的信息，尤其是那种大量的小目标（低于 <code>20x20</code>）可能会直接消失。因此我们可以考虑使用 <a href="https://github.com/CAPTAIN-WHU/DOTA_devkit/blob/master/ImgSplit.py#L241"><code>ImgSplit.py</code></a> 将单张遥感图像裁剪切割成多张图片：</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/修改-YOLOv5-源码在-DOTAv1.5-遥感数据集上进行旋转目标检测/P0770_split.jpg">
</p>

<p>由于切割图片时会有重叠区域（gap），一般 gap 设为切割图像尺寸的 20% 为宜。分割后会将裁剪的位置信息保留在裁剪后的图片名称中，例如图片 <code>P0770__1__586___334.png</code> 是从原图 <code>P0770.png</code> 中 <code>x=586, y=334</code> 的位置处开始裁剪。</p>
<h3 id="最小外接矩形"><a href="#最小外接矩形" class="headerlink" title="最小外接矩形"></a>最小外接矩形</h3><p>DOTA 图片里的标注框为任意四边形，而我们需要的是带旋转角度的标准矩形，这就要用到 OpenCV 的 <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_contours/py_contour_features/py_contour_features.html#b-rotated-rectangle">cv2.minAreaRect()</a> 来求任意四边形的最小外接矩形，示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">image = cv2.imread(<span class="string">&quot;demo.png&quot;</span>)</span><br><span class="line">poly  = [[<span class="number">462.0</span>, <span class="number">785.0</span>],        <span class="comment"># 任意四边形的顶点坐标</span></span><br><span class="line">         [<span class="number">630.0</span>, <span class="number">787.0</span>],</span><br><span class="line">         [<span class="number">623.0</span>, <span class="number">952.0</span>],</span><br><span class="line">         [<span class="number">467.0</span>, <span class="number">953.0</span>]]</span><br><span class="line">poly  = np.array(poly, dtype=np.float32)</span><br><span class="line"></span><br><span class="line">rect  = cv2.minAreaRect(poly)</span><br><span class="line"><span class="comment"># 返回中心坐标 x, y 和长宽 w, h 以及与 x 轴的夹角 Q</span></span><br><span class="line"></span><br><span class="line">box   = cv2.boxPoints(rect).astype(np.<span class="built_in">int</span>)</span><br><span class="line"><span class="comment"># 返回四个顶点的坐标 (x1, y1), (x2, y2), (x3, y3) 和 (x4, y4)</span></span><br><span class="line"></span><br><span class="line">image = cv2.drawContours(image, [box], <span class="number">0</span>, (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/修改-YOLOv5-源码在-DOTAv1.5-遥感数据集上进行旋转目标检测/boundingrect.png">
</p>

<blockquote>
<p>绿色框为任意四边形，红色框是它的最小外接矩形。</p>
</blockquote>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>rotated object detection</tag>
      </tags>
  </entry>
  <entry>
    <title>可变形卷积网络：计算机新“视”界</title>
    <url>/2019/11/18/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C-%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%96%B0%E8%A7%86%E7%95%8C/</url>
    <content><![CDATA[<p>2017年，微软亚洲研究院视觉计算组的研究员在 arXiv 上公布了一篇题为 <a href="https://arxiv.org/pdf/1703.06211.pdf">“Deformable Convolutional Networks”（可变形卷积网络）</a> 的论文，首次在卷积神经网络（convolutional neutral networks，CNN）中引入了学习空间几何形变的能力，得到可变形卷积网络（deformable convolutional networks），从而更好地解决了具有空间形变的图像识别任务。</p>
<p align="center">
    <img width="55%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/可变形卷积网络-计算机新视界/002.png">
</p>

<p>研究员们通过大量的实验结果验证了该方法在复杂的计算机视觉任务（如目标检测和语义分割）上的有效性，首次表明在深度卷积神经网络（deep CNN）中学习空间上密集的几何形变是可行的。</p>
<span id="more"></span>

<h2 id="卷积神经网络的成功和局限"><a href="#卷积神经网络的成功和局限" class="headerlink" title="卷积神经网络的成功和局限"></a>卷积神经网络的成功和局限</h2><p>由于同样的物体在图像中可能呈现出不同的大小、姿态、视角变化甚至非刚体形变，因此如何适应这些复杂的几何形变是物体识别的主要难点，同时也是计算机视觉领域多年来关注的核心问题。很多传统经典方法，如尺度不变的特征变换（scale invariant feature transform, or SIFT）和可变形部件模型（deformable part models）等，都旨在解决这一问题。然而，由于人工设计特征的局限性，传统视觉方法在物体识别问题上多年来并未取得突破性的进展。</p>
<p>由于强大的建模能力和自动的端到端的学习方式，深度卷积神经网络可以从大量数据中学习到有效特征，避免了传统方法人工设计特征的弊端。然而，现有的网络模型对于物体几何形变的适应能力几乎完全来自于数据本身所具有的多样性，其模型内部并不具有适应几何形变的机制。究其根本，是因为卷积操作本身具有固定的几何结构，而由其层叠搭建而成的卷积网络的几何结构也是固定的，所以不具有对于几何形变建模的能力。</p>
<p>举个例子，想要识别出同一图像中不同大小的物体（比如远近不同的两个人），理想的结果是，在对应于每个物体的位置网络需要具有相应大小的感受野（receptive field）。直观的说，为了识别更大的物体网络需要看到更大的图像区域。然而，在现有的卷积网络架构中，图像中任何位置的感受野大小都是相同的，其取决于事先设定的网络参数（卷积核的大小、步长和网络深度等），无法根据图像内容自适应调整，从而限制了识别精度。</p>
<h2 id="消除网络难以适应几何变形的“罪魁祸首”"><a href="#消除网络难以适应几何变形的“罪魁祸首”" class="headerlink" title="消除网络难以适应几何变形的“罪魁祸首”"></a>消除网络难以适应几何变形的“罪魁祸首”</h2><p>追根溯源，上述局限来自于卷积网络的基本构成单元，即卷积操作。该操作在输入图像的每个位置时会进行基于规则格点位置的采样，然后对于采样到的图像值做卷积并作为该位置的输出。通过端到端的梯度反向传播学习，系统将会得到一个用矩阵表示的卷积核的权重。这就是自卷积网络诞生之初，已使用二十多年的基本单元结构。</p>
<p>微软亚洲研究院的研究员们发现，标准卷积中的规则格点采样是导致网络难以适应几何形变的“罪魁祸首”。为了削弱这个限制，研究员们对卷积核中每个采样点的位置都增加了一个偏移的变量。通过这些变量，卷积核就可以在当前位置附近随意的采样，而不再局限于之前的规则格点。这样扩展后的卷积操作被称为可变形卷积（deformable convolution）。标准卷积和可变形卷积在图1中有简要的展示。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/可变形卷积网络-计算机新视界/001.jpg">
</p>

<blockquote>
<p>图1：展示了卷积核大小为 3x3 的正常卷积和可变形卷积的采样方式，(a) 所示的正常卷积规律的采样 9 个点（绿点），(b)(c)(d) 为可变形卷积，在正常的采样坐标上加上一个位移量（蓝色箭头），其中(c)(d) 作为 (b) 的特殊情况，展示了可变形卷积可以作为尺度变换，比例变换和旋转变换的特殊情况</p>
</blockquote>
<p>事实上，可变形卷积单元中增加的偏移量是网络结构的一部分，通过另外一个平行的标准卷积单元计算得到，进而也可以通过梯度反向传播进行端到端的学习。加上该偏移量的学习之后，可变形卷积核的大小和位置可以根据当前需要识别的图像内容进行动态调整，其直观效果就是不同位置的卷积核采样点位置会根据图像内容发生自适应的变化，从而适应不同物体的形状、大小等几何形变，如图2、图3中所展示。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/可变形卷积网络-计算机新视界/003.png">
</p>

<blockquote>
<p>图2：两层3*3的标准卷积和可变形卷积的区别。(a) 标准卷积中固定的感受野和卷积核采样点。(b) 可变性卷积中自适应的感受野和卷积核采样点。</p>
</blockquote>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/可变形卷积网络-计算机新视界/004.jpg">
</p>

<blockquote>
<p>图 3：更多可变形卷积的示例。每个图像三元组显示了三层 3x3 可变形卷积核的采样点位置（共 729 个点），对应于三个不同的图像区域（从左至右，背景，小物体，大物体）。</p>
</blockquote>
<h2 id="可变形卷积神经网络：简明深刻的网络结构革新"><a href="#可变形卷积神经网络：简明深刻的网络结构革新" class="headerlink" title="可变形卷积神经网络：简明深刻的网络结构革新"></a>可变形卷积神经网络：简明深刻的网络结构革新</h2><p>可变形卷积单元具有诸多良好的性质。它不需要任何额外的监督信号，可以直接通过目标任务学习得到。它可以方便地取代任何已有视觉识别任务的卷积神经网络中的若干个标准卷积单元，并通过标准的反向传播进行端到端的训练。由此得到的网络则称为“可变形卷积网络”。</p>
<p>可变形卷积网络是对于传统卷积网络简明而又意义深远的结构革新，具有重要的学术和实践意义。它适用于所有待识别目标具有一定几何形变的任务（几乎所有重要的视觉识别任务都有此特点，人脸、行人、车辆、文字、动物等），可以直接由已有网络结构扩充而来，无需重新预训练。它仅增加了很少的模型复杂度和计算量，且显著提高了识别精度。例如，在用于自动驾驶的图像语义分割数据集（CityScapes）上，可变形卷积神经网络将准确率由70%提高到了75%。</p>
<p>此外，通过增加偏移量来学习几何形变的思想还可方便地扩展到其它计算单元中去。例如，目前业界最好的物体检测方法都使用了基于规则块采样的兴趣区域（region of interests, ROI）池化（pooling）。在该操作中，对于每个采样的规则块增加类似的偏移量，从而得到可变形兴趣区域池化 (deformable ROI pooling）。由此所获得的新的物体检测方法也取得了显著的性能提升。</p>
<h2 id="卷积网络的新思路"><a href="#卷积网络的新思路" class="headerlink" title="卷积网络的新思路"></a>卷积网络的新思路</h2><p>近年来，与神经网络结构相关的研究工作层出不穷，大多是对于各种基本网络单元连接关系的研究。不同于大部分已有的工作，可变形卷积网络首次表明了可以在卷积网络中显式地学习几何形变。它修改了已使用二十余年的基本卷积单元结构，在重要的物体检测和语义分割等计算机视觉任务上获得了重大的性能提升。</p>
<p>可以想象，在不远的未来，在更多的计算机视觉识别任务中（如文字检测、视频物体检测跟踪等）都将看到它的成功应用。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>可变形卷积网络</tag>
      </tags>
  </entry>
  <entry>
    <title>批量归一化层（Batch Normalization)</title>
    <url>/2019/07/09/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E5%B1%82%EF%BC%88Batch-Normalization/</url>
    <content><![CDATA[<p>通常来说，数据标准化预处理对于浅层模型就足够有效了。<font color=red>但随着模型训练的进行，当每层中参数更新时，靠近输出层的输出容易出现剧烈变化。这令我们难以训练出有效的深度模型，而批量归一化（batch normalization）的提出正是为了应对这种挑战。</font></p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/批量归一化层（Batch Normalization)/Internal_Covariate_Shift.jpg"></p>

<h2 id="BN-来源"><a href="#BN-来源" class="headerlink" title="BN 来源"></a>BN 来源</h2><p>在机器学习领域中，满足一个很重要的假设，即独立同分布的假设：就是假设训练数据和测试数据是满足相同分布的，这样通过训练数据获得的模型就能够在测试集获得一个较好的效果。而在实际的神经网络模型训练中，隐层的每一层数据分布老是变来变去的，这就是所谓的 <font color=red><strong>“Internal Covariate Shift”</strong></font>。</p>
<span id="more"></span>

<p>在这种背景下，然后就提出了 BatchNorm 的基本思想：<strong>能不能让每个隐层节点的<font color=red>激活输入分布</font>固定下来呢？</strong></p>
<p>BN不是凭空拍脑袋拍出来的好点子，它是有启发来源的：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话 —— <font color=red><strong>所谓白化，就是对输入数据分布变换到 0 均值，单位方差的正态分布</strong></font> —— 因此 BN 作者推断，如果对神经网络的每一层输出做白化操作的话，模型应该也会较快收敛。</p>
<h2 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h2><p>首先对小批量的样本数据求均值和方差：</p>
<p align="center">
    <img width="35%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/批量归一化层（Batch Normalization)/MommyTalk1600748016395.jpg"></p>


<p>接下来，使用按元素开方和按元素除法对样本数据进行标准化:</p>
<p align="center">
    <img width="20%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/批量归一化层（Batch Normalization)/MommyTalk1600748133659.jpg"></p>

<p align="center">
    <img width="20%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/批量归一化层（Batch Normalization)/MommyTalk1600748180578.jpg"></p>

<p>这里 ε &gt; 0是一个很小的常数，保证分母大于 0。在上面标准化的基础上，批量归一化层引入了<strong>两个需要学习的参数：拉伸(scale)参数 γ 和偏移(shift)参数 β。</strong><font color=red>这两个参数会把标准正态分布左移或者右移一点并长胖一点或者变瘦一点，从而使得网络每一层的数据分布保持相似。</font></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, gluon, init, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span></span><br><span class="line">    <span class="comment"># 通过autograd来判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> autograd.is_training():</span><br><span class="line">        <span class="comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / nd.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上(axis=1)的均值和方差。这里我们需要保持</span></span><br><span class="line">            <span class="comment"># X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / nd.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line"></span><br><span class="line">    Y = gamma * X_hat + beta <span class="comment"># 拉伸和偏移</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean, moving_var</span><br></pre></td></tr></table></figure>

<h2 id="BN-位置"><a href="#BN-位置" class="headerlink" title="BN 位置"></a>BN 位置</h2><p>在 <a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> 一文中，作者指出，“we would like to ensure that for any parameter values, the network always produces activations with the desired distribution”（produces activations with the desired distribution，<font color=red><strong>为激活层提供期望的分布</strong></font>）。</p>
<table><tr><td bgcolor= LightSalmon><strong>因此 `Batch Normalization` 层恰恰插入在 conv 层或全连接层之后，而在 relu 等激活层之前。</strong></td></tr></table>

<h2 id="BN-优点"><a href="#BN-优点" class="headerlink" title="BN 优点"></a>BN 优点</h2><ul>
<li><strong><font color=red>解决了 Internal Covariate Shift 的问题</font></strong>：模型训练会更加稳定，学习率也可以设大一点，同时也减少了对权重参数初始化的依赖；</li>
<li><strong><font color=red>对防止 gradient vanish 有帮助</font></strong>：一旦有了 Batch Normalization，激活函数的 input 都在零附近，都是斜率比较大的地方，能有效减少梯度消失；</li>
<li><strong><font color=red>能有效减少过拟合</font></strong>：据我所知，自从有了 Batch Normaliztion 后，就没有人用 Dropout 了。直观的理解是：对网络的每一层 layer 做了 BN 处理来强制它们的数据分布相似，这相当于对每一层的输入做了约束（regularization）。</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] 李沐，动手深度学习. 2019.9.12</li>
<li>[2] <a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Batch Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title>精确率、召回率和 ROC 曲线</title>
    <url>/2017/03/11/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8C-ROC-%E6%9B%B2%E7%BA%BF/</url>
    <content><![CDATA[<p>机器学习领域里评估指标这么多，今天就简单回顾下常用的准确率(accuracy)，精确率(precision) 和召回率(recall)等概念吧！哎，有时候时间太久了就会记不太清楚了。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/精确率、召回率和-ROC-曲线/ml.png">
</p>

<span id="more"></span>

<p>首先来看看混淆矩阵：</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/精确率、召回率和-ROC-曲线/matrix.jpg">
</p>


<p><font color=blue>精确率(precision)</font>是针对<font color=red>预测结果</font>而言的，它表示的是<font color=red>预测为正的样本中有多少是对的</font>。那么预测为正就有两种可能：一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP)。</p>
<p align="center">
    <img width="18%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/精确率、召回率和-ROC-曲线/MommyTalk1600747363006.jpg">
</p>

<p><font color=blue>召回率(recall)</font>是针对<font color=red>原来样本</font>而言的，它表示的是<font color=red>样本中的正例有多少被预测正确了</font>。那也有两种可能：一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)。</p>
<p align="center">
    <img width="18%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/精确率、召回率和-ROC-曲线/MommyTalk1600747460800.jpg">
</p>


<p>一般来说，精确率和召回率是一对矛盾的度量。精确率高的时候，往往召回率就低；而召回率高的时候，精确率就会下降。当我们不断地调整评判阈值时，就能获得一条 <font color=red><strong>P-R 曲线</strong></font>。</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/精确率、召回率和-ROC-曲线/PR_curve.jpg">
</p>

<p>但是在人脸识别领域里，关注更多的还是 <strong><font color=red>ROC 曲线</font></strong>。例如，某某公司经常说自己的人脸识别算法可以做到在千万分之一误报率下其准确率超过 99%，这说的其实就是 ROC 曲线。要知道这个概念，就必须了解 <font color=blue>TPR（True Positive Rate）</font>和 <font color=blue>FPR（False Positive Rate）</font>的概念。</p>
<p><font color=red>TPR: 原来是对的，预测为对的比例。</font>（当然越大越好，1 为理想状态）</p>
<p align="center">
    <img width="20%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/精确率、召回率和-ROC-曲线/MommyTalk1600747670288.jpg">
</p>


<p><font color=red>FPR: 原来是错的，预测为对的比例，误报率。</font>（当然越小越好，0 为理想状态）</p>
<p align="center">
    <img width="20%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/精确率、召回率和-ROC-曲线/MommyTalk1600747712808.jpg">
</p>

<p>同样不断地调整评判阈值，就能获得一条 ROC 曲线。其中，ROC曲线下与坐标轴围成的面积称为 <font color=red><strong>AUC</strong></font>（AUC 的值越接近于 1，说明该模型就越好）。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/精确率、召回率和-ROC-曲线/ROC.jpg">
</p>

<p>最后，必须要注意的是<font color=blue>准确率(accuracy)</font>和<font color=blue>精确率(precision)</font>是不一样的: </p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/精确率、召回率和-ROC-曲线/MommyTalk1600747502302.jpg">
</p>


<p>参考文献:</p>
<ul>
<li>[1] 张志华，《机器学习》. 清华大学出版社，2016.</li>
<li>[2] <a href="http://www.bio1000.com/news/201811/203913.html">在人脸识别领域业界通常以误报率漏报率作为衡量算法能力的主要指标.</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>精确率、召回率和ROC曲线</tag>
      </tags>
  </entry>
  <entry>
    <title>2D人体姿态估计的总结和梳理</title>
    <url>/2020/10/10/2D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E8%BF%87%E5%8E%BB%EF%BC%8C%E7%8E%B0%E5%9C%A8%E5%92%8C%E6%9C%AA%E6%9D%A5/</url>
    <content><![CDATA[<p>最近在虎牙直播做了一些关于人体姿态估计的工作，也看了蛮多这方面的文章，于是对这个内容做了一些总结和梳理，希望能抛砖引玉吧。我们不妨先把问题抛出来，人体姿态估计是做什么？从名字的角度来看，可以理解为对“人体”的姿态（关键点，比如头，左手，右脚等）的位置估计。根据 RGB 图片里人体的数量，又可以分为<strong>单人姿态估计</strong>和<strong>多人姿态估计</strong>。</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/001.jpg">
</p>

<span id="more"></span>

<h2 id="单人姿态估计"><a href="#单人姿态估计" class="headerlink" title="单人姿态估计"></a>单人姿态估计</h2><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>对于单人姿态估计，输入是一个 crop 出来的行人，然后在行人区域位置内找出需要的关键点，比如头部，左手，右膝等。对于<strong>多人姿态估计</strong>，目前主要有 2 种主流思路，分别是 <strong>top-down</strong> 以及 <strong>bottom-up</strong> 方法。对于 top-down 方法，往往先找到图片中所有行人，然后对每个行人做姿态估计，寻找每个人的关键点。单人姿态估计往往可以被直接用于这个场景。对于 bottom-up，思路正好相反，先是找图片中所有 parts （关键点），比如所有头部，左手，膝盖等，然后把这些 parts（关键点）组装成一个个行人。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul>
<li><a href="https://sam.johnson.io/research/lsp.html">LSP（Leeds Sports Pose Dataset）</a>：单人人体关键点检测数据集，关键点个数为14，样本数2K，在目前的研究中作为第二数据集使用。</li>
<li><a href="https://bensapp.github.io/flic-dataset.html">FLIC（Frames Labeled In Cinema）</a>：单人人体关键点检测数据集，关键点个数为9，样本数2W，在目前的研究中作为第二数据集使用。</li>
<li><a href="http://human-pose.mpi-inf.mpg.de/">MPII（MPII Human Pose Dataset）</a>：单人/多人人体关键点检测数据集，关键点个数为16，样本数25K，是单人人体关键点检测的主要数据集。它是 2014 年由马普所创建的，目前可以认为是单人姿态估计中最常用的 benchmark， 使用的是 <a href="http://human-pose.mpi-inf.mpg.de/#results">PCKh</a> 的指标。</li>
<li><a href="https://cocodataset.org/#home">MSCOCO</a>：多人人体关键点检测数据集，关键点个数为17，样本数量多于30W。目前是多人关键点检测的主要数据集，使用的是 <a href="https://cocodataset.org/#keypoints-eval">AP 和 OKS</a> 指标。</li>
<li><a href="http://vision.imar.ro/human3.6m/description.php">human3.6M</a>：是 3D 人体姿势估计的最大数据集，由 360 万个姿势和相应的视频帧组成，这些视频帧包含11 位演员从4个摄像机视角执行 15 项日常活动的过程。数据集庞大将近100G。</li>
<li><a href="https://posetrack.net/">PoseTrack</a>：最新的关于人体骨骼关键点的数据集，多人人体关键点跟踪数据集，包含单帧关键点检测、多帧关键点检测、多人关键点跟踪三个人物，多于500个视频序列，帧数超过20K，关键点个数为15。</li>
</ul>
<h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p><code>Keypoint detection</code> 度量方法的核心思想就是模仿 <code>Object detection</code> 的度量方法。在 <code>Object detection</code> 中，<code>IoU</code> 是作为一种相似度度量，它通过计算检测框之间的重合度来定义真实目标和预测值目标之间的距离。而在 <code>Keypoint detection</code> 则是根据<font color=Red>预测关键点和真实关键点之间的欧式距离来评判的</font>。</p>
<ul>
<li>PCK(Percentage of Correct Keypoints)</li>
</ul>
<p><font color=Red>关键点正确估计的百分比，计算检测的关键点与其对应的 groundtruth 间的归一化距离小于设定阈值的比例</font>。这是比较老的人体姿态估计指标，在17年以前广泛使用，现在基本不再使用。但是如果仅是用于工程项目中来评价训练模型的好坏还是蛮方便的，因此这里也记录下：</p>
<p align="center">
    <img width="25%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/003.png">
</p>

<p>其中： <code>i</code> 表示关节点的编号， <code>di</code> 表示第i个关节点的预测值和 groundtruth 的欧氏距离。 <code>d</code> 是一个人体的尺度因子，在不同的数据集里的计算方法不同。例如 MPII 中 数据集是以头部长度作为归一化参考，即 <strong>PCKh</strong>。</p>
<ul>
<li>oks (object keypoint similarity)</li>
</ul>
<p><font color=Red>oks 表示关键点的相似度，它和预测关键点和真实关键点之间的欧式距离有关，其范围值为 [0, 1]。当它们的欧式距离为 0 时，其 oks 相似度等于 1</font></p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/002.png"></p>
 
 其中：`i` 为关键点的编号，`di` 表示预测的关键点和 groundtruth 之间的欧式距离，`S` 是一个尺度因子，为人体检测框面积的平方根， `oi` 是一个归一化因子，和关键点标注的难易有关，是通过对所有样本的人工标注和真实值的统计标准差。 `vi` 表示当前关键点是否可见。

<ul>
<li>AP(Average Precision)</li>
</ul>
<p><font color=Red>这个和目标检测里的 <code>AP</code> 概念是一样的，只不过度量方式 <code>iou</code> 替换成了 <code>oks</code>。如果 oks 大于阈值 T，则认为该关键点被成功检测到</font>。单人姿态估计和多人姿态估计的计算方式不同。对于单人姿态估计的AP，目标图片中只有一个人体，所以计算方式为：</p>
<p align="center">
    <img width="27%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/004.png"></p>

<p>对于多人姿态估计而言，由于一张图像中有 <code>M</code> 个目标，假设总共预测出 <code>N</code> 个人体，那么groundtruth和预测值之间能构成一个 <code>MxN</code> 的矩阵，然后将每一行的最大值作为该目标的 oks，则：</p>
<p align="center">
    <img width="35%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/005.png"></p>

<h2 id="过去和现在"><a href="#过去和现在" class="headerlink" title="过去和现在"></a>过去和现在</h2><p>在过去，传统方法一般是基于图结构和形变部件模型，设计 2D 人体部件检测器，使用图模型建立各部件的连通性，然后结合关键点之间的 pair-wise 关系来恢复人体的姿态。传统方法虽然拥有较高的时间效率，但是提取的都是人工设定的浅层特征（如 HOG，SIFT 等）。而在现在的深度学习时代，姿态估计的特征提取，分类，以及空间位置的建模都可以在一个网络中直接建模，不再需要独立的进行拆解，整个过程完全可以端到端进行。</p>
<h3 id="过去的思路"><a href="#过去的思路" class="headerlink" title="过去的思路"></a>过去的思路</h3><p>在 2014 年以前，很多论文讲的都是利用传统方法去解决单人姿态估计的问题。这些工作都是在深度学习爆发之前，我们是如何处理人体姿态估计这个问题。从算法角度来讲，这部分的工作主要是希望解决单人的人体姿态估计问题，也有部分工作已经开始尝试做 3D 的人体姿态估计。可以粗略的方法分成两类。</p>
<p>第一类是直接通过一个全局 feature，把姿态估计问题当成分类或者回归问题直接求解。但是这类方法的问题在于精度一般，并且可能比较适用于背景干净的场景。第二类是基于一个图模型，比如常用pictorial structure model。一般包含 unary term, 是指对单个 part 进行 feature 的表示，单个 part 的位置往往可以使用DPM (Deformable Part-based model) 来获得。同时需要考虑 pair-wise 关系来优化关键点之间的关联。</p>
<p>总结一下，在传统方法里面，需要关注的两个维度是： feature representation 以及关键点的空间位置关系。特征维度来讲，传统方法一般使用的 HOG, Shape Context, SIFT 等浅层特征。 空间位置关系的表示也有很多形式，上面的 Pictorial structure model 可能只是一种。</p>
<h3 id="现在的方法"><a href="#现在的方法" class="headerlink" title="现在的方法"></a>现在的方法</h3><p>自从 2012 年 AlexNet 开始，深度学习开始快速发展，从最早的图片分类问题，到后来的检测，分割问题。<font color=Red>第一次有人在 2014 年成功使用 CNN 来解决单人的姿态估计问题</font>。受限于当时的时代背景，整体网络结构比较简单并且同时也沿用了传统思路：首先是通过滑动窗口的方式，来对每个 patch 进行分类，找到相应的人体关键点。因为直接使用滑动窗口缺少了很多 context 信息，所以会有很多 FP 的出现。所以在 pipeline 中加了一个后处理的步骤，希望能抑制部分FP，具体的实现方式是类似一个空间位置的模型。从这个工作来看，有一定的传统姿态估计方法的惯性，改进的地方是把原来的传统的特征表示改成了深度学习的网络，同时把空间位置关系当成是后处理来做处理。总体性能在当时已经差不多跑过了传统的姿态估计方法。</p>
<p><font color=Red>2014 年的另外一个重要的进展是引入了 <code>MPII</code> 的数据集</font>。此前的大部分 paper 都是基于 FLIC 以及 LSP 来做评估的，但是在深度学习时代，数据量还是相对偏少（K 级别）。MPII 把数据量级提升到 W 级别，同时因为数据是互联网采集，同时是针对 activity 来做筛选的，所以无论从难度还是多样性角度来讲，都比原来的数据集有比较好的提升。</p>
<p>现在几乎所有的主流方法都是围绕着下面两大块进行的：改善监督信息和网络结构。</p>
<h2 id="改善监督信息"><a href="#改善监督信息" class="headerlink" title="改善监督信息"></a>改善监督信息</h2><p>根据真值标签 (Ground Truth) 的类型和监督训练的方式，单人姿态估计主要有 2 种思路：基于坐标回归和基于热图检测，它们各有优缺点。</p>
<h3 id="基于坐标回归（代表作：Deep-Pose"><a href="#基于坐标回归（代表作：Deep-Pose" class="headerlink" title="基于坐标回归（代表作：Deep Pose)"></a>基于坐标回归（代表作：Deep Pose)</h3><p>基于坐标回归的算法是将关节点的二维坐标 (<code>coordinate</code>) 作为 Ground Truth，训练网络直接回归得到每个关节点的坐标，这样的好处在于可以得到 sub-pixel 级别的精度，而且速度会比较快（不需要后处理）。例如在一些人脸关键点检测算法（如 mtcnn）中，会在最后一层使用全连接层直接回归出 landmark 的坐标。</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/021.png">
</p>

<p>在人体姿态估计领域中，这类方法的代表作是谷歌大佬提出的 <a href="https://github.com/mitmul/deeppose"><code>Deep Pose</code></a> 网络。</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/006.jpg"></p>

<blockquote>
<p>在上图中：<font color=Red>蓝色部分是卷积层，绿色部分是全连接层</font>。Deep Pose 算法主要是通过训练一个级联的姿态回归器。在第一个阶段先粗略地估计出部分的姿态轮廓，然后在下个阶段将通过已知关键点的位置不断去优化其他关键点的位置。每个 stage 都使用已经预测的关键点来 crop 出基于这个关键点的邻域子图像并用于后续的网络输入，从而得到一个更加精准的位置信息。</p>
</blockquote>
<p>但是值得注意的是：一方面，全连接层（<code>fully connected layer</code>）容易抹掉人体关键点在图像中的空间位置信息和 context 信息。如果缺乏 context 信息，会使得模型很难区分左右手，导致 FP 容易高。另一方面，人体姿态估计这个问题本身的自由度很大。直接 regression 的方式对自由度小的问题比如人脸 landmark 可能是比较适合的，但是这会对自由度较大的姿态估计问题整体的建模能力会比较弱。</p>
<blockquote>
<p>人体姿态估计的自由度较大的地方在于：例如，人的左手腕关节点既可以出现在人体的左边，也可以出现在人体的右边，因此 context 和空间位置信息就显得很重要。</p>
</blockquote>
<h3 id="基于热图检测（代表作：Simple-Baseline）"><a href="#基于热图检测（代表作：Simple-Baseline）" class="headerlink" title="基于热图检测（代表作：Simple Baseline）"></a>基于热图检测（代表作：Simple Baseline）</h3><p><code>Heatmap</code> 将每个骨骼节点的坐标分别都用一个概率图来表示，这个概率图的分辨率往往是原图的等比例缩放图（一般为 64x48），channel 的个数等于关键节点的数目。</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/011.jpg"></p>
    
热图中的每个像素位置都会给出一个概率，表示该点属于对应类别关键点的概率，比较自然的是，距离关键点位置越近的像素点的概率越接近 1，而距离关键点越远的像素点的概率越接近 0，具体可以通过相应函数进行模拟，如 Gaussian 函数等。

<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/007.jpg"></p>

<p>微软亚洲研究院的 xiao bin 于 2018 年提出了一种基于热图检测的极其简单的单人姿态估计网络。由于它简单有效，所以作者称之为 <a href="https://github.com/microsoft/human-pose-estimation.pytorch">simple baseline</a>。该网络只是在 ResNet 后面添加一些反卷积层，甚至都没有 skip connection，这种结构可以说是从 deep 和 low 分辨率特征生成热图的最简单也是最熟悉的方法。</p>
<p align="center">
    <img width="38%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/008.jpg"></p>
    
最后加入 1×1 的卷积层，生成 k 个关键点的预测热图，与 ground truth 热图计算 MSE 损失。

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JointsMSELoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, use_target_weight</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(JointsMSELoss, self).__init__()</span><br><span class="line">        self.criterion = nn.MSELoss(size_average=<span class="literal">True</span>)</span><br><span class="line">        self.use_target_weight = use_target_weight</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, output, target, target_weight</span>):</span></span><br><span class="line">        batch_size = output.size(<span class="number">0</span>)</span><br><span class="line">        num_joints = output.size(<span class="number">1</span>)</span><br><span class="line">        heatmaps_pred = output.reshape((batch_size, num_joints, -<span class="number">1</span>)).split(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        heatmaps_gt = target.reshape((batch_size, num_joints, -<span class="number">1</span>)).split(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(num_joints):</span><br><span class="line">            heatmap_pred = heatmaps_pred[idx].squeeze()</span><br><span class="line">            heatmap_gt = heatmaps_gt[idx].squeeze()</span><br><span class="line">            <span class="keyword">if</span> self.use_target_weight:</span><br><span class="line">                loss += <span class="number">0.5</span> * self.criterion(</span><br><span class="line">                    heatmap_pred.mul(target_weight[:, idx]),</span><br><span class="line">                    heatmap_gt.mul(target_weight[:, idx])</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                loss += <span class="number">0.5</span> * self.criterion(heatmap_pred, heatmap_gt)</span><br><span class="line">        <span class="keyword">return</span> loss / num_joints</span><br></pre></td></tr></table></figure>
<h2 id="改善网络结构"><a href="#改善网络结构" class="headerlink" title="改善网络结构"></a>改善网络结构</h2><p>后续 2D 人体姿态估计方法几乎都是围绕 heatmap 这种形式来做的：通过使用神经网络来获得更好的特征表示，同时把关键点的空间位置关系隐式地 encode 在 heatmap 中进行监督学习。随着基于热图检测的方法成为标准，往后越来越多的工作聚焦在了网络结构设计上。</p>
<h3 id="CPM（Convolutional-Pose-Machines）"><a href="#CPM（Convolutional-Pose-Machines）" class="headerlink" title="CPM（Convolutional Pose Machines）"></a>CPM（Convolutional Pose Machines）</h3><p>经典的卷积姿态机（Convolutional Pose Machines，CPM）是CMU Yaser Sheikh组的工作，后续非常有名的 openpose 也是他们的工作。CPM 在初始阶段 (stage 1) 只对输入图片进行卷积，输出所有关节点的 heatmap。在后面的每个阶段 中，CPM 首先设计了一个特征提取器 (FeatureExtractor) 将上一个阶段输出的 heatmap 和原始图像的 feature map 级联起来，然后将这种预处理过的融合 feature map 输入 本阶段的 FCN 进行处理，最终得到新的关节点 heatmap。 </p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/012.jpg"></p>

<blockquote>
<p>CPM 由多个 stage 网络级联而成，每个 stage 设计一个小型网络，用于提取 feature。中间层的信息可以给后续层提供 context，后续 stage 可以认为是基于前面的 stage 去做 refinement。这个 refinement 过程需要感受野提供更多的 context 信息，因此感受野会越来越大。</p>
</blockquote>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/013.jpg"></p>

<p>但是通过不断增加卷积层来改变感受野会给网络产生较大的训练负担，造成梯度消失等问题。为避免加大感受野带来的副作用，CPM 采用中继监督训练，将各个阶段产生的 heatmap 与 Ground Truth 产生的误差累加起来作为总误差进行迭代，同时将梯度从各个阶段网络的输 出层反向传播，避免梯度消失，最后得到各个阶段修正后的响应 feature map，即置信图 (belief map)。CPM 在测试时以最后一个阶段的响应图输出为准。</p>
<h3 id="Stacked-Hourglass-Network"><a href="#Stacked-Hourglass-Network" class="headerlink" title="Stacked Hourglass Network"></a>Stacked Hourglass Network</h3><p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/009.png"></p>

<p>在 2016 年的 7 月份，Princeton 的 Deng Jia 组放出了另外一个非常棒的人体姿态估计工作，堆叠式沙漏网络（Stacked Hourglass Network）。它最大的特点就是，网络既简单优美又准确高效。从上图可以看出，网络由很多重复堆叠的 u-shape 模块（如下图所示）所组成。并且在每个 u-shape 模块的旁路都添加了残差模块（Residual Module）分支，它在网络的深层梯度传导和防止过拟合方面起到关键性作用。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/010.png"></p>

<p>作者在整个网络结构中堆叠了许多 hourglass 模块，从而使得网络能够不断重复自底向上和自顶向下的过程。类似于 CPM 的多阶段学习过程，堆叠式沙漏结构 同样采用了“配套的”中继训练。但它是对每一个阶段得到的关节点 heatmap 立即根据 Ground Truth 进行重绘 (remap)，并得到当前的 belief map，而不是像 CPM 累 加所有阶段的总误差再进行迭代反馈。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/020.jpg"></p>

<blockquote>
<p>Illustration of the intermediate supervision process. The network splits and produces a set of heatmaps (outlined in blue) where a loss can be applied. A 1x1 convolution remaps the heatmaps to match the number of channels of the intermediate features. These are added together along with the features from the preceding hourglass.</p>
</blockquote>
<h3 id="HRNet（High-Resolution-Network）"><a href="#HRNet（High-Resolution-Network）" class="headerlink" title="HRNet（High-Resolution Network）"></a>HRNet（High-Resolution Network）</h3><p>从2012年以来随着 AlexNet 横空出世，神经网络在计算机视觉领域成为主流的方法。2014年谷歌发明出了 GoogleNet，牛津大学发明了 VGGNet，2015 年微软发明了 ResNet，2016 年康奈尔大学和清华大学发明了 DenseNet，以上都是围绕分类任务而发明的网络结构。这些网络结构的一个共同的特征便是：逐步减小空间的大小，最终得到一个低分辨率的表征。低分辨率的表征在图像分类任务中是足够的，因为在图像分类里面，只需要给一个全局的标签，而不需要详细的空间信息，我们称之为空间粗粒表征的学习。</p>
<p>但是在其它任务中，比如检测，我们需要知道检测框的空间位置，比如分割，我们需要每个像素的标签，在人脸和人体的关键点的检测中，我们需要关键点的空间位置，这样一系列的任务实际上需要空间精度比较高的表征，我们称之为高分辨率表征。</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/017.jpg"></p>

<p>到了 2019 年， MSRA 的 wang jingdong 组出了一个很好的工作，提出了 spatial resolution 的重要性。他认为以前网络的套路都是 feature map 的分辨率一开始虽然很高，但是会慢慢降低，然后又慢慢升高。在这个过程中，先失去了空间精度，然后慢慢恢复，最终学到的特征空间精度较弱。</p>
<p><font color=Red>因此作者提出了一种新的网络结构：分成多个层级，但是始终保留着最精细的 spaital 那一层的信息，通过 fuse 从高分辨率到低分辨率的子网络输出，来获得更多的 context 以及语义层面信息。</font>它能够在整个过程中保持高分辨率的表示。以高分辨率子网络开始作为第一阶段，逐步增加高分辨率到低分辨率的子网络。以此内推形成更多阶段，并将多分辨率子网络并行连接。在整个过程中，通过在并行的多分辨率子网络上反复交换信息来进行多尺度的重复融合。</p>
<p align="center">
    <img width="70%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/015.jpg"></p>

<blockquote>
<p>一直保持高分辨率的特征能够很好地保留空间位置信息，而逐步增加高分辨率到低分辨率的子网络是为了添加更多丰富的语义信息。<font color=Red>在低分辨率方面，它可以学习到很好的语义信息；在高分辨率里，它的空间精度非常强。在整个过程中，高中低分辨率不停地交互，使得高分辨率可以拿到低分辨率语义性比较强的表征，低分辨率可以拿到高分辨率的空间精度比较强的表征，不停地融合，最终取得更强的高分辨率表征。</font></p>
</blockquote>
<h2 id="多人姿态估计"><a href="#多人姿态估计" class="headerlink" title="多人姿态估计"></a>多人姿态估计</h2><p>通过单人体态估计的方法可以得到人体的2D关节点坐标，但是在一张多人图像中，模型在区别不同人体 的关节点，避免不同人体的关节点之间进行误连时，需 要额外的策略作为指导。为此，多人姿态估计的相关研究大致提供了两种思路: 自顶向下(Top-Down) 和自底向上(Bottom-Up)的。</p>
<h3 id="自底向上（Bottom-Up）"><a href="#自底向上（Bottom-Up）" class="headerlink" title="自底向上（Bottom-Up）"></a>自底向上（Bottom-Up）</h3><p>自底向上的思路为：首先用单人姿态估计的方法构建部件检测器将图片中所有的人体关节点全部检测出来，然后在第二个阶段对不同人体的关节点聚成一类并拼接在一起。这类的代表作为 Open Pose，它在 2016 年的 COCO 比赛中一举夺得第一名。 CMU 团队基于 CPM 为组件，先找到图片中每个 joint 的位置，然后提出部件亲和场（Part Affinity Field，PAF) 来做人体关键点的组装。</p>
<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/2D人体姿态估计的过去，现在和未来/016.jpg"></p>

<p>PAF 的基本原理是在两个相邻关键点之间，建立一个有向场，比如左手腕，左手肘。我们把 CPM 找到的所有的左手腕以及左手肘拿出来建立一个二分图，边权就是基于 PAF 场来计算的。然后进行匹配，匹配成功就认为是同一个人的关节。依次类别，对所有相邻点做此匹配操作，最后就得到每个人的所有关键点。</p>
<h3 id="自顶向下（Top-Down）"><a href="#自顶向下（Top-Down）" class="headerlink" title="自顶向下（Top-Down）"></a>自顶向下（Top-Down）</h3><p>虽然 2016 年 bottom-up 是一个丰富时间点，但是从 2017 年开始，越来的工作开始围绕top-down 展开，一个直接的原因是 top-down 的效果往往更有潜力。top-down 相比bottom-up 效果好的原因可以认为有两点。首先是人的 recall 往往更好。因为 top-down 是先做人体检测，人体往往会比 part 更大，所以从检测角度来讲会更简单，相应找到的recall 也会更高。其次是关键点的定位精度会更准，这部分原因是基于 crop 的框，对空间信息有一定的 align，同时因为在做单人姿态估计的时候，可以获得一些中间层的 context 信息，这对于点的定位是很有帮助的。当然，top-down 往往会被认为速度比 bottom-up 会更慢（特别是人数较多的场景）。所以在很多要求实时速度，特别是手机端上的很多算法都是基于 openpose 来做修改的。不过这个也要例外，我们自己也有做手机端上的多人姿态估计，但是我们是基于 top-down 来做的，主要原因是人体检测器可以做的非常快。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a href="https://jishuin.proginn.com/p/763bfbd299d2">[1] 姿态估计：人体骨骼关键点检测综述（2016-2020）</a></li>
<li><a href="https://blog.csdn.net/ZXF_1991/article/details/104279387">[2] 人体姿态估计－评价指标（一）</a></li>
<li><a href="https://arxiv.org/abs/2006.01423">[3] Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods</a></li>
<li><a href="https://arxiv.org/abs/1910.06278">[4] Distribution-Aware Coordinate Representation for Human Pose Estimation</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/85506259">[5] 人体姿态估计的过去，现在，未来</a></li>
<li><a href="https://arxiv.org/abs/1804.06208">[6] Simple Baselines for Human Pose Estimation and Tracking. ECCV 2018</a></li>
<li><a href="https://arxiv.org/pdf/1602.00134.pdf">[7] Convolutional Pose Machines. CVPR 2016</a></li>
<li><a href="https://arxiv.org/abs/1603.06937">[8] Stacked Hourglass Networks for Human Pose Estimation. ECCV 2016</a></li>
<li><a href="https://arxiv.org/abs/1902.09212">[9] Deep High-Resolution Representation Learning for Human Pose Estimation. CVPR 2019</a></li>
<li><a href="https://arxiv.org/abs/1812.08008">[10] OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. CVPR 2017</a></li>
<li><a href="https://www.msra.cn/zh-cn/news/features/a-universal-architecture-for-visual-recognition">[11] 王井东：下一代视觉识别的通用网络结构是什么样的？</a></li>
<li><a href="https://www.msra.cn/zh-cn/news/features/cvpr-2019-hrnet">[12] 告别低分辨率网络，微软提出高分辨率深度神经网络HRNet</a></li>
</ul>
]]></content>
      <categories>
        <category>姿态估计</category>
      </categories>
      <tags>
        <tag>hourglass 网络</tag>
      </tags>
  </entry>
  <entry>
    <title>用 Python 手撸一个单目视觉里程计的例子</title>
    <url>/2020/12/19/%E7%94%A8-Python-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%95%E7%9B%AE-Slam-%E4%BE%8B%E5%AD%90/</url>
    <content><![CDATA[<p>最近因工作需要，开始接触到一些关于 SLAM（Simultaneous Localization and Mapping）的研究。网上关于 slam 的资料有很多，譬如高博的十四讲，github 上的 VINS 等等。但是他们大多是用 C++ 写的，并且环境依赖复杂。今天， 我使用 Python 手撸了一个简单的单目 slam，对 slam 有了一个初步的认识。完整的代码在<a href="https://github.com/YunYang1994/openwork/tree/main/monocular_slam">这里</a>。</p>
<p align="center">
<iframe src="//player.bilibili.com/player.html?aid=245798532&bvid=BV1Tv411t7aN&cid=270731969&page=1"  width="600" height="400"  scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</p>

<span id="more"></span>

<h2 id="ORB-特征点检测"><a href="#ORB-特征点检测" class="headerlink" title="ORB 特征点检测"></a>ORB 特征点检测</h2><p>ORB 特征由<strong><font color=Red>关键点</font></strong>和<strong><font color=Red>描述子</font></strong>两部分组成，它的关键点称为 “Oriented FAST”，是一种改进的 FAST 角点，而描述子则称为 BRIEF。在 OpenCV 中，我们可以这样：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">orb = cv2.ORB_create()</span><br><span class="line">image = cv2.cvtColor(frame.image, cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment"># detection corners</span></span><br><span class="line">pts = cv2.goodFeaturesToTrack(image, <span class="number">3000</span>, qualityLevel=<span class="number">0.01</span>, minDistance=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># extract features</span></span><br><span class="line">kps = [cv2.KeyPoint(x=pt[<span class="number">0</span>][<span class="number">0</span>], y=pt[<span class="number">0</span>][<span class="number">1</span>], _size=<span class="number">20</span>) <span class="keyword">for</span> pt <span class="keyword">in</span> pts]</span><br><span class="line">kps, des = orb.compute(image, kps)</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/sample_01.gif">
</p>

<p>我们首先需要将图片转成灰度图， 然后利用 <code>goodFeaturesToTrack</code> 找出图片中的高质量的角点， 接着使用 <code>orb</code> 里的 <code>compuete</code> 函数计算出这些角点的特征：它会返回 <code>kps</code> 和 <code>des</code>，<code>kps</code> 给出了角点在图像中坐标，而 <code>des</code> 则是这些角点的描述子，一般为 32 维的特征向量。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">frame 1: kps[0]=(294.0, 217.0), des[0]=[ 66 245  18 ...  39 206]</span><br></pre></td></tr></table></figure>

<h2 id="特征点匹配"><a href="#特征点匹配" class="headerlink" title="特征点匹配"></a>特征点匹配</h2><p>特征点匹配的意思就是将本帧检测的所有角点和上一帧的角点进行匹配，因此需要将上一帧的角点 <code>last_kps</code>  和描述子 <code>last_des</code>  存储起来。此外还需要 <code>idx</code> 记录每帧的序列号，并且从第二帧才开始做匹配。我们构造了一个 Frame 类，并将它们定义为类的属性，在实例初始化的时候再将这些属性传递给对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Frame</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    last_kps, last_des, last_pose = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, image</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        只要一经初始化，Frame 就会把上一帧的信息传递给下一帧</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        Frame.idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.image = image</span><br><span class="line">        self.idx   = Frame.idx</span><br><span class="line">        self.last_kps  = Frame.last_kps</span><br><span class="line">        self.last_des  = Frame.last_des</span><br><span class="line">        self.last_pose = Frame.last_pose</span><br></pre></td></tr></table></figure>

<p>我直接使用了暴力匹配（Brute-Force）的方法对两帧图片的角点进行配准，通过 <code>cv2.BFMatcher</code> 可以创建一个匹配器 <code>bfmatch</code>，它有两个可选的参数：</p>
<ul>
<li><code>normType</code>：度量两个角点之间距离的方式，由于 ORB 是一种基于二进制字符串的描述符，因此可以选择汉明距离 (<code>cv2.NORM_HAMMING</code>)。</li>
<li><code>crossCheck</code>：为布尔变量，默认值为 False。如果设置为 True，匹配条件就会更加严格，只有当两个特征点互相为最佳匹配时才可以。</li>
</ul>
<p>使用 <code>knnMatch()</code>  可以为每个关键点返回 k 个最佳匹配（将序排列之后取前 k 个），其中 k 是用户自己设定的，这里设置成 k=2。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bfmatch = cv2.BFMatcher(cv2.NORM_HAMMING)</span><br><span class="line">matches = bfmatch.knnMatch(frame.curr_des, frame.last_des, k=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>将 <code>frame.curr_des</code> 、 <code>frame.last_des</code> 和 <code>matches</code> 的数量打印出来：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">frame: 16, curr_des: 1660, last_des: 1597, matches: 1660</span><br><span class="line">frame: 17, curr_des: 1484, last_des: 1660, matches: 1484</span><br></pre></td></tr></table></figure>

<p>我们发现 <code>matches</code> 的数量始终与 <code>frame.curr_des</code> 相等，这是因为这里是从上一帧 <code>frame.last_des</code> 给当前帧 <code>frame.curr_des</code> 找最佳匹配点，并且每个 <code>match</code> 返回的是 2 个 <code>DMatch</code> 对象，它们具有以下属性：</p>
<ul>
<li><code>DMatch.distance</code> ：关键点之间的距离，越小越好。</li>
<li><code>DMatch.trainIdx</code> ：目标图像中描述符的索引。</li>
<li><code>DMatch.queryIdx</code> ：查询图像中描述符的索引。</li>
<li><code>DMatch.imgIdx</code>：目标图像的索引。</li>
</ul>
<p>如果第一名的距离小于第二名距离的 75%，那么将认为第一名大概率是匹配上了，此时 <code>m.queryIdx</code> 为当前帧关键点的索引，<code>m.trainIdx</code> 为上一帧关键点的索引，<code>match_kps</code> 返回的是每对配准点的位置。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> m,n <span class="keyword">in</span> matches:</span><br><span class="line">    <span class="keyword">if</span> m.distance &lt; <span class="number">0.75</span>*n.distance:</span><br><span class="line">        idx1.append(m.queryIdx)</span><br><span class="line">        idx2.append(m.trainIdx)</span><br><span class="line"></span><br><span class="line">        p1 = frame.curr_kps[m.queryIdx]     <span class="comment"># 当前帧配准的角点位置</span></span><br><span class="line">        p2 = frame.last_kps[m.trainIdx]     <span class="comment"># 上一帧配置的角点位置</span></span><br><span class="line">        match_kps.append((p1, p2))</span><br></pre></td></tr></table></figure>

<p>在下图中：红色的是当前帧的关键点，蓝色的是当前帧关键点的位置与上一帧关键点位置的连线。由于 🚗 是向前行驶，因此关键点相对 🚗 是往后运动的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> kp1, kp2 <span class="keyword">in</span> <span class="built_in">zip</span>(frame.curr_kps, frame.last_kps):</span><br><span class="line">    u1, v1 = <span class="built_in">int</span>(kp1[<span class="number">0</span>]), <span class="built_in">int</span>(kp1[<span class="number">1</span>])</span><br><span class="line">    u2, v2 = <span class="built_in">int</span>(kp2[<span class="number">0</span>]), <span class="built_in">int</span>(kp2[<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用圆圈画出当前帧角点的位置</span></span><br><span class="line">    cv2.circle(frame.image, (u1, v1), color=(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), radius=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 用直线追踪角点的行动轨迹</span></span><br><span class="line">    cv2.line(frame.image, (u1, v1), (u2, v2), color=(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/3.jpg">
</p>

<h2 id="RANSAC-去噪和本质矩阵"><a href="#RANSAC-去噪和本质矩阵" class="headerlink" title="RANSAC 去噪和本质矩阵"></a>RANSAC 去噪和本质矩阵</h2><p>RANSAC (RAndom SAmple Consensus, 随机采样一致) 算法是从一组含有 “外点” (outliers) 的数据中正确估计数学模型参数的迭代算法。RANSAC 算法有 2 个基本的假设：</p>
<ul>
<li>假设数据是由“内点”和“外点”组成的。“内点”就是组成模型参数的数据，“外点”就是不适合模型的异常值，通常是那些估计曲线以外的离群点。</li>
<li>假设在给定一组含有少部分“内点”的数据中，存在一个模型可以估计出符合“内点”变化的规律。</li>
</ul>
<p>具体的细节这里不再展开，感兴趣的话可以看<a href="https://zhuanlan.zhihu.com/p/62238520">这里</a>，这里是直接使用三方库里的 <code>scikit-image</code> 里的 <code>ransac</code> 算法进行求解。由于我们在求解本质矩阵的时候，<strong><font color=Red>需要利用相机内参将角点的像素坐标进行归一化：</font></strong></p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/0.png">
</p>

<p>其中 <code>p1</code> 和 <code>p2</code> 分别为配对角点在图片上的像素位置，那么归一化的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">pts</span>):</span></span><br><span class="line">    Kinv = np.linalg.inv(K)</span><br><span class="line">    <span class="comment"># turn [[x,y]] -&gt; [[x,y,1]]</span></span><br><span class="line">    add_ones = <span class="keyword">lambda</span> x: np.concatenate([x, np.ones((x.shape[<span class="number">0</span>], <span class="number">1</span>))], axis=<span class="number">1</span>)</span><br><span class="line">    norm_pts = np.dot(Kinv, add_ones(pts).T).T[:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> norm_pts</span><br></pre></td></tr></table></figure>

<p>slam 知识里给出了本质矩阵和归一化坐标之间的关系，它可以用一个简洁的公式来表达：</p>
<p align="center">
    <img width="15%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/2.jpg">
</p>

<p>其中本质矩阵 <strong>E</strong> 是平移向量 <strong>t</strong> 和旋转矩阵 <strong>R</strong> 的外积：</p>
<p align="center">
    <img width="12%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/7.jpg">
</p>

<p>本质矩阵 <code>E</code> 是一个 <code>3x3</code> 的矩阵，有 9 个未知元素。然而，上面的公式中 <code>x</code> 使用的是齐次坐标（已经有一个已知的 <code>1</code>）。而齐次坐标在相差一个常数因子下是相等，因此在单位尺度下只需 8 个点即可求解。</p>
<p align="center">
    <img width="48%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/1.jpg">
</p>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_essential_matrix</span>(<span class="params">match_kps</span>):</span></span><br><span class="line">    match_kps = np.array(match_kps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用相机内参对角点坐标归一化</span></span><br><span class="line">    norm_curr_kps = normalize(match_kps[:, <span class="number">0</span>])</span><br><span class="line">    norm_last_kps = normalize(match_kps[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求解本质矩阵和内点数据</span></span><br><span class="line">    model, inliers = ransac((norm_curr_kps, norm_last_kps),</span><br><span class="line">                            EssentialMatrixTransform,</span><br><span class="line">                            min_samples=<span class="number">8</span>,              <span class="comment"># 最少需要 8 个点</span></span><br><span class="line">                            residual_threshold=<span class="number">0.005</span>,</span><br><span class="line">                            max_trials=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    frame.curr_kps = frame.curr_kps[inliers]   <span class="comment"># 保留当前帧的内点数据</span></span><br><span class="line">    frame.last_kps = frame.last_kps[inliers]   <span class="comment"># 保留上一帧的内点数据</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model.params       <span class="comment"># 返回本质矩阵</span></span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/4.jpg">
</p>

<blockquote>
<p>可以看到，经过 RANSAC 去燥后，噪点数据消失了很多，角点的追踪情况基本稳定。但是经过筛选后，角点的数量只有原来的三分之一左右了。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">frame: 46, curr_des: 1555, last_des: 1467, match_kps: 549</span><br><span class="line">---------------- Essential Matrix ----------------</span><br><span class="line">[[-2.86637732e-04 -1.16930419e+00  1.12798916e-01]</span><br><span class="line"> [ 1.16673848e+00 -2.40819717e-03 -2.60028204e-01]</span><br><span class="line"> [-1.10221539e-01  2.67480554e-01 -1.20159639e-03]]</span><br></pre></td></tr></table></figure>

<h2 id="本质矩阵分解"><a href="#本质矩阵分解" class="headerlink" title="本质矩阵分解"></a>本质矩阵分解</h2><p>接下来的问题是如何根据已经估计得到的本质矩阵 <strong>E</strong>，恢复出相机的运动 <strong>R</strong>，<strong>t</strong>。这个过程是由奇异值分解得到的：</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/6.png">
</p>

<p>我们发现对角矩阵 <code>diag([1, 1, 0])</code> 可以由 <code>Z</code> 和 <code>W</code> 拆分得到。</p>
<p align="center">
    <img width="38%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/9.png">
</p>

<p align="center">
    <img width="53%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/8.png">
</p>

<p>将 <code>Z</code> 和 <code>W</code> 代入进来，令 <code>E = S R</code>。可以分解成两种情况：</p>
<ul>
<li>情况 1:</li>
</ul>
<p align="center">
    <img width="65%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/11.jpg">
</p>

<p align="center">
    <img width="32%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/13.jpg">
</p>

<ul>
<li>情况 2:</li>
</ul>
<p align="center">
    <img width="78%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/12.jpg">
</p>

<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/15.jpg">
</p>

<p>我们发现，此时已经将旋转矩阵 <code>R</code> 分离出来了，它有两种情况：分别等于 <code>R1</code> 和 <code>R2</code>。接下来我们需要考虑平移向量 <strong>t</strong>，可以证明出 <strong>t</strong> 其实是在 <strong><code>S</code></strong> 的零向量空间里，因为：</p>
<p align="center">
    <img width="18%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/16.jpg">
</p>

<p>结合线性代数的知识，不难求出 <code>t = U * (0, 0, 1) = u3</code> (即 <code>U</code> 的最后一列)。考虑到给 <code>t</code> 乘以一个非零尺度因子 <code>λ</code>， 对于 <code>E</code> 而言这种情况依旧有效，而对于 <code>t</code> 而言， 当 <code>λ = ± 1</code> 时，它们物理的意义（方向）却是不同的。综上，在已知第一个相机矩阵 <code>P = [ I ∣ 0 ]</code> 的情况下，第二个相机矩阵 <code>P′</code> 有如下 4 种可能的解：</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/17.jpg">
</p>

<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/18.png">
</p>

<blockquote>
<p>我们发现上面 4 种解其实是 2 种 R 和 2 种 t 之间的排列组合，只有当点 P 位于两个相机前方时才具有正深度，即 (1) 才是唯一正确解。</p>
</blockquote>
<p><a href="https://github.com/opencv/opencv/blob/3.1.0/modules/calib3d/src/five-point.cpp#L617">OpenCV</a> 提供了从本质矩阵中恢复相机的 <code>Rt</code> 的方法：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cv::decomposeEssentialMat</span><span class="params">( InputArray _E, OutputArray _R1, OutputArray _R2, OutputArray <span class="keyword">_t</span> )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Mat E = _E.<span class="built_in">getMat</span>().<span class="built_in">reshape</span>(<span class="number">1</span>, <span class="number">3</span>);</span><br><span class="line">	<span class="built_in">CV_Assert</span>(E.cols == <span class="number">3</span> &amp;&amp; E.rows == <span class="number">3</span>);</span><br><span class="line">	</span><br><span class="line">    Mat D, U, Vt;</span><br><span class="line">	SVD::<span class="built_in">compute</span>(E, D, U, Vt);</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> (<span class="built_in">determinant</span>(U) &lt; <span class="number">0</span>) U *= <span class="number">-1.</span>;</span><br><span class="line">	<span class="keyword">if</span> (<span class="built_in">determinant</span>(Vt) &lt; <span class="number">0</span>) Vt *= <span class="number">-1.</span>;</span><br><span class="line">	</span><br><span class="line">    Mat W = (Mat_&lt;<span class="keyword">double</span>&gt;(<span class="number">3</span>, <span class="number">3</span>) &lt;&lt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">    W.<span class="built_in">convertTo</span>(W, E.<span class="built_in">type</span>());</span><br><span class="line">    Mat R1, R2, t;</span><br><span class="line">    </span><br><span class="line">    R1 = U * W * Vt;</span><br><span class="line">    R2 = U * W.<span class="built_in">t</span>() * Vt;</span><br><span class="line">    t = U.<span class="built_in">col</span>(<span class="number">2</span>) * <span class="number">1.0</span>;</span><br><span class="line">    </span><br><span class="line">    R1.<span class="built_in">copyTo</span>(_R1);</span><br><span class="line">    R2.<span class="built_in">copyTo</span>(_R2);</span><br><span class="line">    t.<span class="built_in">copyTo</span>(<span class="keyword">_t</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出  <a href="https://docs.opencv.org/master/d9/d0c/group__calib3d.html#ga54a2f5b3f8aeaf6c76d4a31dece85d5d">cv::decomposeEssentialMat</a>  函数输出了 <code>R1</code>、<code>R2</code> 和 <code>t</code>，因此 4 种可能的解分别为：<code>[R1,t]</code>, <code>[R1,−t]</code>, <code>[R2,t]</code>, <code>[R2,−t]</code>， ORB_SLAM2  里使用了 <a href="https://gitee.com/paopaoslam/ORB-SLAM2/blob/wubo&jiajia/src/Initializer.cpp?dir=0&filepath=src/Initializer.cpp&oid=ebe440148231a2c288d0aa11425db799468a92ab&sha=3ccff875e95723673258573b665ee2e33511f843#L1021">CheckRT</a> 函数对它们进行判断。考虑到 demo 视频里  🚗 是一直往前行驶，且没有转弯。因此相机 <code>t = (x, y, z)</code> 里的 <code>z &gt; 0</code>，并且相机 <code>R</code> 的对角矩阵将接近 <code>diag([1, 1, 1])</code> ，从而我们可以直接过滤出唯一解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_Rt</span>(<span class="params">E</span>):</span></span><br><span class="line">    W = np.mat([[<span class="number">0</span>,-<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]],dtype=<span class="built_in">float</span>)</span><br><span class="line">    U,d,Vt = np.linalg.svd(E)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(U)  &lt; <span class="number">0</span>: U  *= -<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(Vt) &lt; <span class="number">0</span>: Vt *= -<span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 相机没有转弯，因此 R 的对角矩阵非常接近 diag([1,1,1])</span></span><br><span class="line">    R = (np.dot(np.dot(U, W), Vt))</span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">sum</span>(R.diagonal()) &lt; <span class="number">0</span>:</span><br><span class="line">        R = np.dot(np.dot(U, W.T), Vt)</span><br><span class="line"></span><br><span class="line">    t = U[:, <span class="number">2</span>]     <span class="comment"># 相机一直向前，分量 t[2] &gt; 0</span></span><br><span class="line">    <span class="keyword">if</span> t[<span class="number">2</span>] &lt; <span class="number">0</span>:</span><br><span class="line">        t *= -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    Rt = np.eye(<span class="number">4</span>)</span><br><span class="line">    Rt[:<span class="number">3</span>, :<span class="number">3</span>] = R</span><br><span class="line">    Rt[:<span class="number">3</span>, <span class="number">3</span>] = t</span><br><span class="line">    <span class="keyword">return</span> Rt          <span class="comment"># Rt 为从相机坐标系的位姿变换到世界坐标系的位姿</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>由于平移向量的分量 t[2] &gt; 0，我们很容易知道 Rt 为从相机坐标系的位姿变换到世界坐标系的位姿</p>
</blockquote>
<h2 id="三角测量"><a href="#三角测量" class="headerlink" title="三角测量"></a>三角测量</h2><p>下一步我们需要用相机的运动估计特征点的空间位置，在单目 SLAM 中仅通过单目图像是无法获得像素的深度信息，我们需要通过<strong>三角测量（Triangulation）</strong>的方法估计图像的深度，然后通过直接线性变化（DLT）进行求解。 </p>
<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/19.png">
</p>


<p>假设点 <code>P</code> 的世界坐标为 <code>X_&#123;w&#125;</code>，图像坐标为 <code>X_&#123;uv&#125;</code>，相机的内参和位姿分别为 <code>K</code> 和 <code>P_&#123;cw&#125;</code>，那么得到：</p>
<p align="center">
    <img width="25%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1608815330116.jpg">
</p>

<p>将下标去掉，使用相机内参将两个匹配的角点像素坐标进行归一化，代入到上述方程中便得到：</p>
<p align="center">
    <img width="11%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1608816102196.jpg">
</p>

<p>使用 DLT 的话我们对上面两个公式进行一个简单的变换，对等式两边分别做外积运算：</p>
<p align="center">
    <img width="23%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1608816174696.jpg">
</p>

<p>由于 <code>x=&#123;u, v, 1&#125;</code> ，结合外积运算的知识（详见 slam 十四讲 75 页），我们便得到以下方程：</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1608816576374.jpg">
</p>

<p>我们不妨令：</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1611127747577.jpg">
</p>

<p>将两个匹配的角点和相机位姿代入上述方程中便得到 <code>A</code>：</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1611128389750.jpg">
</p>

<p>因此便可以简化成 <code>AX=0</code>，从而可以使用最小二乘法来求解出 <code>X</code>，<a href="https://github.com/raulmur/ORB_SLAM2/blob/f2e6f51cdc8d067655d90a78c06261378e07e8f3/src/Initializer.cc#L734">ORB_SLAM2</a> 中的求解过程如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Initializer::Triangulate</span><span class="params">(<span class="keyword">const</span> cv::KeyPoint &amp;kp1, <span class="keyword">const</span> cv::KeyPoint &amp;kp2, <span class="keyword">const</span> cv::Mat &amp;P1, <span class="keyword">const</span> cv::Mat &amp;P2, cv::Mat &amp;x3D)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">cv::Mat <span class="title">A</span><span class="params">(<span class="number">4</span>,<span class="number">4</span>,CV_32F)</span></span>;</span><br><span class="line"></span><br><span class="line">    A.<span class="built_in">row</span>(<span class="number">0</span>) = kp1.pt.x*P1.<span class="built_in">row</span>(<span class="number">2</span>)-P1.<span class="built_in">row</span>(<span class="number">0</span>);</span><br><span class="line">    A.<span class="built_in">row</span>(<span class="number">1</span>) = kp1.pt.y*P1.<span class="built_in">row</span>(<span class="number">2</span>)-P1.<span class="built_in">row</span>(<span class="number">1</span>);</span><br><span class="line">    A.<span class="built_in">row</span>(<span class="number">2</span>) = kp2.pt.x*P2.<span class="built_in">row</span>(<span class="number">2</span>)-P2.<span class="built_in">row</span>(<span class="number">0</span>);</span><br><span class="line">    A.<span class="built_in">row</span>(<span class="number">3</span>) = kp2.pt.y*P2.<span class="built_in">row</span>(<span class="number">2</span>)-P2.<span class="built_in">row</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    cv::Mat u,w,vt;</span><br><span class="line">    cv::SVD::<span class="built_in">compute</span>(A,w,u,vt,cv::SVD::MODIFY_A| cv::SVD::FULL_UV);</span><br><span class="line">    x3D = vt.<span class="built_in">row</span>(<span class="number">3</span>).<span class="built_in">t</span>();</span><br><span class="line">    x3D = x3D.<span class="built_in">rowRange</span>(<span class="number">0</span>,<span class="number">3</span>)/x3D.at&lt;<span class="keyword">float</span>&gt;(<span class="number">3</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Camera 1 Projection Matrix K[I|0]                  # 这里假设世界坐标系为相机 1 坐标系</span></span><br><span class="line"><span class="function">cv::Mat <span class="title">P1</span><span class="params">(<span class="number">3</span>,<span class="number">4</span>,CV_32F,cv::Scalar(<span class="number">0</span>))</span></span>;</span><br><span class="line">K.<span class="built_in">copyTo</span>(P1.<span class="built_in">rowRange</span>(<span class="number">0</span>,<span class="number">3</span>).<span class="built_in">colRange</span>(<span class="number">0</span>,<span class="number">3</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Camera 2 Projection Matrix K[R|t]                  # 相机 2 与 相机的位姿 R，t</span></span><br><span class="line"><span class="function">cv::Mat <span class="title">P2</span><span class="params">(<span class="number">3</span>,<span class="number">4</span>,CV_32F)</span></span>;</span><br><span class="line">R.<span class="built_in">copyTo</span>(P2.<span class="built_in">rowRange</span>(<span class="number">0</span>,<span class="number">3</span>).<span class="built_in">colRange</span>(<span class="number">0</span>,<span class="number">3</span>));</span><br><span class="line">t.<span class="built_in">copyTo</span>(P2.<span class="built_in">rowRange</span>(<span class="number">0</span>,<span class="number">3</span>).<span class="built_in">col</span>(<span class="number">3</span>));</span><br><span class="line">P2 = K*P2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> cv::KeyPoint &amp;kp1 = vKeys1[vMatches12[i].first];</span><br><span class="line"><span class="keyword">const</span> cv::KeyPoint &amp;kp2 = vKeys2[vMatches12[i].second];</span><br><span class="line">cv::Mat p3dC1;</span><br><span class="line"></span><br><span class="line"><span class="built_in">Triangulate</span>(kp1,kp2,P1,P2,p3dC1);</span><br></pre></td></tr></table></figure>

<p>如下所示，我使用了 Python 对整个三角测量的计算过程进行了复现。值得注意的是，上述的相机位姿是指从空间点 P 从世界坐标系变换到相机坐标下点变换矩阵。如果你不清楚相机位姿的概念，请看<a href="https://yunyang1994.gitee.io/2019/12/27/CameraPose/">什么是相机位姿？</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triangulate</span>(<span class="params">pts1, pts2, pose1, pose2</span>):</span></span><br><span class="line">    pose1 = np.linalg.inv(pose1)            <span class="comment"># 从世界坐标系变换到相机坐标系的位姿, 因此取逆</span></span><br><span class="line">    pose2 = np.linalg.inv(pose2)</span><br><span class="line"></span><br><span class="line">    pts1 = normalize(pts1)                 <span class="comment"># 使用相机内参对角点坐标归一化</span></span><br><span class="line">    pts2 = normalize(pts2)</span><br><span class="line"></span><br><span class="line">    points4d = np.zeros((pts1.shape[<span class="number">0</span>], <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> i, (kp1, kp2) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(pts1, pts2)):</span><br><span class="line">        A = np.zeros((<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line">        A[<span class="number">0</span>] = kp1[<span class="number">0</span>] * pose1[<span class="number">2</span>] - pose1[<span class="number">0</span>]</span><br><span class="line">        A[<span class="number">1</span>] = kp1[<span class="number">1</span>] * pose1[<span class="number">2</span>] - pose1[<span class="number">1</span>]</span><br><span class="line">        A[<span class="number">2</span>] = kp2[<span class="number">0</span>] * pose2[<span class="number">2</span>] - pose2[<span class="number">0</span>]</span><br><span class="line">        A[<span class="number">3</span>] = kp2[<span class="number">1</span>] * pose2[<span class="number">2</span>] - pose2[<span class="number">1</span>]</span><br><span class="line">        _, _, vt = np.linalg.svd(A)         <span class="comment"># 对 A 进行奇异值分解</span></span><br><span class="line">        points4d[i] = vt[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    points4d /= points4d[:, <span class="number">3</span>:]            <span class="comment"># 归一化变换成齐次坐标 [x, y, z, 1]</span></span><br><span class="line">    <span class="keyword">return</span> points4d</span><br></pre></td></tr></table></figure>

<h2 id="pipeline-流程"><a href="#pipeline-流程" class="headerlink" title="pipeline 流程"></a>pipeline 流程</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_frame</span>(<span class="params">frame</span>):</span></span><br><span class="line">    <span class="comment"># 提取当前帧的角点和描述子特征</span></span><br><span class="line">    frame.curr_kps, frame.curr_des = extract_points(frame)</span><br><span class="line">    <span class="comment"># 将角点位置和描述子通过类的属性传递给下一帧作为上一帧的角点信息</span></span><br><span class="line">    Frame.last_kps, Frame.last_des = frame.curr_kps, frame.curr_des</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> frame.idx == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 设置第一帧为初始帧，并以相机坐标系为世界坐标系</span></span><br><span class="line">        frame.curr_pose = np.eye(<span class="number">4</span>)</span><br><span class="line">        points4d = [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]]      <span class="comment"># 原点为 [0, 0, 0] , 1 表示颜色</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 角点配准, 此时会用 RANSAC 过滤掉一些噪声</span></span><br><span class="line">        match_kps = match_points(frame)</span><br><span class="line">        <span class="comment"># 使用八点法拟合出本质矩阵</span></span><br><span class="line">        essential_matrix = fit_essential_matrix(match_kps)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;---------------- Essential Matrix ----------------&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(essential_matrix)</span><br><span class="line">        <span class="comment"># 利用本质矩阵分解出相机的位姿 Rt</span></span><br><span class="line">        Rt = extract_Rt(essential_matrix)</span><br><span class="line">        <span class="comment"># 计算出当前帧相对于初始帧的相机位姿</span></span><br><span class="line">        frame.curr_pose = np.dot(Rt, frame.last_pose)</span><br><span class="line">        <span class="comment"># 三角测量获得角点的深度信息</span></span><br><span class="line">        points4d = triangulate(frame.last_kps, frame.curr_kps, frame.last_pose, frame.curr_pose)</span><br><span class="line">		<span class="comment"># 判断3D点是否在两个摄像头前方</span></span><br><span class="line">        good_pt4d = check_points(points4d)</span><br><span class="line">        points4d = points4d[good_pt4d]</span><br><span class="line"></span><br><span class="line">        draw_points(frame)</span><br><span class="line">    mapp.add_observation(frame.curr_pose, points4d)     <span class="comment"># 将当前的 pose 和点云放入地图中</span></span><br><span class="line">    <span class="comment"># 将当前帧的 pose 信息存储为下一帧的 last_pose 信息</span></span><br><span class="line">    Frame.last_pose = frame.curr_pose</span><br><span class="line">    <span class="keyword">return</span> frame</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>立体视觉</category>
      </categories>
      <tags>
        <tag>本质矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv3 算法的一点理解</title>
    <url>/2018/12/28/YOLOv3/</url>
    <content><![CDATA[<p>今天讲一讲 YOLOv3, 目标检测网络的巅峰之作, 疾如风，快如闪电。</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/sayit.jpg">
</p>

<span id="more"></span>

<h2 id="算法背景"><a href="#算法背景" class="headerlink" title="算法背景"></a>算法背景</h2><p>假设我们想对下面这张 416 X 416 大小的图片进行预测，把图中 dog、bicycle 和 car 三种物体给框出来，这涉及到以下三个过程：</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/dog.png">
</p>

<ul>
<li>怎么在图片上找出很多有价值的候选框？</li>
<li>接着判断候选框里有没有物体？</li>
<li>如果有物体的话，那么它属于哪个类别？</li>
</ul>
<p>听起来就像把大象装进冰箱，分三步走。事实上，目前的 anchor-based 机制算法例如 RCNN、Faster rcnn 以及 YOLO 算法都是这个思想。最早的时候，RCNN 是这么干的，它首先利用 Selective Search 的方法通过图片上像素之间的相似度和纹理特征进行区域合并，然后提出很多候选框并喂给 CNN 网络提取出特征向量 (embeddings)，最后利用特征向量训练 SVM 来对目标和背景进行分类。</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/RCNN.jpg" alt="image"></p>
<p>这是最早利用神经网络进行目标检测的开山之作，虽然现在看来有不少瑕疵，例如：</p>
<ul>
<li>Selective Search 会在图片上提取2000个候选区域，每个候选区域都会喂给 CNN 进行特征提取，这个过程太冗余啦，其实这些候选区域之间很多特征其实是可以共享的；</li>
<li>由于 CNN 最后一层是全连接层，因此输入图片的尺寸大小也有限制，只能进行 Crop 或者 Warp，这样一来图片就会扭曲、变形和失真；</li>
<li>在利用 SVM 分类器对候选框进行分类的时候，每个候选框的特征向量都要保留在磁盘上，很浪费空间！</li>
</ul>
<p>尽管如此，但仍不可否认它具有划时代的意义，至少告诉后人我们是可以利用神经网络进行目标检测的。后面，一些大神们在此基础上提出了很多改进，从 Fast RCNN 到 Faster RCNN 再到 Mask RCNN, 目标检测的 region proposal 过程变得越来越有针对性，并提出了著名的 RPN 网络去学习如何给出高质量的候选框，然后再去判断所属物体的类别。简单说来就是: 提出候选框，然后分类，这就是我们常说的 two-stage 算法。two-stage 算法的好处就是精度较高，但是检测速度满足不了实时性的要求。</p>
<p>在这样的背景下，YOLO 算法横空出世，江湖震惊！</p>
<h2 id="YOLO-算法简介"><a href="#YOLO-算法简介" class="headerlink" title="YOLO 算法简介"></a>YOLO 算法简介</h2><h3 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a>发展历程</h3><p>2015 年 Redmon J 等提出 YOLO 网络, 其特点是将生成候选框与分类回归合并成一个步骤, 预测时特征图被分成 7x7 个 cell, 对每个 cell 进行预测, 这就大大降低了计算复杂度, 加快了目标检测的速度, 帧率最高可达 45 fps！</p>
<p>时隔一年，Redmon J 再次提出了YOLOv2, 与前代相比, 在VOC2007 测试集上的 mAP 由 67.4% 提高到 78.6%, 然而由于一个 cell 只负责预测一个物体, 面对重叠性的目标的识别得并不够好。</p>
<p>最终在 2018 年 4 月, 作者又发布了第三个版本 YOLOv3, 在 COCO 数据集上的 mAP-50 由 YOLOv2 的 44.0% 提高到 57.9%, 与 mAP 61.1% 的 RetinaNet 相比, RetinaNet 在输入尺寸 500×500 的情况下检测速度约 98 ms/帧, 而 YOLOv3 在输入尺寸 416×416 时检测速 度可达 29 ms/帧。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/yolov3_speed.jpg">
</p>

<p>上面这张图足以秒杀一切, 说明 YOLOv3 在保证速度的前提下, 也达到了很高的准确率。</p>
<h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>作者在YOLO算法中把物体检测（object detection）问题处理成回归问题，并将图像分为S×S的网格。如果一个目标的中心落入格子，该格子就负责检测该目标。</p>
<blockquote>
<p>If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.</p>
</blockquote>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/grid.jpg">
</p>

<p>每个网格都会输出 bounding box，confidence 和 class probability map。其中：</p>
<ul>
<li>bounding box 包含4个值：x，y，w，h，（x，y）代表 box 的中心。（w，h）代表 box 的宽和高；</li>
<li>confidence 表示这个预测框中包含物体的概率，其实也是预测框与真实框之间的 iou 值;</li>
<li>class probability 表示的是该物体的类别概率，在 YOLOv3 中采用的是二分类的方法。</li>
</ul>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>下面这幅图就是 YOLOv3 网络的整体结构，在图中我们可以看到：尺寸为 416X416 的输入图片进入 Darknet-53 网络后得到了 3 个分支，这些分支在经过一系列的卷积、上采样以及合并等操作后最终得到了三个尺寸不一的 feature map，形状分别为 [13, 13, 255]、[26, 26, 255] 和 [52, 52, 255]。</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/levio.png" alt="YOLOv3 的网络结构"></p>
<p>讲了这么多，还是不如看代码来得亲切。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">YOLOv3</span>(<span class="params">input_layer</span>):</span></span><br><span class="line">    <span class="comment"># 输入层进入 Darknet-53 网络后，得到了三个分支</span></span><br><span class="line">    route_1, route_2, conv = backbone.darknet53(input_layer)</span><br><span class="line">    <span class="comment"># 见上图中的橘黄色模块(DBL)，一共需要进行5次卷积操作</span></span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1024</span>,  <span class="number">512</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>,  <span class="number">512</span>, <span class="number">1024</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1024</span>,  <span class="number">512</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>,  <span class="number">512</span>, <span class="number">1024</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1024</span>,  <span class="number">512</span>))</span><br><span class="line">    conv_lobj_branch = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">512</span>, <span class="number">1024</span>))</span><br><span class="line">    <span class="comment"># conv_lbbox 用于预测大尺寸物体，shape = [None, 13, 13, 255]</span></span><br><span class="line">    conv_lbbox = common.convolutional(conv_lobj_branch, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1024</span>, <span class="number">3</span>*(NUM_CLASS + <span class="number">5</span>)), </span><br><span class="line">                                                            activate=<span class="literal">False</span>, bn=<span class="literal">False</span>)</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>,  <span class="number">512</span>,  <span class="number">256</span>))</span><br><span class="line">    <span class="comment"># 这里的 upsample 使用的是最近邻插值方法，这样的好处在于上采样过程不需要学习，从而减少了网络参数</span></span><br><span class="line">    conv = common.upsample(conv)</span><br><span class="line">    conv = tf.concat([conv, route_2], axis=-<span class="number">1</span>)</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">768</span>, <span class="number">256</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">512</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>, <span class="number">256</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">512</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>, <span class="number">256</span>))</span><br><span class="line">    conv_mobj_branch = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">512</span>))</span><br><span class="line">    <span class="comment"># conv_mbbox 用于预测中等尺寸物体，shape = [None, 26, 26, 255]</span></span><br><span class="line">    conv_mbbox = common.convolutional(conv_mobj_branch, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>, <span class="number">3</span>*(NUM_CLASS + <span class="number">5</span>)),</span><br><span class="line">                                                            activate=<span class="literal">False</span>, bn=<span class="literal">False</span>)</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">128</span>))</span><br><span class="line">    conv = common.upsample(conv)</span><br><span class="line">    conv = tf.concat([conv, route_1], axis=-<span class="number">1</span>)</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">384</span>, <span class="number">128</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">256</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">128</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">256</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">128</span>))</span><br><span class="line"></span><br><span class="line">    conv_sobj_branch = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">256</span>))</span><br><span class="line">    <span class="comment"># conv_sbbox 用于预测小尺寸物体，shape = [None, 52, 52, 255]</span></span><br><span class="line">    conv_sbbox = common.convolutional(conv_sobj_branch, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">3</span>*(NUM_CLASS +<span class="number">5</span>)), </span><br><span class="line">                                                            activate=<span class="literal">False</span>, bn=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> [conv_sbbox, conv_mbbox, conv_lbbox]</span><br></pre></td></tr></table></figure>

<h3 id="Darknet53-结构"><a href="#Darknet53-结构" class="headerlink" title="Darknet53 结构"></a>Darknet53 结构</h3><p>Darknet-53 的主体框架如下图所示，它主要由 Convolutional 和 Residual 结构所组成。需要特别注意的是，最后三层 Avgpool、Connected 和 softmax layer 是用于在 Imagenet 数据集上作分类训练用的。当我们用 Darknet-53 层对图片提取特征时，是不会用到这三层的。</p>
<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/darknet53.png">
</p>

<p>Darknet-53 有多牛逼？看看下面这张图，作者进行了比较，得出的结论是 Darknet-53 在精度上可以与最先进的分类器进行媲美，同时它的浮点运算更少，计算速度也最快。和 ReseNet-101 相比，Darknet-53 网络的速度是前者的1.5倍；虽然 ReseNet-152 和它性能相似，但是用时却是它的2倍以上。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/darknet_speed.png">
</p>


<p>此外，Darknet-53 还可以实现每秒最高的测量浮点运算，这就意味着网络结构可以更好地利用 GPU，从而使其测量效率更高，速度也更快。</p>
<h3 id="Convolutional-结构"><a href="#Convolutional-结构" class="headerlink" title="Convolutional 结构"></a>Convolutional 结构</h3><p>Convolutional 结构其实很简单，就是普通的卷积层，其实没啥讲的。但是对于 if downsample 的情况，初学者可能觉得有点陌生， ZeroPadding2D 是什么层？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional</span>(<span class="params">input_layer, filters_shape, downsample=<span class="literal">False</span>, activate=<span class="literal">True</span>, bn=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> downsample:</span><br><span class="line">        input_layer = tf.keras.layers.ZeroPadding2D(((<span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>)))(input_layer)</span><br><span class="line">        padding = <span class="string">&#x27;valid&#x27;</span></span><br><span class="line">        strides = <span class="number">2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        strides = <span class="number">1</span></span><br><span class="line">        padding = <span class="string">&#x27;same&#x27;</span></span><br><span class="line">    conv = tf.keras.layers.Conv2D(filters=filters_shape[-<span class="number">1</span>], </span><br><span class="line">                        kernel_size = filters_shape[<span class="number">0</span>], </span><br><span class="line">                        strides=strides, padding=padding, use_bias=<span class="keyword">not</span> bn, </span><br><span class="line">                        kernel_regularizer=tf.keras.regularizers.l2(<span class="number">0.0005</span>),</span><br><span class="line">                        kernel_initializer=tf.random_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">                        bias_initializer=tf.constant_initializer(<span class="number">0.</span>))(input_layer)</span><br><span class="line">    <span class="keyword">if</span> bn: conv = BatchNormalization()(conv)</span><br><span class="line">    <span class="keyword">if</span> activate == <span class="literal">True</span>: conv = tf.nn.leaky_relu(conv, alpha=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> conv</span><br></pre></td></tr></table></figure>

<p>讲到 ZeroPadding2D层，我们得先了解它是什么，为什么有这个层。对于它的定义，Keras 官方给了很好的解释:</p>
<blockquote>
<p>keras.layers.convolutional.ZeroPadding2D(padding=(1, 1), data_format=None) 说明: 对2D输入（如图片）的边界填充0，以控制卷积以后特征图的大小</p>
</blockquote>
<p align="center">
    <img width="15%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/zeropadding.gif">
</p>

<p>其实就是对图片的上下左右四个边界填充0而已，padding=((top_pad, bottom_pad), (left_pad, right_pad))。 很简单吧，快打开你的 ipython 试试吧！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">2</span>]: x=tf.keras.layers.Input([<span class="number">416</span>,<span class="number">416</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: tf.keras.layers.ZeroPadding2D(padding=((<span class="number">1</span>,<span class="number">0</span>),(<span class="number">1</span>,<span class="number">0</span>)))(x)</span><br><span class="line">Out[<span class="number">3</span>]: &lt;tf.Tensor <span class="string">&#x27;zero_padding2d/Identity:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">417</span>, <span class="number">417</span>, <span class="number">3</span>) dtype=float32&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: tf.keras.layers.ZeroPadding2D(padding=((<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">1</span>)))(x)</span><br><span class="line">Out[<span class="number">4</span>]: &lt;tf.Tensor <span class="string">&#x27;zero_padding2d_1/Identity:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">418</span>, <span class="number">418</span>, <span class="number">3</span>) dtype=float32&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Residual-残差模块"><a href="#Residual-残差模块" class="headerlink" title="Residual 残差模块"></a>Residual 残差模块</h3><p>残差模块最显著的特点是使用了 short cut 机制（有点类似于电路中的短路机制）来缓解在神经网络中增加深度带来的梯度消失问题，从而使得神经网络变得更容易优化。它通过恒等映射(identity mapping)的方法使得输入和输出之间建立了一条直接的关联通道，从而使得网络集中学习输入和输出之间的残差。</p>
<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/residual.png">
</p>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">residual_block</span>(<span class="params">input_layer, input_channel, filter_num1, filter_num2</span>):</span></span><br><span class="line">    short_cut = input_layer</span><br><span class="line">    conv = convolutional(input_layer, filters_shape=(<span class="number">1</span>, <span class="number">1</span>, input_channel, filter_num1))</span><br><span class="line">    conv = convolutional(conv       , filters_shape=(<span class="number">3</span>, <span class="number">3</span>, filter_num1,   filter_num2))</span><br><span class="line">    residual_output = short_cut + conv</span><br><span class="line">    <span class="keyword">return</span> residual_output</span><br></pre></td></tr></table></figure>

<h2 id="提取特征"><a href="#提取特征" class="headerlink" title="提取特征"></a>提取特征</h2><p>要想详细地知道 YOLO 的预测过程，就非常有必要先来了解一下什么是特征映射 (feature map) 和特征向量 (embeddings)。</p>
<h3 id="特征映射"><a href="#特征映射" class="headerlink" title="特征映射"></a>特征映射</h3><p>当我们谈及 CNN 网络，总能听到 feature map 这个词。它也叫<em>特征映射，简单说来就是输入图像在与卷积核进行卷积操作后得到图像特征</em>。</p>
<p>一般而言，CNN 网络在对图像自底向上提取特征时，feature map 的数量(其实也对应的就是卷积核的数目) 会越来越多，而空间信息会越来越少，其特征也会变得越来越抽象。比如著名的 VGG16 网络，它的 feature map 变化就是这个样子。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/vgg16.jpg">
</p>

<blockquote>
<p>feature map 在空间尺寸上越来越小，但在通道尺寸上变得越来越深，这就是 VGG16 的特点。</p>
</blockquote>
<h3 id="特征向量"><a href="#特征向量" class="headerlink" title="特征向量"></a>特征向量</h3><p>讲到 feature map 哦，就不得不提一下人脸识别领域里经常提到的 embedding. 一般来说，它其实就是 feature map 被最后一层全连接层所提取到特征向量。早在2006年，深度学习鼻祖 hinton 就在《SCIENCE》上发表了一篇论文，首次利用自编码网络对 mnist 手写数字提取出了特征向量(一个2维或3维的向量)。值得一提的是，也是这篇论文揭开了深度学习兴起的序幕。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/mnist.jpg">
</p>

<p>下面就是上面这张图片里的数字在 CNN 空间里映射后得到的特征向量在2维和3维空间里的样子:</p>
<p>前面我们提到：CNN 网络在对图像自底向上提取特征时，得到的 feature map 一般都是在空间尺寸上越来越小，而在通道尺寸上变得越来越深。 那么，为什么要这么做？</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/ROI.jpg">
</p>

<p>其实，这就与 ROI (感兴趣区域)映射到 Feature Map 有关。在上面这幅图里：原图里的一块 ROI 在 CNN 网络空间里映射后，在 feature map 上空间尺寸会变得更小，甚至是一个点, 但是这个点的通道信息会很丰富，这些通道信息是 ROI 区域里的图片信息在 CNN 网络里映射得到的特征表示。由于图像中各个相邻像素在空间上的联系很紧密，这在空间上造成具有很大的冗余性。因此，我们往往会通过在空间上降维，而在通道上升维的方式来消除这种冗余性，尽量以最小的维度来获得它最本质的特征。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/prediction.jpg">
</p>


<blockquote>
<p>原图左上角红色 ROI 经 CNN 映射后在 feature map 空间上只得到了一个点，但是这个点有85个通道。那么，ROI的维度由原来的 [32, 32, 3] 变成了现在的 85 维，这难道又不是降维打击么？👊</p>
</blockquote>
<p>按照我的理解，这其实就是 CNN 网络对 ROI 进行特征提取后得到的一个 85 维的特征向量。这个特征向量前4个维度代表的是候选框信息，中间这个维度代表是判断有无物体的概率，后面80个维度代表的是对 80 个类别的分类概率信息。</p>
<h2 id="如何检测"><a href="#如何检测" class="headerlink" title="如何检测"></a>如何检测</h2><h3 id="多尺度检测"><a href="#多尺度检测" class="headerlink" title="多尺度检测"></a>多尺度检测</h3><p>YOLOv3 对输入图片进行了粗、中和细网格划分，以便分别实现对大、中和小物体的预测。假如输入图片的尺寸为 416X416, 那么得到粗、中和细网格尺寸分别为 13X13、26X26 和 52X52。这样一算，那就是在长宽尺寸上分别缩放了 32、16 和 8 倍。</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/multi-scale_pred.png" alt="image"></p>
<h3 id="decode-处理"><a href="#decode-处理" class="headerlink" title="decode 处理"></a>decode 处理</h3><p>YOLOv3 网络的三个分支输出会被送入 decode 函数中对 Feature Map 的通道信息进行解码。 在下面这幅图里：黑色虚线框代表先验框(anchor)，蓝色框表示的是预测框.</p>
<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/decode_anchor.png">
</p>


<ul>
<li>$b_{h}$ 和 $b_{w}$ 分别表示预测框的长宽，$P_{h}$ 和 $P_{w}$ 分别表示先验框的长和宽。</li>
<li>$t_{x}$ 和 $t_{y}$ 表示的是物体中心距离网格左上角位置的偏移量，$C_{x}$ 和 $C_{y}$ 则代表网格左上角的坐标。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">conv_output, i=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="comment"># 这里的 i=0、1 或者 2， 以分别对应三种网格尺度</span></span><br><span class="line">    conv_shape  = tf.shape(conv_output)</span><br><span class="line">    batch_size  = conv_shape[<span class="number">0</span>]</span><br><span class="line">    output_size = conv_shape[<span class="number">1</span>]</span><br><span class="line">    conv_output = tf.reshape(conv_output, (batch_size, output_size, </span><br><span class="line">                                           output_size, <span class="number">3</span>, <span class="number">5</span> + NUM_CLASS))</span><br><span class="line">    conv_raw_dxdy = conv_output[:, :, :, :, <span class="number">0</span>:<span class="number">2</span>] <span class="comment"># 中心位置的偏移量</span></span><br><span class="line">    conv_raw_dwdh = conv_output[:, :, :, :, <span class="number">2</span>:<span class="number">4</span>] <span class="comment"># 预测框长宽的偏移量</span></span><br><span class="line">    conv_raw_conf = conv_output[:, :, :, :, <span class="number">4</span>:<span class="number">5</span>] <span class="comment"># 预测框的置信度</span></span><br><span class="line">    conv_raw_prob = conv_output[:, :, :, :, <span class="number">5</span>: ] <span class="comment"># 预测框的类别概率</span></span><br><span class="line">    <span class="comment"># 好了，接下来需要画网格了。其中，output_size 等于 13、26 或者 52</span></span><br><span class="line">    y = tf.tile(tf.<span class="built_in">range</span>(output_size, dtype=tf.int32)[:, tf.newaxis], [<span class="number">1</span>, output_size])</span><br><span class="line">    x = tf.tile(tf.<span class="built_in">range</span>(output_size, dtype=tf.int32)[tf.newaxis, :], [output_size, <span class="number">1</span>])</span><br><span class="line">    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-<span class="number">1</span>)</span><br><span class="line">    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">    xy_grid = tf.cast(xy_grid, tf.float32) <span class="comment"># 计算网格左上角的位置</span></span><br><span class="line">    <span class="comment"># 根据上图公式计算预测框的中心位置</span></span><br><span class="line">    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]</span><br><span class="line">    <span class="comment"># 根据上图公式计算预测框的长和宽大小</span></span><br><span class="line">    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]</span><br><span class="line">    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-<span class="number">1</span>) </span><br><span class="line">    pred_conf = tf.sigmoid(conv_raw_conf) <span class="comment"># 计算预测框里object的置信度</span></span><br><span class="line">    pred_prob = tf.sigmoid(conv_raw_prob) <span class="comment"># 计算预测框里object的类别概率</span></span><br><span class="line">    <span class="keyword">return</span> tf.concat([pred_xywh, pred_conf, pred_prob], axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="NMS-处理"><a href="#NMS-处理" class="headerlink" title="NMS 处理"></a>NMS 处理</h3><p>非极大值抑制（Non-Maximum Suppression，NMS），顾名思义就是抑制不是极大值的元素，说白了就是去除掉那些重叠率较高并且 score 评分较低的边界框。 NMS 的算法非常简单，迭代流程如下:</p>
<ul>
<li>流程1: 判断边界框的数目是否大于0，如果不是则结束迭代；</li>
<li>流程2: 按照 socre 排序选出评分最大的边界框 A 并取出；</li>
<li>流程3: 计算这个边界框 A 与剩下所有边界框的 iou 并剔除那些 iou 值高于阈值的边界框，重复上述步骤；</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 流程1: 判断边界框的数目是否大于0</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(cls_bboxes) &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># 流程2: 按照 socre 排序选出评分最大的边界框 A</span></span><br><span class="line">    max_ind = np.argmax(cls_bboxes[:, <span class="number">4</span>])</span><br><span class="line">    <span class="comment"># 将边界框 A 取出并剔除</span></span><br><span class="line">    best_bbox = cls_bboxes[max_ind]</span><br><span class="line">    best_bboxes.append(best_bbox)</span><br><span class="line">    cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + <span class="number">1</span>:]])</span><br><span class="line">    <span class="comment"># 流程3: 计算这个边界框 A 与剩下所有边界框的 iou 并剔除那些 iou 值高于阈值的边界框</span></span><br><span class="line">    iou = bboxes_iou(best_bbox[np.newaxis, :<span class="number">4</span>], cls_bboxes[:, :<span class="number">4</span>])</span><br><span class="line">    weight = np.ones((<span class="built_in">len</span>(iou),), dtype=np.float32)</span><br><span class="line">    iou_mask = iou &gt; iou_threshold</span><br><span class="line">    weight[iou_mask] = <span class="number">0.0</span></span><br><span class="line">    cls_bboxes[:, <span class="number">4</span>] = cls_bboxes[:, <span class="number">4</span>] * weight</span><br><span class="line">    score_mask = cls_bboxes[:, <span class="number">4</span>] &gt; <span class="number">0.</span></span><br><span class="line">    cls_bboxes = cls_bboxes[score_mask]</span><br></pre></td></tr></table></figure>

<p>最后所有取出来的边界框 A 就是我们想要的。不妨举个简单的例子：假如5个边界框及评分为: A: 0.9，B: 0.08，C: 0.8, D: 0.6，E: 0.5，设定的评分阈值为 0.3，计算步骤如下。</p>
<ul>
<li>步骤1: 边界框的个数为5，满足迭代条件；</li>
<li>步骤2: 按照 socre 排序选出评分最大的边界框 A 并取出；</li>
<li>步骤3: 计算边界框 A 与其他 4 个边界框的 iou，假设得到的 iou 值为：B: 0.1，C: 0.7, D: 0.02, E: 0.09, 剔除边界框 C;</li>
<li>步骤4: 现在只剩下边界框 B、D、E，满足迭代条件；</li>
<li>步骤5: 按照 socre 排序选出评分最大的边界框 D 并取出；</li>
<li>步骤6: 计算边界框 D 与其他 2 个边界框的 iou，假设得到的 iou 值为：B: 0.06，E: 0.8，剔除边界框 E；</li>
<li>步骤7: 现在只剩下边界框 B，满足迭代条件；</li>
<li>步骤8: 按照 socre 排序选出评分最大的边界框 B 并取出；</li>
<li>步骤9: 此时边界框的个数为零，结束迭代。</li>
</ul>
<p>最后我们得到了边界框 A、B、D，但其中边界框 B 的评分非常低，这表明该边界框是没有物体的，因此应当抛弃掉。在代码中:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># # (5) discard some boxes with low scores</span></span><br><span class="line">classes = np.argmax(pred_prob, axis=-<span class="number">1</span>)</span><br><span class="line">scores = pred_conf * pred_prob[np.arange(<span class="built_in">len</span>(pred_coor)), classes]</span><br><span class="line">score_mask = scores &gt; score_threshold</span><br></pre></td></tr></table></figure>

<blockquote>
<p>在 YOLO 算法中，NMS 的处理有两种情况：一种是所有的预测框一起做 NMS 处理，另一种情况是分别对每个类别的预测框做 NMS 处理。后者会出现一个预测框既属于类别 A 又属于类别 B 的现象，这比较适合于一个小单元格中同时存在多个物体的情况。</p>
</blockquote>
<h2 id="anchor-响应机制"><a href="#anchor-响应机制" class="headerlink" title="anchor 响应机制"></a>anchor 响应机制</h2><h3 id="K-means-聚类"><a href="#K-means-聚类" class="headerlink" title="K-means 聚类"></a>K-means 聚类</h3><p>首先需要抛出一个问题：先验框 anchor 是怎么来的？对于这点，作者在 YOLOv2 论文里给出了很好的解释：</p>
<blockquote>
<p>we run k-means clustering on the training set bounding boxes to automatically find good priors.</p>
</blockquote>
<p>其实就是使用 k-means 算法对训练集上的 boudnding box 尺度做聚类。此外，考虑到训练集上的图片尺寸不一，因此对此过程进行归一化处理。</p>
<p>k-means 聚类算法有个坑爹的地方在于，类别的个数需要人为事先指定。这就带来一个问题，先验框 anchor 的数目等于多少最合适？一般来说，anchor 的类别越多，那么 YOLO 算法就越能在不同尺度下与真实框进行回归，但是这样就会导致模型的复杂度更高，网络的参数量更庞大。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/k-means.png">
</p>

<blockquote>
<p>We choose k = 5 as a good tradeoff between model complexity and high recall. If we use 9 centroids we see a much higher average IOU. This indicates that using k-means to generate our bounding box starts the model off with a better representation and makes the task easier to learn.</p>
</blockquote>
<p>在上面这幅图里，作者发现 k = 5 时就能较好地实现高召回率与模型复杂度之间的平衡。由于在 YOLOv3 算法里一共有3种尺度预测，因此只能是3的倍数，所以最终选择了 9 个先验框。这里还有个问题需要解决，k-means 度量距离的选取很关键。距离度量如果使用标准的欧氏距离，大框框就会比小框产生更多的错误。在目标检测领域，我们度量两个边界框之间的相似度往往以 IOU 大小作为标准。因此，这里的度量距离也和 IOU 有关。需要特别注意的是，这里的IOU计算只用到了 boudnding box 的长和宽。在我的代码里，是认为两个先验框的左上角位置是相重合的。(其实在这里偏移至哪都无所谓，因为聚类的时候是不考虑 anchor 框的位置信息的。)</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/MommyTalk1600745437831.jpg">
</p>

<p>如果两个边界框之间的IOU值越大，那么它们之间的距离就会越小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span>(<span class="params">boxes, k, dist=np.median,seed=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculates k-means clustering with the Intersection over Union (IoU) metric.</span></span><br><span class="line"><span class="string">    :param boxes: numpy array of shape (r, 2), where r is the number of rows</span></span><br><span class="line"><span class="string">    :param k: number of clusters</span></span><br><span class="line"><span class="string">    :param dist: distance function</span></span><br><span class="line"><span class="string">    :return: numpy array of shape (k, 2)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    rows = boxes.shape[<span class="number">0</span>]</span><br><span class="line">    distances     = np.empty((rows, k)) <span class="comment">## N row x N cluster</span></span><br><span class="line">    last_clusters = np.zeros((rows,))</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    <span class="comment"># initialize the cluster centers to be k items</span></span><br><span class="line">    clusters = boxes[np.random.choice(rows, k, replace=<span class="literal">False</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 为每个点指定聚类的类别（如果这个点距离某类别最近，那么就指定它是这个类别)</span></span><br><span class="line">        <span class="keyword">for</span> icluster <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            distances[:,icluster] = <span class="number">1</span> - iou(clusters[icluster], boxes)</span><br><span class="line">        nearest_clusters = np.argmin(distances, axis=<span class="number">1</span>)</span><br><span class="line">	<span class="comment"># 如果聚类簇的中心位置基本不变了，那么迭代终止。</span></span><br><span class="line">        <span class="keyword">if</span> (last_clusters == nearest_clusters).<span class="built_in">all</span>():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 重新计算每个聚类簇的平均中心位置，并它作为聚类中心点</span></span><br><span class="line">        <span class="keyword">for</span> cluster <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            clusters[cluster] = dist(boxes[nearest_clusters == cluster], axis=<span class="number">0</span>)</span><br><span class="line">        last_clusters = nearest_clusters</span><br><span class="line">    <span class="keyword">return</span> clusters,nearest_clusters,distances</span><br></pre></td></tr></table></figure>

<h3 id="正负样本分配"><a href="#正负样本分配" class="headerlink" title="正负样本分配"></a>正负样本分配</h3><ul>
<li>如果 Anchor 与 Ground-truth Bounding Boxes 的 IoU &gt; 0.3，标定为正样本;</li>
<li>在第 1 种规则下基本能够产生足够多的样本，但是如果它们的 iou 不大于 0.3，那么只能把 iou 最大的那个 Anchor 标记为正样本，这样便能保证每个 Ground-truth 框都至少匹配一个先验框。</li>
</ul>
<p>按照上述原则，一个 ground-truth 框会同时与多个先验框进行匹配。记得之前有人问过我，为什么不能只用 iou 最大的 anchor 去负责预测该物体？其实我想回答的是，如果按照这种原则去分配正负样本，那么势必会导致正负样本的数量极其不均衡（正样本特别少，负样本特别多），这将使得模型在预测时会出现大量漏检的情况。实际上很多目标检测网络都会避免这种情况，并且尽量保持正负样本的数目相平衡。例如，SSD 网络就使用了 hard negative mining 的方法对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差较大的 top-k 作为训练的负样本，以保证正负样本的比例接近1:3。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>): <span class="comment"># 针对 3 种网格尺寸</span></span><br><span class="line">    <span class="comment"># 设定变量，用于存储每种网格尺寸下 3 个 anchor 框的中心位置和宽高</span></span><br><span class="line">    anchors_xywh = np.zeros((self.anchor_per_scale, <span class="number">4</span>))</span><br><span class="line">    <span class="comment"># 将这 3 个 anchor 框都偏移至网格中心</span></span><br><span class="line">    anchors_xywh[:, <span class="number">0</span>:<span class="number">2</span>] = np.floor(bbox_xywh_scaled[i, <span class="number">0</span>:<span class="number">2</span>]).astype(np.int32) + <span class="number">0.5</span></span><br><span class="line">    <span class="comment"># 填充这 3 个 anchor 框的宽和高</span></span><br><span class="line">    anchors_xywh[:, <span class="number">2</span>:<span class="number">4</span>] = self.anchors[i]</span><br><span class="line">    <span class="comment"># 计算真实框与 3 个 anchor 框之间的 iou 值</span></span><br><span class="line">    iou_scale = self.bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)</span><br><span class="line">    iou.append(iou_scale)</span><br><span class="line">    <span class="comment"># 找出 iou 值大于 0.3 的 anchor 框</span></span><br><span class="line">    iou_mask = iou_scale &gt; <span class="number">0.3</span></span><br><span class="line">    exist_positive = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">any</span>(iou_mask): <span class="comment"># 规则 1: 对于那些 iou &gt; 0.3 的 anchor 框，做以下处理</span></span><br><span class="line">    	<span class="comment"># 根据真实框的坐标信息来计算所属网格左上角的位置</span></span><br><span class="line">        xind, yind = np.floor(bbox_xywh_scaled[i, <span class="number">0</span>:<span class="number">2</span>]).astype(np.int32)</span><br><span class="line">        label[i][yind, xind, iou_mask, :] = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 填充真实框的中心位置和宽高</span></span><br><span class="line">        label[i][yind, xind, iou_mask, <span class="number">0</span>:<span class="number">4</span>] = bbox_xywh</span><br><span class="line">        <span class="comment"># 设定置信度为 1.0，表明该网格包含物体</span></span><br><span class="line">        label[i][yind, xind, iou_mask, <span class="number">4</span>:<span class="number">5</span>] = <span class="number">1.0</span></span><br><span class="line">        <span class="comment"># 设置网格内 anchor 框的类别概率，做平滑处理</span></span><br><span class="line">        label[i][yind, xind, iou_mask, <span class="number">5</span>:] = smooth_onehot</span><br><span class="line">        exist_positive = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exist_positive: <span class="comment"># 规则 2: 所有 iou 都不大于0.3， 那么只能选择 iou 最大的</span></span><br><span class="line">    	best_anchor_ind = np.argmax(np.array(iou).reshape(-<span class="number">1</span>), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>按照上面两种规则标记出正样本后，剩下的都是负样本了。这些负样本是不会参与到边界框损失和分类损失的计算中去，而只会参与到置信度损失的计算（因为你需要告诉神经网络什么是负样本）。在这里，你不必纠结 Anchor 是否能够准确地框到物体。你只要关心 Anchor 能不能框到物体，如果框到很多了(比如iou&gt;0.3)，那么它就是个正样本了，否则就不是了。 后面的损失函数会进一步告诉神经网络怎么去做精确的尺寸和位置回归，并给出一个置信度评分。最后，那些评分比较低和重叠度较高的预测框就会被 NMS 算法给过滤掉。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>在 YOLOv3 中，作者将目标检测任务看作目标区域预测和类别预测的回归问题, 因此它的损失函数也有些与众不同。对于损失函数, Redmon J 在论文中并 没有进行详细的讲解。但通过对 darknet 源代码的解读，可以总结得到 YOLOv3 的损失函数如下:</p>
<ul>
<li>置信度损失，判断预测框有无物体；</li>
<li>框回归损失，仅当预测框内包含物体时计算；</li>
<li>分类损失，判断预测框内的物体属于哪个类别</li>
</ul>
<h3 id="置信度损失"><a href="#置信度损失" class="headerlink" title="置信度损失"></a>置信度损失</h3><p>YOLOv3 直接优化置信度损失是为了让模型去学习分辨图片的背景和前景区域，这类似于在 Faster rcnn 里 RPN 功能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])</span><br><span class="line"><span class="comment"># 找出与真实框 iou 值最大的预测框</span></span><br><span class="line">max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-<span class="number">1</span>), axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 如果最大的 iou 小于阈值，那么认为该预测框不包含物体,则为背景框</span></span><br><span class="line">respond_bgd = (<span class="number">1.0</span> - respond_bbox) * tf.cast( max_iou &lt; IOU_LOSS_THRESH, tf.float32 )</span><br><span class="line">conf_focal = tf.<span class="built_in">pow</span>(respond_bbox - pred_conf, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 计算置信度的损失（我们希望假如该网格中包含物体，那么网络输出的预测框置信度为 1，无物体时则为 0。</span></span><br><span class="line">conf_loss = conf_focal * (</span><br><span class="line">     respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)</span><br><span class="line">            +</span><br><span class="line">     respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>判定的规则很简单：如果一个预测框与所有真实框的 iou 都小于某个阈值，那么就判定它是背景，否则为前景（包含物体）。</p>
<h3 id="分类损失"><a href="#分类损失" class="headerlink" title="分类损失"></a>分类损失</h3><p>这里分类损失采用的是二分类的交叉熵，即把所有类别的分类问题归结为是否属于这个类别，这样就把多分类看做是二分类问题。这样做的好处在于排除了类别的互斥性，特别是解决了因多个类别物体的重叠而出现漏检的问题。</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/MommyTalk1600745936778.jpg">
</p>


<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">respond_bbox  = label[:, :, :, :, <span class="number">4</span>:<span class="number">5</span>]</span><br><span class="line">prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)</span><br></pre></td></tr></table></figure>

<h3 id="框回归损失"><a href="#框回归损失" class="headerlink" title="框回归损失"></a>框回归损失</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">respond_bbox  = label[:, :, :, :, <span class="number">4</span>:<span class="number">5</span>]  <span class="comment"># 置信度，判断网格内有无物体</span></span><br><span class="line">...</span><br><span class="line">bbox_loss_scale = <span class="number">2.0</span> - <span class="number">1.0</span> * label_xywh[:, :, :, :, <span class="number">2</span>:<span class="number">3</span>] * label_xywh[:, :, :, :, <span class="number">3</span>:<span class="number">4</span>] / (input_size ** <span class="number">2</span>)</span><br><span class="line">giou_loss = respond_bbox * bbox_loss_scale * (<span class="number">1</span> - giou)</span><br></pre></td></tr></table></figure>

<ul>
<li>边界框的尺寸越小，bbox_loss_scale 的值就越大。实际上，我们知道 YOLOv1 里作者在 loss 里对宽高都做了开根号处理，这是为了弱化边界框尺寸对损失值的影响；</li>
<li>respond_bbox 的意思是如果网格单元中包含物体，那么就会计算边界框损失；</li>
<li>两个边界框之间的 GIoU 值越大，giou 的损失值就会越小, 因此网络会朝着预测框与真实框重叠度较高的方向去优化。</li>
</ul>
<p>受 g-darknet 所启示，将原始 iou loss 替换成了 giou loss ，检测精度提高了大约 1 个百分点。 GIoU 的好处在于，改进了预测框与先验框的距离度量方式。</p>
<h4 id="GIoU-的背景介绍"><a href="#GIoU-的背景介绍" class="headerlink" title="GIoU 的背景介绍"></a>GIoU 的背景介绍</h4><p>这篇论文 出自于 CVPR 2019，这篇论文提出了一种优化边界框的新方式 —— GIoU (Generalized IoU，广义 IoU )。边界框一般由左上角和右下角坐标所表示，即 (x1,y1,x2,y2)。那么，你发现这其实也是一个向量。向量的距离一般可以 L1 范数或者 L2 范数来度量。但是在L1及L2范数取到相同的值时，实际上检测效果却是差异巨大的，直接表现就是预测和真实检测框的IoU值变化较大，这说明L1和L2范数不能很好的反映检测效果。</p>
<blockquote>
<p>L1 范数：向量元素的绝对值之和；<br>L2 范数：即欧几里德范数，常用于计算向量的长度；</p>
</blockquote>
<p align="center">
    <img width="80%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/giou.png">
</p>

<p>当 L1 或 L2 范数都相同的时候，发现 IoU 和 GIoU 的值差别都很大，这表明使用 L 范数来度量边界框的距离是不合适的。在这种情况下，学术界普遍使用 IoU 来衡量两个边界框之间的相似性。作者发现使用 IoU 会有两个缺点，导致其不太适合做损失函数:</p>
<ul>
<li>预测框和真实框之间没有重合时，IoU 值为 0， 导致优化损失函数时梯度也为 0，意味着无法优化。例如，场景 A 和场景 B 的 IoU 值都为 0，但是显然场景 B 的预测效果较 A 更佳，因为两个边界框的距离更近( L 范数更小)。</li>
</ul>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/giou_AB.png">
</p>

<blockquote>
<p>尽管场景 A 和场景 B 的 IoU 值都为 0，但是场景 B 的预测效果较 A 更佳，这是因为两个边界框的距离更近。</p>
</blockquote>
<ul>
<li>即使预测框和真实框之间相重合且具有相同的 IoU 值时，检测的效果也具有较大差异，如下图所示。</li>
</ul>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/gt_pred.png">
</p>

<blockquote>
<p>上面三幅图的 IoU = 0.33， 但是 GIoU 值分别是 0.33, 0.24 和 -0.1， 这表明如果两个边界框重叠和对齐得越好，那么得到的 GIoU 值就会越高。</p>
</blockquote>
<h4 id="GIoU-的计算公式"><a href="#GIoU-的计算公式" class="headerlink" title="GIoU 的计算公式"></a>GIoU 的计算公式</h4><p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/giou_algorithm.png">
</p>

<p>the smallest enclosing convex object C 指的是最小闭合凸面 C，例如在上述场景 A 和 B 中，C 的形状分别为:</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/new_AB.png">
</p>

<blockquote>
<p>图中绿色包含的区域就是最小闭合凸面 C，the smallest enclosing convex object。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_giou</span>(<span class="params">boxes1, boxes2</span>):</span></span><br><span class="line">	......</span><br><span class="line">    <span class="comment"># 计算两个边界框之间的 iou 值</span></span><br><span class="line">    iou = inter_area / union_area</span><br><span class="line">    <span class="comment"># 计算最小闭合凸面 C 左上角和右下角的坐标</span></span><br><span class="line">    enclose_left_up = tf.minimum(boxes1[..., :<span class="number">2</span>], boxes2[..., :<span class="number">2</span>])</span><br><span class="line">    enclose_right_down = tf.maximum(boxes1[..., <span class="number">2</span>:], boxes2[..., <span class="number">2</span>:])</span><br><span class="line">    enclose = tf.maximum(enclose_right_down - enclose_left_up, <span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># 计算最小闭合凸面 C 的面积</span></span><br><span class="line">    enclose_area = enclose[..., <span class="number">0</span>] * enclose[..., <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 根据 GIoU 公式计算 GIoU 值</span></span><br><span class="line">    giou = iou - <span class="number">1.0</span> * (enclose_area - union_area) / enclose_area</span><br><span class="line">    <span class="keyword">return</span> giou</span><br></pre></td></tr></table></figure>

<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p>训练神经网络尤其是深度神经网络所面临的一个问题是，梯度消失或梯度爆炸，也就是说 当你训练深度网络时，导数或坡度有时会变得非常大，或非常小甚至以指数方式变小，这个时候我们看到的损失就会变成了 NaN。假设你正在训练下面这样一个极深的神经网络，为了简单起见，这里激活函数 g(z) = z 并且忽略偏置参数。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/x1x2.png">
</p>

<p>这里我们首先假定 g(z)=z, b[l]=0，所以对目标输出有：</p>
<p align="center">
    <img width="35%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/MommyTalk1600746076683.jpg">
</p>

<p>其实这里直观的理解是：如果权重 W 只比 1 略大一点，或者说只比单位矩阵大一点，深度神经网络的输出将会以爆炸式增长，而如果 W 比 1 略小一点，可能是 0.9, 0.9，每层网络的输出值将会以指数级递减。因此合适的初始化权重值就显得尤为重要! 下面就写个简单的代码给大家演示一下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.random.randn(<span class="number">2000</span>, <span class="number">800</span>) * <span class="number">0.01</span> <span class="comment"># 制作输入数据</span></span><br><span class="line">stds = [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.01</span>, <span class="number">0.005</span>, <span class="number">0.001</span>] <span class="comment"># 尝试使用不同标准差，这样初始权重大小也不一样</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, std <span class="keyword">in</span> <span class="built_in">enumerate</span>(stds):</span><br><span class="line">    <span class="comment"># 第一层全连接层</span></span><br><span class="line">    dense_1 = tf.keras.layers.Dense(<span class="number">750</span>, kernel_initializer=tf.random_normal_initializer(std), activation=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">    output_1 = dense_1(x)</span><br><span class="line">    <span class="comment"># 第二层全连接层</span></span><br><span class="line">    dense_2 = tf.keras.layers.Dense(<span class="number">700</span>, kernel_initializer=tf.random_normal_initializer(std), activation=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">    output_2 = dense_2(output_1)</span><br><span class="line">    <span class="comment"># 第三层全连接层</span></span><br><span class="line">    dense_3 = tf.keras.layers.Dense(<span class="number">650</span>, kernel_initializer=tf.random_normal_initializer(std), activation=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">    output_3 = dense_3(output_2).numpy().flatten()</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="built_in">len</span>(stds), i+<span class="number">1</span>)</span><br><span class="line">    plt.hist(output_3, bins=<span class="number">60</span>, <span class="built_in">range</span>=[-<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;std = %.3f&#x27;</span> %std)</span><br><span class="line">    plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/NAN.png">
</p>

<p>我们可以看到当标准差较大( std = 0.1 和 0.05 )时，几乎所有的输出值集中在 -1 或1 附近，这表明此时的神经网络发生了梯度爆炸；当标准差较小( std = 0.005 和 0.001）时，我们看到输出值迅速向 0 靠拢，这表明此时的神经网络发生了梯度消失。其实笔者也曾在 YOLOv3 网络里做过实验，初始化权重的标准差如果太大或太小，都容易出现 NaN 。</p>
<h3 id="学习率的设置"><a href="#学习率的设置" class="headerlink" title="学习率的设置"></a>学习率的设置</h3><p>学习率是最影响性能的超参数之一，如果我们只能调整一个超参数，那么最好的选择就是它。 其实在我们的大多数的炼丹过程中，遇到 loss 变成 NaN 的情况大多数是由于学习率选择不当引起的。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/lr.png">
</p>


<p>有句话讲得好啊，步子大了容易扯到蛋。由于神经网络在刚开始训练的时候是非常不稳定的，因此刚开始的学习率应当设置得很低很低，这样可以保证网络能够具有良好的收敛性。但是较低的学习率会使得训练过程变得非常缓慢，因此这里会采用以较低学习率逐渐增大至较高学习率的方式实现网络训练的“热身”阶段，称为 warmup stage。但是如果我们使得网络训练的 loss 最小，那么一直使用较高学习率是不合适的，因为它会使得权重的梯度一直来回震荡，很难使训练的损失值达到全局最低谷。因此最后采用了这篇论文里[8]的 cosine 的衰减方式，这个阶段可以称为 consine decay stage。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> global_steps &lt; warmup_steps:</span><br><span class="line">    lr = global_steps / warmup_steps *cfg.TRAIN.LR_INIT</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    lr = cfg.TRAIN.LR_END + <span class="number">0.5</span> * (cfg.TRAIN.LR_INIT - cfg.TRAIN.LR_END) * (</span><br><span class="line">        (<span class="number">1</span> + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/yolov3_lr.png">
</p>

<h3 id="加载预训练模型"><a href="#加载预训练模型" class="headerlink" title="加载预训练模型"></a>加载预训练模型</h3><p>目前针对目标检测的主流做法是基于 Imagenet 数据集预训练的模型来提取特征，然后在 COCO 数据集进行目标检测fine-tunning训练（比如 yolo 算法)，也就是大家常说的迁移学习。其实迁移学习是建立在数据集分布相似的基础上的，像 yymnist 这种与 COCO 数据集分布完全不同的情况，就没有必要加载 COCO 预训练模型的必要了吧。</p>
<p>在 tensorflow-yolov3 版本里，由于 README 里训练的是 VOC 数据集，因此推荐加载预训练模型。由于在 YOLOv3 网络的三个分支里的最后卷积层与训练的类别数目有关，因此除掉这三层的网络权重以外，其余所有的网络权重都加载进来了。</p>
<p>下面是 tensorflow-yolov3 在 PASCAL VOC 2012 上比赛刷的成绩，最后进了榜单的前十名。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/yolov3_mAP.png">
</p>


<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. <a href="https://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation</a>, CVPR 2014</li>
<li>[2] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. <a href="https://arxiv.org/abs/1506.01497">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>, CVPR 2016</li>
<li>[3] Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. <a href="https://arxiv.org/abs/1506.02640">You Only Look Once: Unified, Real-Time Object Detection</a>, CVPR 2016</li>
<li>[4] Joseph Redmon, Ali Farhadi. <a href="https://arxiv.org/abs/1612.08242">YOLO9000: Better, Faster, Stronger</a>, CVPR 2017</li>
<li>[5] Joseph Redmon, Ali Farhadi. <a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">YOLOv3: An Incremental Improvement</a></li>
<li>[6] Conv2DTranspose 层，<a href="https://keras-cn.readthedocs.io/en/latest/layers/convolutional_layer/">Keras 中文文档</a>.</li>
<li>[7] Rezatofighi, Hamid. <a href="https://arxiv.org/pdf/1902.09630.pdf">Generalized Intersection over Union, A Metric and A Loss for Bounding Box Regression</a>, CVPR 2018</li>
<li>[8] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li. <a href="https://arxiv.org/pdf/1812.01187.pdf">Bag of Tricks for Image Classification with Convolutional Neural Networks</a></li>
</ul>
]]></content>
      <categories>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>YOLOv3</tag>
      </tags>
  </entry>
</search>
