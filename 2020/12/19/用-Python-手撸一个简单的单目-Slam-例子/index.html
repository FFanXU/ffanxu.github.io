<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>用 Python 手撸一个单目视觉里程计的例子 | 四一的世界</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">用 Python 手撸一个单目视觉里程计的例子</h1><a id="logo" href="/.">四一的世界</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">用 Python 手撸一个单目视觉里程计的例子</h1><div class="post-meta">2020-12-19<span> | </span><span class="category"><a href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 3.5k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 15</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>最近因工作需要，开始接触到一些关于 SLAM（Simultaneous Localization and Mapping）的研究。网上关于 slam 的资料有很多，譬如高博的十四讲，github 上的 VINS 等等。但是他们大多是用 C++ 写的，并且环境依赖复杂。今天， 我使用 Python 手撸了一个简单的单目 slam，对 slam 有了一个初步的认识。完整的代码在<a target="_blank" rel="noopener" href="https://github.com/YunYang1994/openwork/tree/main/monocular_slam">这里</a>。</p>
<p align="center">
<iframe src="//player.bilibili.com/player.html?aid=245798532&bvid=BV1Tv411t7aN&cid=270731969&page=1"  width="600" height="400"  scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</p>

<span id="more"></span>

<h2 id="ORB-特征点检测"><a href="#ORB-特征点检测" class="headerlink" title="ORB 特征点检测"></a>ORB 特征点检测</h2><p>ORB 特征由<strong><font color=Red>关键点</font></strong>和<strong><font color=Red>描述子</font></strong>两部分组成，它的关键点称为 “Oriented FAST”，是一种改进的 FAST 角点，而描述子则称为 BRIEF。在 OpenCV 中，我们可以这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">orb = cv2.ORB_create()</span><br><span class="line">image = cv2.cvtColor(frame.image, cv2.COLOR_BGR2GRAY)</span><br><span class="line"><span class="comment"># detection corners</span></span><br><span class="line">pts = cv2.goodFeaturesToTrack(image, <span class="number">3000</span>, qualityLevel=<span class="number">0.01</span>, minDistance=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># extract features</span></span><br><span class="line">kps = [cv2.KeyPoint(x=pt[<span class="number">0</span>][<span class="number">0</span>], y=pt[<span class="number">0</span>][<span class="number">1</span>], _size=<span class="number">20</span>) <span class="keyword">for</span> pt <span class="keyword">in</span> pts]</span><br><span class="line">kps, des = orb.compute(image, kps)</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/sample_01.gif">
</p>

<p>我们首先需要将图片转成灰度图， 然后利用 <code>goodFeaturesToTrack</code> 找出图片中的高质量的角点， 接着使用 <code>orb</code> 里的 <code>compuete</code> 函数计算出这些角点的特征：它会返回 <code>kps</code> 和 <code>des</code>，<code>kps</code> 给出了角点在图像中坐标，而 <code>des</code> 则是这些角点的描述子，一般为 32 维的特征向量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frame 1: kps[0]=(294.0, 217.0), des[0]=[ 66 245  18 ...  39 206]</span><br></pre></td></tr></table></figure>

<h2 id="特征点匹配"><a href="#特征点匹配" class="headerlink" title="特征点匹配"></a>特征点匹配</h2><p>特征点匹配的意思就是将本帧检测的所有角点和上一帧的角点进行匹配，因此需要将上一帧的角点 <code>last_kps</code>  和描述子 <code>last_des</code>  存储起来。此外还需要 <code>idx</code> 记录每帧的序列号，并且从第二帧才开始做匹配。我们构造了一个 Frame 类，并将它们定义为类的属性，在实例初始化的时候再将这些属性传递给对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Frame</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    last_kps, last_des, last_pose = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, image</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        只要一经初始化，Frame 就会把上一帧的信息传递给下一帧</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        Frame.idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        self.image = image</span><br><span class="line">        self.idx   = Frame.idx</span><br><span class="line">        self.last_kps  = Frame.last_kps</span><br><span class="line">        self.last_des  = Frame.last_des</span><br><span class="line">        self.last_pose = Frame.last_pose</span><br></pre></td></tr></table></figure>

<p>我直接使用了暴力匹配（Brute-Force）的方法对两帧图片的角点进行配准，通过 <code>cv2.BFMatcher</code> 可以创建一个匹配器 <code>bfmatch</code>，它有两个可选的参数：</p>
<ul>
<li><code>normType</code>：度量两个角点之间距离的方式，由于 ORB 是一种基于二进制字符串的描述符，因此可以选择汉明距离 (<code>cv2.NORM_HAMMING</code>)。</li>
<li><code>crossCheck</code>：为布尔变量，默认值为 False。如果设置为 True，匹配条件就会更加严格，只有当两个特征点互相为最佳匹配时才可以。</li>
</ul>
<p>使用 <code>knnMatch()</code>  可以为每个关键点返回 k 个最佳匹配（将序排列之后取前 k 个），其中 k 是用户自己设定的，这里设置成 k=2。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bfmatch = cv2.BFMatcher(cv2.NORM_HAMMING)</span><br><span class="line">matches = bfmatch.knnMatch(frame.curr_des, frame.last_des, k=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>将 <code>frame.curr_des</code> 、 <code>frame.last_des</code> 和 <code>matches</code> 的数量打印出来：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">frame: 16, curr_des: 1660, last_des: 1597, matches: 1660</span><br><span class="line">frame: 17, curr_des: 1484, last_des: 1660, matches: 1484</span><br></pre></td></tr></table></figure>

<p>我们发现 <code>matches</code> 的数量始终与 <code>frame.curr_des</code> 相等，这是因为这里是从上一帧 <code>frame.last_des</code> 给当前帧 <code>frame.curr_des</code> 找最佳匹配点，并且每个 <code>match</code> 返回的是 2 个 <code>DMatch</code> 对象，它们具有以下属性：</p>
<ul>
<li><code>DMatch.distance</code> ：关键点之间的距离，越小越好。</li>
<li><code>DMatch.trainIdx</code> ：目标图像中描述符的索引。</li>
<li><code>DMatch.queryIdx</code> ：查询图像中描述符的索引。</li>
<li><code>DMatch.imgIdx</code>：目标图像的索引。</li>
</ul>
<p>如果第一名的距离小于第二名距离的 75%，那么将认为第一名大概率是匹配上了，此时 <code>m.queryIdx</code> 为当前帧关键点的索引，<code>m.trainIdx</code> 为上一帧关键点的索引，<code>match_kps</code> 返回的是每对配准点的位置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> m,n <span class="keyword">in</span> matches:</span><br><span class="line">    <span class="keyword">if</span> m.distance &lt; <span class="number">0.75</span>*n.distance:</span><br><span class="line">        idx1.append(m.queryIdx)</span><br><span class="line">        idx2.append(m.trainIdx)</span><br><span class="line"></span><br><span class="line">        p1 = frame.curr_kps[m.queryIdx]     <span class="comment"># 当前帧配准的角点位置</span></span><br><span class="line">        p2 = frame.last_kps[m.trainIdx]     <span class="comment"># 上一帧配置的角点位置</span></span><br><span class="line">        match_kps.append((p1, p2))</span><br></pre></td></tr></table></figure>

<p>在下图中：红色的是当前帧的关键点，蓝色的是当前帧关键点的位置与上一帧关键点位置的连线。由于 🚗 是向前行驶，因此关键点相对 🚗 是往后运动的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> kp1, kp2 <span class="keyword">in</span> <span class="built_in">zip</span>(frame.curr_kps, frame.last_kps):</span><br><span class="line">    u1, v1 = <span class="built_in">int</span>(kp1[<span class="number">0</span>]), <span class="built_in">int</span>(kp1[<span class="number">1</span>])</span><br><span class="line">    u2, v2 = <span class="built_in">int</span>(kp2[<span class="number">0</span>]), <span class="built_in">int</span>(kp2[<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 用圆圈画出当前帧角点的位置</span></span><br><span class="line">    cv2.circle(frame.image, (u1, v1), color=(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), radius=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 用直线追踪角点的行动轨迹</span></span><br><span class="line">    cv2.line(frame.image, (u1, v1), (u2, v2), color=(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/3.jpg">
</p>

<h2 id="RANSAC-去噪和本质矩阵"><a href="#RANSAC-去噪和本质矩阵" class="headerlink" title="RANSAC 去噪和本质矩阵"></a>RANSAC 去噪和本质矩阵</h2><p>RANSAC (RAndom SAmple Consensus, 随机采样一致) 算法是从一组含有 “外点” (outliers) 的数据中正确估计数学模型参数的迭代算法。RANSAC 算法有 2 个基本的假设：</p>
<ul>
<li>假设数据是由“内点”和“外点”组成的。“内点”就是组成模型参数的数据，“外点”就是不适合模型的异常值，通常是那些估计曲线以外的离群点。</li>
<li>假设在给定一组含有少部分“内点”的数据中，存在一个模型可以估计出符合“内点”变化的规律。</li>
</ul>
<p>具体的细节这里不再展开，感兴趣的话可以看<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/62238520">这里</a>，这里是直接使用三方库里的 <code>scikit-image</code> 里的 <code>ransac</code> 算法进行求解。由于我们在求解本质矩阵的时候，<strong><font color=Red>需要利用相机内参将角点的像素坐标进行归一化：</font></strong></p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/0.png">
</p>

<p>其中 <code>p1</code> 和 <code>p2</code> 分别为配对角点在图片上的像素位置，那么归一化的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">pts</span>):</span></span><br><span class="line">    Kinv = np.linalg.inv(K)</span><br><span class="line">    <span class="comment"># turn [[x,y]] -&gt; [[x,y,1]]</span></span><br><span class="line">    add_ones = <span class="keyword">lambda</span> x: np.concatenate([x, np.ones((x.shape[<span class="number">0</span>], <span class="number">1</span>))], axis=<span class="number">1</span>)</span><br><span class="line">    norm_pts = np.dot(Kinv, add_ones(pts).T).T[:, <span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> norm_pts</span><br></pre></td></tr></table></figure>

<p>slam 知识里给出了本质矩阵和归一化坐标之间的关系，它可以用一个简洁的公式来表达：</p>
<p align="center">
    <img width="15%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/2.jpg">
</p>

<p>其中本质矩阵 <strong>E</strong> 是平移向量 <strong>t</strong> 和旋转矩阵 <strong>R</strong> 的外积：</p>
<p align="center">
    <img width="12%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/7.jpg">
</p>

<p>本质矩阵 <code>E</code> 是一个 <code>3x3</code> 的矩阵，有 9 个未知元素。然而，上面的公式中 <code>x</code> 使用的是齐次坐标（已经有一个已知的 <code>1</code>）。而齐次坐标在相差一个常数因子下是相等，因此在单位尺度下只需 8 个点即可求解。</p>
<p align="center">
    <img width="48%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/1.jpg">
</p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_essential_matrix</span>(<span class="params">match_kps</span>):</span></span><br><span class="line">    match_kps = np.array(match_kps)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用相机内参对角点坐标归一化</span></span><br><span class="line">    norm_curr_kps = normalize(match_kps[:, <span class="number">0</span>])</span><br><span class="line">    norm_last_kps = normalize(match_kps[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求解本质矩阵和内点数据</span></span><br><span class="line">    model, inliers = ransac((norm_curr_kps, norm_last_kps),</span><br><span class="line">                            EssentialMatrixTransform,</span><br><span class="line">                            min_samples=<span class="number">8</span>,              <span class="comment"># 最少需要 8 个点</span></span><br><span class="line">                            residual_threshold=<span class="number">0.005</span>,</span><br><span class="line">                            max_trials=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    frame.curr_kps = frame.curr_kps[inliers]   <span class="comment"># 保留当前帧的内点数据</span></span><br><span class="line">    frame.last_kps = frame.last_kps[inliers]   <span class="comment"># 保留上一帧的内点数据</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model.params       <span class="comment"># 返回本质矩阵</span></span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/4.jpg">
</p>

<blockquote>
<p>可以看到，经过 RANSAC 去燥后，噪点数据消失了很多，角点的追踪情况基本稳定。但是经过筛选后，角点的数量只有原来的三分之一左右了。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">frame: 46, curr_des: 1555, last_des: 1467, match_kps: 549</span><br><span class="line">---------------- Essential Matrix ----------------</span><br><span class="line">[[-2.86637732e-04 -1.16930419e+00  1.12798916e-01]</span><br><span class="line"> [ 1.16673848e+00 -2.40819717e-03 -2.60028204e-01]</span><br><span class="line"> [-1.10221539e-01  2.67480554e-01 -1.20159639e-03]]</span><br></pre></td></tr></table></figure>

<h2 id="本质矩阵分解"><a href="#本质矩阵分解" class="headerlink" title="本质矩阵分解"></a>本质矩阵分解</h2><p>接下来的问题是如何根据已经估计得到的本质矩阵 <strong>E</strong>，恢复出相机的运动 <strong>R</strong>，<strong>t</strong>。这个过程是由奇异值分解得到的：</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/6.png">
</p>

<p>我们发现对角矩阵 <code>diag([1, 1, 0])</code> 可以由 <code>Z</code> 和 <code>W</code> 拆分得到。</p>
<p align="center">
    <img width="38%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/9.png">
</p>

<p align="center">
    <img width="53%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/8.png">
</p>

<p>将 <code>Z</code> 和 <code>W</code> 代入进来，令 <code>E = S R</code>。可以分解成两种情况：</p>
<ul>
<li>情况 1:</li>
</ul>
<p align="center">
    <img width="65%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/11.jpg">
</p>

<p align="center">
    <img width="32%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/13.jpg">
</p>

<ul>
<li>情况 2:</li>
</ul>
<p align="center">
    <img width="78%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/12.jpg">
</p>

<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/15.jpg">
</p>

<p>我们发现，此时已经将旋转矩阵 <code>R</code> 分离出来了，它有两种情况：分别等于 <code>R1</code> 和 <code>R2</code>。接下来我们需要考虑平移向量 <strong>t</strong>，可以证明出 <strong>t</strong> 其实是在 <strong><code>S</code></strong> 的零向量空间里，因为：</p>
<p align="center">
    <img width="18%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/16.jpg">
</p>

<p>结合线性代数的知识，不难求出 <code>t = U * (0, 0, 1) = u3</code> (即 <code>U</code> 的最后一列)。考虑到给 <code>t</code> 乘以一个非零尺度因子 <code>λ</code>， 对于 <code>E</code> 而言这种情况依旧有效，而对于 <code>t</code> 而言， 当 <code>λ = ± 1</code> 时，它们物理的意义（方向）却是不同的。综上，在已知第一个相机矩阵 <code>P = [ I ∣ 0 ]</code> 的情况下，第二个相机矩阵 <code>P′</code> 有如下 4 种可能的解：</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/17.jpg">
</p>

<p align="center">
    <img width="100%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/18.png">
</p>

<blockquote>
<p>我们发现上面 4 种解其实是 2 种 R 和 2 种 t 之间的排列组合，只有当点 P 位于两个相机前方时才具有正深度，即 (1) 才是唯一正确解。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/opencv/opencv/blob/3.1.0/modules/calib3d/src/five-point.cpp#L617">OpenCV</a> 提供了从本质矩阵中恢复相机的 <code>Rt</code> 的方法：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">cv::decomposeEssentialMat</span><span class="params">( InputArray _E, OutputArray _R1, OutputArray _R2, OutputArray <span class="keyword">_t</span> )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Mat E = _E.<span class="built_in">getMat</span>().<span class="built_in">reshape</span>(<span class="number">1</span>, <span class="number">3</span>);</span><br><span class="line">	<span class="built_in">CV_Assert</span>(E.cols == <span class="number">3</span> &amp;&amp; E.rows == <span class="number">3</span>);</span><br><span class="line">	</span><br><span class="line">    Mat D, U, Vt;</span><br><span class="line">	SVD::<span class="built_in">compute</span>(E, D, U, Vt);</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span> (<span class="built_in">determinant</span>(U) &lt; <span class="number">0</span>) U *= <span class="number">-1.</span>;</span><br><span class="line">	<span class="keyword">if</span> (<span class="built_in">determinant</span>(Vt) &lt; <span class="number">0</span>) Vt *= <span class="number">-1.</span>;</span><br><span class="line">	</span><br><span class="line">    Mat W = (Mat_&lt;<span class="keyword">double</span>&gt;(<span class="number">3</span>, <span class="number">3</span>) &lt;&lt; <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">    W.<span class="built_in">convertTo</span>(W, E.<span class="built_in">type</span>());</span><br><span class="line">    Mat R1, R2, t;</span><br><span class="line">    </span><br><span class="line">    R1 = U * W * Vt;</span><br><span class="line">    R2 = U * W.<span class="built_in">t</span>() * Vt;</span><br><span class="line">    t = U.<span class="built_in">col</span>(<span class="number">2</span>) * <span class="number">1.0</span>;</span><br><span class="line">    </span><br><span class="line">    R1.<span class="built_in">copyTo</span>(_R1);</span><br><span class="line">    R2.<span class="built_in">copyTo</span>(_R2);</span><br><span class="line">    t.<span class="built_in">copyTo</span>(<span class="keyword">_t</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出  <a target="_blank" rel="noopener" href="https://docs.opencv.org/master/d9/d0c/group__calib3d.html#ga54a2f5b3f8aeaf6c76d4a31dece85d5d">cv::decomposeEssentialMat</a>  函数输出了 <code>R1</code>、<code>R2</code> 和 <code>t</code>，因此 4 种可能的解分别为：<code>[R1,t]</code>, <code>[R1,−t]</code>, <code>[R2,t]</code>, <code>[R2,−t]</code>， ORB_SLAM2  里使用了 <a target="_blank" rel="noopener" href="https://gitee.com/paopaoslam/ORB-SLAM2/blob/wubo&jiajia/src/Initializer.cpp?dir=0&filepath=src/Initializer.cpp&oid=ebe440148231a2c288d0aa11425db799468a92ab&sha=3ccff875e95723673258573b665ee2e33511f843#L1021">CheckRT</a> 函数对它们进行判断。考虑到 demo 视频里  🚗 是一直往前行驶，且没有转弯。因此相机 <code>t = (x, y, z)</code> 里的 <code>z &gt; 0</code>，并且相机 <code>R</code> 的对角矩阵将接近 <code>diag([1, 1, 1])</code> ，从而我们可以直接过滤出唯一解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_Rt</span>(<span class="params">E</span>):</span></span><br><span class="line">    W = np.mat([[<span class="number">0</span>,-<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]],dtype=<span class="built_in">float</span>)</span><br><span class="line">    U,d,Vt = np.linalg.svd(E)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(U)  &lt; <span class="number">0</span>: U  *= -<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(Vt) &lt; <span class="number">0</span>: Vt *= -<span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 相机没有转弯，因此 R 的对角矩阵非常接近 diag([1,1,1])</span></span><br><span class="line">    R = (np.dot(np.dot(U, W), Vt))</span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">sum</span>(R.diagonal()) &lt; <span class="number">0</span>:</span><br><span class="line">        R = np.dot(np.dot(U, W.T), Vt)</span><br><span class="line"></span><br><span class="line">    t = U[:, <span class="number">2</span>]     <span class="comment"># 相机一直向前，分量 t[2] &gt; 0</span></span><br><span class="line">    <span class="keyword">if</span> t[<span class="number">2</span>] &lt; <span class="number">0</span>:</span><br><span class="line">        t *= -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    Rt = np.eye(<span class="number">4</span>)</span><br><span class="line">    Rt[:<span class="number">3</span>, :<span class="number">3</span>] = R</span><br><span class="line">    Rt[:<span class="number">3</span>, <span class="number">3</span>] = t</span><br><span class="line">    <span class="keyword">return</span> Rt          <span class="comment"># Rt 为从相机坐标系的位姿变换到世界坐标系的位姿</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>由于平移向量的分量 t[2] &gt; 0，我们很容易知道 Rt 为从相机坐标系的位姿变换到世界坐标系的位姿</p>
</blockquote>
<h2 id="三角测量"><a href="#三角测量" class="headerlink" title="三角测量"></a>三角测量</h2><p>下一步我们需要用相机的运动估计特征点的空间位置，在单目 SLAM 中仅通过单目图像是无法获得像素的深度信息，我们需要通过<strong>三角测量（Triangulation）</strong>的方法估计图像的深度，然后通过直接线性变化（DLT）进行求解。 </p>
<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/19.png">
</p>


<p>假设点 <code>P</code> 的世界坐标为 <code>X_&#123;w&#125;</code>，图像坐标为 <code>X_&#123;uv&#125;</code>，相机的内参和位姿分别为 <code>K</code> 和 <code>P_&#123;cw&#125;</code>，那么得到：</p>
<p align="center">
    <img width="25%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1608815330116.jpg">
</p>

<p>将下标去掉，使用相机内参将两个匹配的角点像素坐标进行归一化，代入到上述方程中便得到：</p>
<p align="center">
    <img width="11%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1608816102196.jpg">
</p>

<p>使用 DLT 的话我们对上面两个公式进行一个简单的变换，对等式两边分别做外积运算：</p>
<p align="center">
    <img width="23%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1608816174696.jpg">
</p>

<p>由于 <code>x=&#123;u, v, 1&#125;</code> ，结合外积运算的知识（详见 slam 十四讲 75 页），我们便得到以下方程：</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1608816576374.jpg">
</p>

<p>我们不妨令：</p>
<p align="center">
    <img width="30%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1611127747577.jpg">
</p>

<p>将两个匹配的角点和相机位姿代入上述方程中便得到 <code>A</code>：</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/用-Python-手撸一个简单的单目-Slam-例子/MommyTalk1611128389750.jpg">
</p>

<p>因此便可以简化成 <code>AX=0</code>，从而可以使用最小二乘法来求解出 <code>X</code>，<a target="_blank" rel="noopener" href="https://github.com/raulmur/ORB_SLAM2/blob/f2e6f51cdc8d067655d90a78c06261378e07e8f3/src/Initializer.cc#L734">ORB_SLAM2</a> 中的求解过程如下：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Initializer::Triangulate</span><span class="params">(<span class="keyword">const</span> cv::KeyPoint &amp;kp1, <span class="keyword">const</span> cv::KeyPoint &amp;kp2, <span class="keyword">const</span> cv::Mat &amp;P1, <span class="keyword">const</span> cv::Mat &amp;P2, cv::Mat &amp;x3D)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">cv::Mat <span class="title">A</span><span class="params">(<span class="number">4</span>,<span class="number">4</span>,CV_32F)</span></span>;</span><br><span class="line"></span><br><span class="line">    A.<span class="built_in">row</span>(<span class="number">0</span>) = kp1.pt.x*P1.<span class="built_in">row</span>(<span class="number">2</span>)-P1.<span class="built_in">row</span>(<span class="number">0</span>);</span><br><span class="line">    A.<span class="built_in">row</span>(<span class="number">1</span>) = kp1.pt.y*P1.<span class="built_in">row</span>(<span class="number">2</span>)-P1.<span class="built_in">row</span>(<span class="number">1</span>);</span><br><span class="line">    A.<span class="built_in">row</span>(<span class="number">2</span>) = kp2.pt.x*P2.<span class="built_in">row</span>(<span class="number">2</span>)-P2.<span class="built_in">row</span>(<span class="number">0</span>);</span><br><span class="line">    A.<span class="built_in">row</span>(<span class="number">3</span>) = kp2.pt.y*P2.<span class="built_in">row</span>(<span class="number">2</span>)-P2.<span class="built_in">row</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    cv::Mat u,w,vt;</span><br><span class="line">    cv::SVD::<span class="built_in">compute</span>(A,w,u,vt,cv::SVD::MODIFY_A| cv::SVD::FULL_UV);</span><br><span class="line">    x3D = vt.<span class="built_in">row</span>(<span class="number">3</span>).<span class="built_in">t</span>();</span><br><span class="line">    x3D = x3D.<span class="built_in">rowRange</span>(<span class="number">0</span>,<span class="number">3</span>)/x3D.at&lt;<span class="keyword">float</span>&gt;(<span class="number">3</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Camera 1 Projection Matrix K[I|0]                  # 这里假设世界坐标系为相机 1 坐标系</span></span><br><span class="line"><span class="function">cv::Mat <span class="title">P1</span><span class="params">(<span class="number">3</span>,<span class="number">4</span>,CV_32F,cv::Scalar(<span class="number">0</span>))</span></span>;</span><br><span class="line">K.<span class="built_in">copyTo</span>(P1.<span class="built_in">rowRange</span>(<span class="number">0</span>,<span class="number">3</span>).<span class="built_in">colRange</span>(<span class="number">0</span>,<span class="number">3</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// Camera 2 Projection Matrix K[R|t]                  # 相机 2 与 相机的位姿 R，t</span></span><br><span class="line"><span class="function">cv::Mat <span class="title">P2</span><span class="params">(<span class="number">3</span>,<span class="number">4</span>,CV_32F)</span></span>;</span><br><span class="line">R.<span class="built_in">copyTo</span>(P2.<span class="built_in">rowRange</span>(<span class="number">0</span>,<span class="number">3</span>).<span class="built_in">colRange</span>(<span class="number">0</span>,<span class="number">3</span>));</span><br><span class="line">t.<span class="built_in">copyTo</span>(P2.<span class="built_in">rowRange</span>(<span class="number">0</span>,<span class="number">3</span>).<span class="built_in">col</span>(<span class="number">3</span>));</span><br><span class="line">P2 = K*P2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> cv::KeyPoint &amp;kp1 = vKeys1[vMatches12[i].first];</span><br><span class="line"><span class="keyword">const</span> cv::KeyPoint &amp;kp2 = vKeys2[vMatches12[i].second];</span><br><span class="line">cv::Mat p3dC1;</span><br><span class="line"></span><br><span class="line"><span class="built_in">Triangulate</span>(kp1,kp2,P1,P2,p3dC1);</span><br></pre></td></tr></table></figure>

<p>如下所示，我使用了 Python 对整个三角测量的计算过程进行了复现。值得注意的是，上述的相机位姿是指从空间点 P 从世界坐标系变换到相机坐标下点变换矩阵。如果你不清楚相机位姿的概念，请看<a target="_blank" rel="noopener" href="https://yunyang1994.gitee.io/2019/12/27/CameraPose/">什么是相机位姿？</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triangulate</span>(<span class="params">pts1, pts2, pose1, pose2</span>):</span></span><br><span class="line">    pose1 = np.linalg.inv(pose1)            <span class="comment"># 从世界坐标系变换到相机坐标系的位姿, 因此取逆</span></span><br><span class="line">    pose2 = np.linalg.inv(pose2)</span><br><span class="line"></span><br><span class="line">    pts1 = normalize(pts1)                 <span class="comment"># 使用相机内参对角点坐标归一化</span></span><br><span class="line">    pts2 = normalize(pts2)</span><br><span class="line"></span><br><span class="line">    points4d = np.zeros((pts1.shape[<span class="number">0</span>], <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> i, (kp1, kp2) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(pts1, pts2)):</span><br><span class="line">        A = np.zeros((<span class="number">4</span>,<span class="number">4</span>))</span><br><span class="line">        A[<span class="number">0</span>] = kp1[<span class="number">0</span>] * pose1[<span class="number">2</span>] - pose1[<span class="number">0</span>]</span><br><span class="line">        A[<span class="number">1</span>] = kp1[<span class="number">1</span>] * pose1[<span class="number">2</span>] - pose1[<span class="number">1</span>]</span><br><span class="line">        A[<span class="number">2</span>] = kp2[<span class="number">0</span>] * pose2[<span class="number">2</span>] - pose2[<span class="number">0</span>]</span><br><span class="line">        A[<span class="number">3</span>] = kp2[<span class="number">1</span>] * pose2[<span class="number">2</span>] - pose2[<span class="number">1</span>]</span><br><span class="line">        _, _, vt = np.linalg.svd(A)         <span class="comment"># 对 A 进行奇异值分解</span></span><br><span class="line">        points4d[i] = vt[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    points4d /= points4d[:, <span class="number">3</span>:]            <span class="comment"># 归一化变换成齐次坐标 [x, y, z, 1]</span></span><br><span class="line">    <span class="keyword">return</span> points4d</span><br></pre></td></tr></table></figure>

<h2 id="pipeline-流程"><a href="#pipeline-流程" class="headerlink" title="pipeline 流程"></a>pipeline 流程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_frame</span>(<span class="params">frame</span>):</span></span><br><span class="line">    <span class="comment"># 提取当前帧的角点和描述子特征</span></span><br><span class="line">    frame.curr_kps, frame.curr_des = extract_points(frame)</span><br><span class="line">    <span class="comment"># 将角点位置和描述子通过类的属性传递给下一帧作为上一帧的角点信息</span></span><br><span class="line">    Frame.last_kps, Frame.last_des = frame.curr_kps, frame.curr_des</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> frame.idx == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 设置第一帧为初始帧，并以相机坐标系为世界坐标系</span></span><br><span class="line">        frame.curr_pose = np.eye(<span class="number">4</span>)</span><br><span class="line">        points4d = [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]]      <span class="comment"># 原点为 [0, 0, 0] , 1 表示颜色</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 角点配准, 此时会用 RANSAC 过滤掉一些噪声</span></span><br><span class="line">        match_kps = match_points(frame)</span><br><span class="line">        <span class="comment"># 使用八点法拟合出本质矩阵</span></span><br><span class="line">        essential_matrix = fit_essential_matrix(match_kps)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;---------------- Essential Matrix ----------------&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(essential_matrix)</span><br><span class="line">        <span class="comment"># 利用本质矩阵分解出相机的位姿 Rt</span></span><br><span class="line">        Rt = extract_Rt(essential_matrix)</span><br><span class="line">        <span class="comment"># 计算出当前帧相对于初始帧的相机位姿</span></span><br><span class="line">        frame.curr_pose = np.dot(Rt, frame.last_pose)</span><br><span class="line">        <span class="comment"># 三角测量获得角点的深度信息</span></span><br><span class="line">        points4d = triangulate(frame.last_kps, frame.curr_kps, frame.last_pose, frame.curr_pose)</span><br><span class="line">		<span class="comment"># 判断3D点是否在两个摄像头前方</span></span><br><span class="line">        good_pt4d = check_points(points4d)</span><br><span class="line">        points4d = points4d[good_pt4d]</span><br><span class="line"></span><br><span class="line">        draw_points(frame)</span><br><span class="line">    mapp.add_observation(frame.curr_pose, points4d)     <span class="comment"># 将当前的 pose 和点云放入地图中</span></span><br><span class="line">    <span class="comment"># 将当前帧的 pose 信息存储为下一帧的 last_pose 信息</span></span><br><span class="line">    Frame.last_pose = frame.curr_pose</span><br><span class="line">    <span class="keyword">return</span> frame</span><br></pre></td></tr></table></figure>

</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2020/12/19/%E7%94%A8-Python-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%95%E7%9B%AE-Slam-%E4%BE%8B%E5%AD%90/" data-id="ckofioh71002xn3ragmmr4pik" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAT4AAAE+CAAAAAAxUyPsAAAFLElEQVR42u3aQY7jOBAEwP7/p2exe157MrMkYIAJnxpuW6SCBlSs5M9P/Pr132v7/Pe/P73z6b/fr/PpM/mIP2+88OHDhw8fvg8X/T6V75P79Ml23AQlmUk+t3wh/2csfPjw4cOHb+JLio/tVtvyJYHeSqu2hIpGwYcPHz58+F7jayf3vVDIi5hni6d2gfHhw4cPH74/gS8va7aGQluaJGXTVnLhw4cPHz58b/DlEfWzW/E2ut6u3471StaBDx8+fPjwTUXD3/D3wy98+PDhw4cveDzn4fcWdV+291v0vjU+Ps4cHz58+PDhWzSeaFiXjfjHtuLxuHn0Xo+CDx8+fPjwTaO0zfG2LLjswfMy5bK0bUSBDx8+fPjwbXyXA1t5KdNG2vk826t9L1/aUfDhw4cPH74LX16gbNvvy4GwbZnzK7dH5fDhw4cPH76W776l//6Av2zjE9yc8pVIAB8+fPjw4XuIr92K56xbayBpqSfR+50YHz58+PDhu/A91QrPC5S2WMk/81ScPx6ew4cPHz58+OI2fXvb2yuf6NbQf4o7b5Tgw4cPHz58d752u54/zbdREpR8e39p63+MyfHhw4cPH75H+Z5qgm80W4h+X/6oOYIPHz58+PBNffUW6KmJ5kXGpTRpQ/r2p4MPHz58+PDlfBtEvo1v2+v3Zn0b9rfN+p+WBh8+fPjw4Zu231tN1HJc3t+C8AQ9+g3iw4cPHz58h138FmPn2/u2vMg/89Q7+X3hw4cPHz58Ld97BHmjfFvzbZm331TRMsCHDx8+fPjKzz+7Cc8XKV+SvM2xlVl1yx4fPnz48OEL+E59hXNruz3ElkfaW6xely/48OHDhw/fmW+LpS8lRXLNtp3xHf2xcfHhw4cPH77D7jg/+NVOrr3mPTzIiS/NEXz48OHDh+/Clz/Ikwm1G/t2MdoiI//u6Wg4Pnz48OHDd+Db9sjbRLdx2yJpO6ZWzBYfPnz48OELZts24vOptMVBjpUXQHm43jbx8eHDhw8fvpYv3x63IfqzN9MeIMvjgctd4MOHDx8+fNse+X68bIu98+g9B3oqPi+WEx8+fPjw4XuoErgfJrs0ILaW+mWB27IGHz58+PDhe4qvPaqVE1yOr7UR+Bbtbz8jfPjw4cOHL+HbHu2XuLrd/Lc3387w9LvDhw8fPnz4ppLlV/zaGv1tcZO8s4UK91Ipisnx4cOHDx++D/PfNts55f2w2hbY5/TbneLDhw8fPnwXvtMlysh5G33M+ae5FWPhw4cPHz58Sw/84c18G5NvjYbL0m4xQ9Ssx4cPHz58+Mrnbz7FhKAlyw/GJe31Z7/1mwXDhw8fPnz4glnlD/5ny5r2Ju/z3A6i/YYeHz58+PDhm/jah3rb4K5PzMU3li/wi1kHPnz48OHDN2XKz97GGzH2pXhqC5oxLMeHDx8+fPi+XqFtUidN9raMuITo99jg+xV+8198+PDhw4dv4ssPim0b7O0IWkt/b0/k5VTUrMeHDx8+fPgCvvZcW14W5BvyfNyWJl/aMevAhw8fPnz44pz3Uii0ZFswsDUO8lj9jaIHHz58+PDhuxQf7UP90pJov7XdV7sAUcseHz58+PDhK8Py9oDXe8fO7ofbNspinvjw4cOHD99rfC10XnwkW/08jM/fuY+CDx8+fPjwvc2Xb+mfurF2rK0RPy4ePnz48OHDN/G1NG1r+x6l5+F6/iNom/4PxOT48OHDh++v58tb4XlBc5lKUsTcm/WXdklNiQ8fPnz48P37+gcbpDQDPzsq+wAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/"><i class="fa fa-tag"></i>本质矩阵</a></div><div class="post-nav"><a class="pre" href="/2021/04/20/%E4%BF%AE%E6%94%B9-YOLOv5-%E6%BA%90%E7%A0%81%E5%9C%A8-DOTAv1.5-%E9%81%A5%E6%84%9F%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E6%97%8B%E8%BD%AC%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">修改 YOLOv5 源码在 DOTAv1.5 遥感数据集上进行旋转目标检测</a><a class="next" href="/2020/10/10/2D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E8%BF%87%E5%8E%BB%EF%BC%8C%E7%8E%B0%E5%9C%A8%E5%92%8C%E6%9C%AA%E6%9D%A5/">2D人体姿态估计的总结和梳理</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/04/20/%E4%BF%AE%E6%94%B9-YOLOv5-%E6%BA%90%E7%A0%81%E5%9C%A8-DOTAv1.5-%E9%81%A5%E6%84%9F%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E6%97%8B%E8%BD%AC%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">修改 YOLOv5 源码在 DOTAv1.5 遥感数据集上进行旋转目标检测</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/19/%E7%94%A8-Python-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%95%E7%9B%AE-Slam-%E4%BE%8B%E5%AD%90/">用 Python 手撸一个单目视觉里程计的例子</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/10/2D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E8%BF%87%E5%8E%BB%EF%BC%8C%E7%8E%B0%E5%9C%A8%E5%92%8C%E6%9C%AA%E6%9D%A5/">2D人体姿态估计的总结和梳理</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/16/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B9%8B%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/">人脸识别之人脸矫正</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/TensorFlow-%E7%9A%84%E5%A4%9A%E5%8D%A1-GPU-%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%88%B6/">TensorFlow 的多卡 GPU 训练机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/22/AffineTransformation/">说说图像的仿射变换</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/21/SGD/">能不能用梯度下降法求解平方根 ？</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/20/SGM_02/">手写双目立体匹配 SGM 算法（下)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/17/SGM_01/">手写双目立体匹配 SGM 算法（上)</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/29/StereoVision/">双目测距和三维重建</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/YunYang1994" title="YunYang1994's GitHub" target="_blank">YunYang1994's GitHub</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a><ul></ul><a href="http://pengzhihui.xyz" title="稚晖的个人站" target="_blank">稚晖的个人站</a><ul></ul><a href="https://wizyoung.github.io" title="CaptainChen" target="_blank">CaptainChen</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的世界.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>