<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>手写双目立体匹配 SGM 算法（上) | 四一的随写</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">手写双目立体匹配 SGM 算法（上)</h1><a id="logo" href="/.">四一的随写</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">手写双目立体匹配 SGM 算法（上)</h1><div class="post-meta">2020-01-17<span> | </span><span class="category"><a href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 7</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p><strong>SGM（Semi-Global Matching）</strong>是一个基于双目视觉的半全局立体匹配算法，专门用于计算图像的视差。在 SGM 算法中，匹配代价计算是双目立体匹配的第一步，本文将使用 <strong>Census Transform</strong> 方法对此进行介绍。</p>
<h2 id="1-读取图片"><a href="#1-读取图片" class="headerlink" title="1. 读取图片"></a>1. 读取图片</h2><p>首先使用 OpenCV 将左图和右图读取进来，并需要将它们转成单通道的灰度图输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">left_image  = cv2.imread(<span class="string">&quot;./left.png&quot;</span>,  <span class="number">0</span>)</span><br><span class="line">right_image = cv2.imread(<span class="string">&quot;./right.png&quot;</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="40%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/手写双目立体匹配-SGM-算法-上-20210508235656.jpg">
</p>

<span id="more"></span>

<blockquote>
<p>left.png 和 right.png 可以从<font color=Red><a target="_blank" rel="noopener" href="https://github.com/YunYang1994/YunYang1994.github.io/tree/master/images/SGM">这里</a></font>进行下载。</p>
</blockquote>
<h2 id="2-高斯平滑"><a href="#2-高斯平滑" class="headerlink" title="2. 高斯平滑"></a>2. 高斯平滑</h2><p>为了减小双目图片的噪声和细节的层次感，有必要使用高斯平滑算法对它们进行预处理。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">left_image  = cv2.GaussianBlur(left_image,  (<span class="number">3</span>,<span class="number">3</span>), <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">right_image = cv2.GaussianBlur(right_image, (<span class="number">3</span>,<span class="number">3</span>), <span class="number">0</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h2 id="3-Census-变换"><a href="#3-Census-变换" class="headerlink" title="3. Census 变换"></a>3. Census 变换</h2><p>最早的匹配测度算法使用的是互信息法：对于两幅配准好的影像来说，它们的联合熵是很小的，因为其中一张影像可以通过另外一张影像预测，这表示两者之间的相关性最大，从而互信息也最大。但是它的数学原理非常复杂，且计算需要迭代，计算效率不高。在实际应用中，有一种更简单高效的方法叫 Census 变换更容易收到青睐（OpenCV 里用的就是这种方法）。</p>
<p>Census 变换的基本原理：在图像区域定义一个矩形窗口，用这个矩形窗口遍历整幅图像。选取中心像素作为参考像素，将矩形窗口中每个像素的灰度值与参考像素的灰度值进行比较，灰度值小于参考值的像素标记为 0，大于或等于参考值的像素标记为 1，最后再将它们按位连接，得到变换后的结果，变换后的结果是由 0 和 1 组成的二进制码流。<strong>Census 变换的实质是将邻域像素灰度值相对于中心像素灰度值的差异编码成二进制码流。</strong></p>
<p align="center">
    <img width="70%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/手写双目立体匹配-SGM-算法-上-20210508235752.png">
</p>

<p>我们不妨首先定义矩形窗口的大小为 7x7，由于不会对图像边界进行填充，因此计算图像的补偿尺寸：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image_h, image_w = left_image.shape[:<span class="number">2</span>]      <span class="comment"># 获取图片尺寸</span></span><br><span class="line">census_ksize = <span class="number">7</span></span><br><span class="line">x_offset = y_offset = census_ksize // <span class="number">2</span>      <span class="comment"># 补偿尺寸 = 3</span></span><br></pre></td></tr></table></figure>
<p>计算好这些必要的参数后，根据 Census 的变换原理可以获得它们的二进制编码，并将二进制编码存储为十进制数字。整个过程如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CensusTransform</span>(<span class="params">image</span>):</span></span><br><span class="line">    <span class="keyword">global</span> image_h, image_w, census_ksize, x_offset, y_offset</span><br><span class="line">    census  = np.zeros(shape=(image_h, image_w), dtype=np.uint64)</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(y_offset, image_h-y_offset):</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(x_offset, image_w-x_offset):</span><br><span class="line">            center_pixel = image[y, x]</span><br><span class="line">            reference = np.full(shape=(census_ksize, census_ksize), fill_value=center_pixel, dtype=np.int64)</span><br><span class="line">            <span class="comment"># 定义二进制编码流</span></span><br><span class="line">            binary_pattern = []</span><br><span class="line">            <span class="comment"># 定义矩形窗口</span></span><br><span class="line">            window_image = image[(y - y_offset):(y + y_offset + <span class="number">1</span>), (x - x_offset):(x + x_offset + <span class="number">1</span>)]</span><br><span class="line">            <span class="comment"># 比较矩形窗口其他像素和中心像素的大小</span></span><br><span class="line">            comparison = window_image - reference</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(comparison.shape[<span class="number">0</span>]):</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(comparison.shape[<span class="number">1</span>]):</span><br><span class="line">                    <span class="keyword">if</span> (i, j) != (y_offset, x_offset):</span><br><span class="line">                        <span class="comment"># 如果比中心像素小则编码为 1， 否则为 0</span></span><br><span class="line">                        <span class="keyword">if</span> comparison[j, i] &lt; <span class="number">0</span>:</span><br><span class="line">                            bit = <span class="number">1</span></span><br><span class="line">                        <span class="keyword">else</span>:</span><br><span class="line">                            bit = <span class="number">0</span></span><br><span class="line">                        binary_pattern.append(<span class="built_in">str</span>(bit))</span><br><span class="line"></span><br><span class="line">            binary_pattern = <span class="string">&quot;&quot;</span>.join(binary_pattern)</span><br><span class="line">            <span class="comment"># 将二进制编码存储为十进制数字</span></span><br><span class="line">            decimal_number = <span class="built_in">int</span>(binary_pattern, base=<span class="number">2</span>)</span><br><span class="line">            census[y, x] = decimal_number</span><br><span class="line">    <span class="keyword">return</span> census</span><br></pre></td></tr></table></figure>

<p>现在利用 CensusTransform 函数对左右图进行变换得到它们的 census 特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">left_census  = CensusTransform(left_image)</span><br><span class="line">right_census = CensusTransform(right_image)</span><br></pre></td></tr></table></figure>

<p>我们可以对左图的 Census 特征进行可视化，得到下图所示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv2.imwrite(<span class="string">&quot;left_census.png&quot;</span>, left_census.astype(np.uint8))</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="40%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/手写双目立体匹配-SGM-算法-上-20210508235826.png">
</p>

<h2 id="4-Cost-Volume"><a href="#4-Cost-Volume" class="headerlink" title="4. Cost Volume"></a>4. Cost Volume</h2><p>经过census变换后的图像可以使用汉明距离来计算左右两个匹配点之间相似度，这里并没有使用它们的灰度值，而是它们的 census 序列。这是因为单个像素点的灰度差异进行比较没有多大意义，而用该像素点领域范围内的纹理特征（census 序列）进行比较更具有代表性。</p>
<h3 id="4-1-汉明距离"><a href="#4-1-汉明距离" class="headerlink" title="4.1 汉明距离"></a>4.1 汉明距离</h3><p>两个 census 序列之间的相似度比较使用的是 Hamming 距离，它的度量方式为：<strong>两个字符串对应位置的不同字符的个数</strong>，它本身是一个异或问题，可以使用 <code>numpy.bitwise_xor</code> 进行求解。</p>
<table><tr><td bgcolor=Bisque>异或（xor）问题：如果 a、b 两个值不相同，则异或结果为 1。如果 a、b 两个值相同，异或结果为 0。</td></tr></table>

<p>Examples：数字 13 的二进制编码为 <code>00001101</code>， 17 则为 <code>00010001</code>，那么它们之间的 Hamming 距离为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.bitwise_xor(<span class="number">13</span>, <span class="number">17</span>)</span><br><span class="line"><span class="number">28</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>xor = np.binary_repr(<span class="number">28</span>)</span><br><span class="line"><span class="string">&#x27;11100&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>xor.count(<span class="string">&#x27;1&#x27;</span>)</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>13 和 17 的二进制编码有 3 个字符不同，所以它们的 Hamming 距离为 3。综上来说，Hamming 距离的计算代码如下：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HanMingDistance</span>(<span class="params">a, b</span>):</span></span><br><span class="line">    xor = np.int64(np.bitwise_xor(a, b))</span><br><span class="line">    xor = np.binary_repr(xor)</span><br><span class="line">    distance = xor.count(<span class="string">&#x27;1&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> distance</span><br></pre></td></tr></table></figure>

<h3 id="4-2-代价计算"><a href="#4-2-代价计算" class="headerlink" title="4.2 代价计算"></a>4.2 代价计算</h3><p>在极线约束下，我们会对右图从左至右进行扫描: 在右图 u 的位置得到该像素的 census 序列，然后与左图 u+d 位置处进行比较。由于我们事先不知道该处的视差到底有多大，因此我们会假设一个最大视差值 <code>max_disparity</code>，并计算 <code>0, 1, 2, ..., max_disparity</code> 处所有的 Hamming 距离。这个过程称为代价计算，如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/%E6%89%8B%E5%86%99%E5%8F%8C%E7%9B%AE%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D-SGM-%E7%AE%97%E6%B3%95-%E4%B8%8A-20210508235905.jpg" alt="image"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">max_disparity = <span class="number">64</span></span><br><span class="line">cost_volume = np.zeros(shape=(image_h, image_w, max_disparity), dtype=np.uint32)</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, max_disparity):</span><br><span class="line">    shift_census = np.zeros(shape=(image_h, image_w), dtype=np.int64)</span><br><span class="line">    shift_census[:, x_offset:(image_w - x_offset - d)] = left_census[:, (x_offset + d):(image_w - x_offset)]</span><br><span class="line">    shift_census[:, (image_w - x_offset - d):(image_w - x_offset)] = \</span><br><span class="line">            left_census[:, (image_w - x_offset - max_disparity):(image_w - x_offset - max_disparity + d)]</span><br><span class="line"></span><br><span class="line">    f = np.frompyfunc(HanMingDistance, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    distance = f(shift_census, right_census)</span><br><span class="line">    cost_volume[:, :, d] = distance</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="35%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/手写双目立体匹配-SGM-算法-上-20210508235937.png">
</p>

<p>既然现在已经计算出了每个像素在不同视差 d 时的汉明距离，那么其最小值对应的视差理应最接近该像素的真实视差，从而我们可以得到它的视差图并将其进行归一化：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">disparity_map, max_disparity=<span class="number">64</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    transforms values from the range (0, 64) to (0, 255).</span></span><br><span class="line"><span class="string">    :param volume: n dimension array to normalize.</span></span><br><span class="line"><span class="string">    :param max_disparity: maximuim value of disparity</span></span><br><span class="line"><span class="string">    :return: normalized array.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">255.0</span> * disparity_map / max_disparity</span><br><span class="line">    </span><br><span class="line">disp = np.argmin(cost_volume, -<span class="number">1</span>).astype(np.uint8)</span><br><span class="line">disp = normalize(disp)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;cost_volume_disp.png&quot;</span>, disp)</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="40%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/手写双目立体匹配-SGM-算法-上-20210509000006.png">
</p>

<p>我们可以发现视差图中出现了很多椒盐噪声，因此可以考虑使用中值滤波算法进行去燥，得到下图：</p>
<p align="center">
    <img width="40%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/手写双目立体匹配-SGM-算法-上-20210509000032.png">
</p>

<p>图中的一些连续平面区域依然出现了很多噪声，而且对于视差不连续的区域其效果特别差。因此我们还需要在此基础上加入一些平滑处理，并构造出了一个能量方程。从而使得立体匹配问题可以转换成寻找最优视差图 <code>D</code>，让能量方程 <code>E(D)</code> 取得最小值。</p>
<p align="center">
    <img width="80%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/手写双目立体匹配-SGM-算法-上-20210509000059.jpg">
</p>

<p>该能量方程由两部分组成：等式右边第一项表示像素点 <code>p</code> 在视差范围内所以匹配代价之和; 第二项和第三项是指当前像素 <code>p</code> 和其邻域内所有像素 <code>q</code> 之间的平滑性约束, 它是 SGM 算法的核心，将在下节对此进行讲述。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Heiko Hirschmuller. <a target="_blank" rel="noopener" href="https://core.ac.uk/download/pdf/11134866.pdf">stereo Processing by Semi-Global Matching and Mutual Information</a>. CVPR 2005</li>
<li>[2] <a target="_blank" rel="noopener" href="http://lunokhod.org/?p=1356">LUNOKHOD SGM Blog Post</a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2020/01/17/%E6%89%8B%E5%86%99%E5%8F%8C%E7%9B%AE%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D-SGM-%E7%AE%97%E6%B3%95-%E4%B8%8A/" data-id="ckw5vsrc7001z1vraet7970zv" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEhUlEQVR42u3a0Y6jMAwF0P7/T8++7kqle6+TSoQ5PFXAhHAYybHj1ys+fi6Oq6ufR/j7TPI7GfnzPK/G33xgwoQJE6ZbMs2Gvjpz9Uqfx7+iTJ6Vz2HGigkTJkyYnsGUD5e/UoveLjiSRUYyw8/vjgkTJkyYMH2eVsu3cj7nxoQJEyZMmNoQ3obbpNia3Jl/PEyYMGHC9JuZ2uRzdme+V5hshc6Kwl+vhWPChAkTppsx5V0rz/v9lf4mTJgwYcJ0G6af0ZEkqG1iPHtK28QzfF9MmDBhwnQsUxtE87LvDCIqti6cX9rsxIQJEyZMBzK1hdcWcTbpNsDvWoJESwFMmDBhwnQUU9v+sutq+/TZAmV2/j9XMWHChAnTUUx722VWWmf2FnPzq1GKjgkTJkyYHsE0C97rxdy27Ns2FeVp+X8qBJgwYcKE6UCm2RZmG2JnReHvte8Mi7yYMGHChOlYpjwwz6aeLwhmTT8rQG3xFxMmTJgwncU0a3ZZSZvz8ZNy894U9/IpmDBhwoTpEUwriWJC0FLmhd12ydIuQTBhwoQJ07lMeWI5e418crs+WLu4yf8WEyZMmDCdyLTSXjP72zY1bYN6vrFa/ENgwoQJE6ZHM+Xpa74BOZvDenNP/uEvV0yYMGHChOlYppYjCaht6jsr4LYQ9XIBEyZMmDAdy/QZpX2lPJlMlguzYu7sKD4YJkyYMGE6iikv1Oav0aavKwF+pZWnaAnChAkTJkyHM+WJa/7gJJHOi8L5J1nZXr28igkTJkyYjmXKOfIU9xspdFJr3fX78jNgwoQJE6YDmdqyaRLOk2S1CMMj+vX3woQJEyZMT2XaVepNprvSfNM25SyVejFhwoQJ07FMbfq6UkidvWq+vMjPJ4sMTJgwYcL0DKaVtpsVuLYf5rV85En1m6uYMGHChOlYpjZM5o9faZRJRstbhVYahordVEyYMGHCdGOmZGOyDdJt/r2rSWjzfw0mTJgwYXoQUz5ovlDIU9/Z1mmbxM4WDZgwYcKE6VymvL0mD8O7AvlsM3IlmhfbmZgwYcKE6RCmPHGdbRPmHDO+WTNQXmiuy76YMGHChOmWTHn6uiuJnbXmrGxz5sn8a/YATJgwYcJ0S6aV4JpXkXcF8pWjnds/92PChAkTpsOZZqXSNj1uW23aIvKu82+eggkTJkyYDmfKw/w3Fg35dma74bq+hYkJEyZMmJ7BlExiVjzNS7Szrcf2zuE8MWHChAnTI5h27d/lZ9pkOJlVPk7xFEyYMGHCdCzTDCgv7LYl11lxNt+q3NZIhAkTJkyYDmH6KY82sWwXEN8rOs/uxIQJEyZMpzPlR7LRmNAX7TJxwJ6dnzX6YMKECROmE5nyF0uA8jLuSpo6+7R5A9Cbq5gwYcKE6XCm9vVmqfLK2iQvMe/CHdabMWHChAnTsUyzTcq8QNx+gJUlS9vQgwkTJkyYfgNT+7BZsJ/d0y5E6tEwYcKECdPhTLMi7yyN3NUYlHO0C4U3Y2LChAkTpmOZZgG72AiM79mFMpvnthZVTJgwYcJ0F6Y/Xd4mBiNNNrkAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/"><i class="fa fa-tag"></i>视差估计</a><a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/"><i class="fa fa-tag"></i>立体匹配</a><a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/"><i class="fa fa-tag"></i>汉明距离</a></div><div class="post-nav"><a class="pre" href="/2020/01/20/%E6%89%8B%E5%86%99%E5%8F%8C%E7%9B%AE%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D-SGM-%E7%AE%97%E6%B3%95-%E4%B8%8B/">手写双目立体匹配 SGM 算法（下)</a><a class="next" href="/2019/12/29/%E5%8F%8C%E7%9B%AE%E6%B5%8B%E8%B7%9D%E5%92%8C%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/">双目测距和三维重建</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">杂七杂八</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/">目标跟踪</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/anchor-free/" style="font-size: 15px;">anchor free</a> <a href="/tags/DeepSort/" style="font-size: 15px;">DeepSort</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/sort/" style="font-size: 15px;">sort</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/TensorRT-%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">TensorRT 部署</a> <a href="/tags/INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/" style="font-size: 15px;">INT8 加速原理</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/C-%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">C++ 编程学习</a> <a href="/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/" style="font-size: 15px;">卡尔曼滤波</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/09/18/%E8%AE%B2%E4%B8%80%E8%AE%B2%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF/">讲一讲目前深度学习下基于单目的三维人体重建技术</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/09/FairMOT-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E9%87%8C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%86%8D%E8%AF%86%E5%88%AB%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7/">FairMOT：讨论多目标跟踪里检测与再识别的公平性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/08/%E6%BB%9A%E8%9B%8B%E5%90%A7-Anchor-%E5%90%9B-%E6%97%B7%E8%A7%86%E6%96%B0%E7%A7%91%E6%8A%80-YOLOX/">滚蛋吧，Anchor 君！旷视新科技，YOLOX</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Distilling-the-Knowledge-in-a-Neural-Network/">知识蒸馏：Distilling the Knowledge in a Neural Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/05/UnitBox%E4%B8%80%E7%A7%8D%E6%96%B0%E7%9A%84-iou-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E6%8A%8A%20box-%E5%BD%93%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%95%B4%E4%BD%93%E5%8E%BB%E9%A2%84%E6%B5%8B/">UnitBox：一种新的 IoU 损失函数，把 box 当作一个整体去预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/03/%E8%A7%A3%E5%86%B3%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94Focal-Loss/">老生常谈 Focal Loss —— 解决正负样本不均衡问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/02/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS：Fully Convolutional One-Stage Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CenterNet-Objects-as-Points/">CenterNet 和 CenterTrack：以点代物，同时进行目标检测和跟踪</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CornerNet-Detecting-Objects-as-Paired-Keypoints/">CornerNet：Detecting Objects as Paired Keypoints</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.zhihu.com/people/yang-xiao-yun-tong-xue" title="我的知乎" target="_blank">我的知乎</a><ul></ul><a href="https://github.com/YunYang1994" title="我的 GitHub" target="_blank">我的 GitHub</a><ul></ul><a href="https://leetcode-cn.com/u/yunyang1994/" title="我的力扣" target="_blank">我的力扣</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的随写.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>