<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>2D人体姿态估计的总结和梳理 | 四一的随写</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">2D人体姿态估计的总结和梳理</h1><a id="logo" href="/.">四一的随写</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">2D人体姿态估计的总结和梳理</h1><div class="post-meta">2020-10-10<span> | </span><span class="category"><a href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 5.8k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 21</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>最近在虎牙直播做了一些关于人体姿态估计的工作，也看了蛮多这方面的文章，于是对这个内容做了一些总结和梳理，希望能抛砖引玉吧。我们不妨先把问题抛出来，人体姿态估计是做什么？从名字的角度来看，可以理解为对“人体”的姿态（关键点，比如头，左手，右脚等）的位置估计。根据 RGB 图片里人体的数量，又可以分为<strong>单人姿态估计</strong>和<strong>多人姿态估计</strong>。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509001617.jpg">
</p>

<span id="more"></span>

<h2 id="1-单人姿态估计"><a href="#1-单人姿态估计" class="headerlink" title="1. 单人姿态估计"></a>1. 单人姿态估计</h2><h3 id="1-1-问题"><a href="#1-1-问题" class="headerlink" title="1.1 问题"></a>1.1 问题</h3><p>对于单人姿态估计，输入是一个 crop 出来的行人，然后在行人区域位置内找出需要的关键点，比如头部，左手，右膝等。对于<strong>多人姿态估计</strong>，目前主要有 2 种主流思路，分别是 <strong>top-down</strong> 以及 <strong>bottom-up</strong> 方法。对于 top-down 方法，往往先找到图片中所有行人，然后对每个行人做姿态估计，寻找每个人的关键点。单人姿态估计往往可以被直接用于这个场景。对于 bottom-up，思路正好相反，先是找图片中所有 parts （关键点），比如所有头部，左手，膝盖等，然后把这些 parts（关键点）组装成一个个行人。</p>
<h3 id="1-2-数据集"><a href="#1-2-数据集" class="headerlink" title="1.2 数据集"></a>1.2 数据集</h3><ul>
<li><a target="_blank" rel="noopener" href="https://sam.johnson.io/research/lsp.html">LSP（Leeds Sports Pose Dataset）</a>：单人人体关键点检测数据集，关键点个数为14，样本数2K，在目前的研究中作为第二数据集使用。</li>
<li><a target="_blank" rel="noopener" href="https://bensapp.github.io/flic-dataset.html">FLIC（Frames Labeled In Cinema）</a>：单人人体关键点检测数据集，关键点个数为9，样本数2W，在目前的研究中作为第二数据集使用。</li>
<li><a target="_blank" rel="noopener" href="http://human-pose.mpi-inf.mpg.de/">MPII（MPII Human Pose Dataset）</a>：单人/多人人体关键点检测数据集，关键点个数为16，样本数25K，是单人人体关键点检测的主要数据集。它是 2014 年由马普所创建的，目前可以认为是单人姿态估计中最常用的 benchmark， 使用的是 <a target="_blank" rel="noopener" href="http://human-pose.mpi-inf.mpg.de/#results">PCKh</a> 的指标。</li>
<li><a target="_blank" rel="noopener" href="https://cocodataset.org/#home">MSCOCO</a>：多人人体关键点检测数据集，关键点个数为17，样本数量多于30W。目前是多人关键点检测的主要数据集，使用的是 <a target="_blank" rel="noopener" href="https://cocodataset.org/#keypoints-eval">AP 和 OKS</a> 指标。</li>
<li><a target="_blank" rel="noopener" href="http://vision.imar.ro/human3.6m/description.php">human3.6M</a>：是 3D 人体姿势估计的最大数据集，由 360 万个姿势和相应的视频帧组成，这些视频帧包含11 位演员从4个摄像机视角执行 15 项日常活动的过程。数据集庞大将近100G。</li>
<li><a target="_blank" rel="noopener" href="https://posetrack.net/">PoseTrack</a>：最新的关于人体骨骼关键点的数据集，多人人体关键点跟踪数据集，包含单帧关键点检测、多帧关键点检测、多人关键点跟踪三个人物，多于500个视频序列，帧数超过20K，关键点个数为15。</li>
</ul>
<h3 id="1-3-评估指标"><a href="#1-3-评估指标" class="headerlink" title="1.3 评估指标"></a>1.3 评估指标</h3><p><code>Keypoint detection</code> 度量方法的核心思想就是模仿 <code>Object detection</code> 的度量方法。在 <code>Object detection</code> 中，<code>IoU</code> 是作为一种相似度度量，它通过计算检测框之间的重合度来定义真实目标和预测值目标之间的距离。而在 <code>Keypoint detection</code> 则是根据<font color=Red>预测关键点和真实关键点之间的欧式距离来评判的</font>。</p>
<ul>
<li>PCK(Percentage of Correct Keypoints)</li>
</ul>
<p><font color=Red>关键点正确估计的百分比，计算检测的关键点与其对应的 groundtruth 间的归一化距离小于设定阈值的比例</font>。这是比较老的人体姿态估计指标，在17年以前广泛使用，现在基本不再使用。但是如果仅是用于工程项目中来评价训练模型的好坏还是蛮方便的，因此这里也记录下：</p>
<p align="center">
    <img width="25%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509001841.png">
</p>

<p>其中： <code>i</code> 表示关节点的编号， <code>di</code> 表示第i个关节点的预测值和 groundtruth 的欧氏距离。 <code>d</code> 是一个人体的尺度因子，在不同的数据集里的计算方法不同。例如 MPII 中 数据集是以头部长度作为归一化参考，即 <strong>PCKh</strong>。</p>
<ul>
<li>oks (object keypoint similarity)</li>
</ul>
<p><font color=Red>oks 表示关键点的相似度，它和预测关键点和真实关键点之间的欧式距离有关，其范围值为 [0, 1]。当它们的欧式距离为 0 时，其 oks 相似度等于 1</font></p>
<p align="center">
    <img width="40%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509001908.png"></p>
 
<p> 其中：<code>i</code> 为关键点的编号，<code>di</code> 表示预测的关键点和 groundtruth 之间的欧式距离，<code>S</code> 是一个尺度因子，为人体检测框面积的平方根， <code>oi</code> 是一个归一化因子，和关键点标注的难易有关，是通过对所有样本的人工标注和真实值的统计标准差。 <code>vi</code> 表示当前关键点是否可见。</p>
<ul>
<li>AP(Average Precision)</li>
</ul>
<p><font color=Red>这个和目标检测里的 <code>AP</code> 概念是一样的，只不过度量方式 <code>iou</code> 替换成了 <code>oks</code>。如果 oks 大于阈值 T，则认为该关键点被成功检测到</font>。单人姿态估计和多人姿态估计的计算方式不同。对于单人姿态估计的AP，目标图片中只有一个人体，所以计算方式为：</p>
<p align="center">
    <img width="27%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509001945.png"></p>

<p>对于多人姿态估计而言，由于一张图像中有 <code>M</code> 个目标，假设总共预测出 <code>N</code> 个人体，那么groundtruth和预测值之间能构成一个 <code>MxN</code> 的矩阵，然后将每一行的最大值作为该目标的 oks，则：</p>
<p align="center">
    <img width="35%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509001952.png"></p>

<h2 id="2-1-过去和现在"><a href="#2-1-过去和现在" class="headerlink" title="2.1 过去和现在"></a>2.1 过去和现在</h2><p>在过去，传统方法一般是基于图结构和形变部件模型，设计 2D 人体部件检测器，使用图模型建立各部件的连通性，然后结合关键点之间的 pair-wise 关系来恢复人体的姿态。传统方法虽然拥有较高的时间效率，但是提取的都是人工设定的浅层特征（如 HOG，SIFT 等）。而在现在的深度学习时代，姿态估计的特征提取，分类，以及空间位置的建模都可以在一个网络中直接建模，不再需要独立的进行拆解，整个过程完全可以端到端进行。</p>
<h3 id="2-2-过去的思路"><a href="#2-2-过去的思路" class="headerlink" title="2.2 过去的思路"></a>2.2 过去的思路</h3><p>在 2014 年以前，很多论文讲的都是利用传统方法去解决单人姿态估计的问题。这些工作都是在深度学习爆发之前，我们是如何处理人体姿态估计这个问题。从算法角度来讲，这部分的工作主要是希望解决单人的人体姿态估计问题，也有部分工作已经开始尝试做 3D 的人体姿态估计。可以粗略的方法分成两类。</p>
<p>第一类是直接通过一个全局 feature，把姿态估计问题当成分类或者回归问题直接求解。但是这类方法的问题在于精度一般，并且可能比较适用于背景干净的场景。第二类是基于一个图模型，比如常用pictorial structure model。一般包含 unary term, 是指对单个 part 进行 feature 的表示，单个 part 的位置往往可以使用DPM (Deformable Part-based model) 来获得。同时需要考虑 pair-wise 关系来优化关键点之间的关联。</p>
<p>总结一下，在传统方法里面，需要关注的两个维度是： feature representation 以及关键点的空间位置关系。特征维度来讲，传统方法一般使用的 HOG, Shape Context, SIFT 等浅层特征。 空间位置关系的表示也有很多形式，上面的 Pictorial structure model 可能只是一种。</p>
<h3 id="2-3-现在的方法"><a href="#2-3-现在的方法" class="headerlink" title="2.3 现在的方法"></a>2.3 现在的方法</h3><p>自从 2012 年 AlexNet 开始，深度学习开始快速发展，从最早的图片分类问题，到后来的检测，分割问题。<font color=Red>第一次有人在 2014 年成功使用 CNN 来解决单人的姿态估计问题</font>。受限于当时的时代背景，整体网络结构比较简单并且同时也沿用了传统思路：首先是通过滑动窗口的方式，来对每个 patch 进行分类，找到相应的人体关键点。因为直接使用滑动窗口缺少了很多 context 信息，所以会有很多 FP 的出现。所以在 pipeline 中加了一个后处理的步骤，希望能抑制部分FP，具体的实现方式是类似一个空间位置的模型。从这个工作来看，有一定的传统姿态估计方法的惯性，改进的地方是把原来的传统的特征表示改成了深度学习的网络，同时把空间位置关系当成是后处理来做处理。总体性能在当时已经差不多跑过了传统的姿态估计方法。</p>
<p><font color=Red>2014 年的另外一个重要的进展是引入了 <code>MPII</code> 的数据集</font>。此前的大部分 paper 都是基于 FLIC 以及 LSP 来做评估的，但是在深度学习时代，数据量还是相对偏少（K 级别）。MPII 把数据量级提升到 W 级别，同时因为数据是互联网采集，同时是针对 activity 来做筛选的，所以无论从难度还是多样性角度来讲，都比原来的数据集有比较好的提升。</p>
<p>现在几乎所有的主流方法都是围绕着下面两大块进行的：改善监督信息和网络结构。</p>
<h2 id="3-改善监督信息"><a href="#3-改善监督信息" class="headerlink" title="3. 改善监督信息"></a>3. 改善监督信息</h2><p>根据真值标签 (Ground Truth) 的类型和监督训练的方式，单人姿态估计主要有 2 种思路：基于坐标回归和基于热图检测，它们各有优缺点。</p>
<h3 id="3-1-基于坐标回归（代表作：Deep-Pose"><a href="#3-1-基于坐标回归（代表作：Deep-Pose" class="headerlink" title="3.1 基于坐标回归（代表作：Deep Pose)"></a>3.1 基于坐标回归（代表作：Deep Pose)</h3><p>基于坐标回归的算法是将关节点的二维坐标 (<code>coordinate</code>) 作为 Ground Truth，训练网络直接回归得到每个关节点的坐标，这样的好处在于可以得到 sub-pixel 级别的精度，而且速度会比较快（不需要后处理）。例如在一些人脸关键点检测算法（如 mtcnn）中，会在最后一层使用全连接层直接回归出 landmark 的坐标。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002136.png">
</p>

<p>在人体姿态估计领域中，这类方法的代表作是谷歌大佬提出的 <a target="_blank" rel="noopener" href="https://github.com/mitmul/deeppose"><code>Deep Pose</code></a> 网络。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002102.jpg"></p>

<blockquote>
<p>在上图中：<font color=Red>蓝色部分是卷积层，绿色部分是全连接层</font>。Deep Pose 算法主要是通过训练一个级联的姿态回归器。在第一个阶段先粗略地估计出部分的姿态轮廓，然后在下个阶段将通过已知关键点的位置不断去优化其他关键点的位置。每个 stage 都使用已经预测的关键点来 crop 出基于这个关键点的邻域子图像并用于后续的网络输入，从而得到一个更加精准的位置信息。</p>
</blockquote>
<p>但是值得注意的是：一方面，全连接层（<code>fully connected layer</code>）容易抹掉人体关键点在图像中的空间位置信息和 context 信息。如果缺乏 context 信息，会使得模型很难区分左右手，导致 FP 容易高。另一方面，人体姿态估计这个问题本身的自由度很大。直接 regression 的方式对自由度小的问题比如人脸 landmark 可能是比较适合的，但是这会对自由度较大的姿态估计问题整体的建模能力会比较弱。</p>
<blockquote>
<p>人体姿态估计的自由度较大的地方在于：例如，人的左手腕关节点既可以出现在人体的左边，也可以出现在人体的右边，因此 context 和空间位置信息就显得很重要。</p>
</blockquote>
<h3 id="3-2-基于热图检测（代表作：Simple-Baseline）"><a href="#3-2-基于热图检测（代表作：Simple-Baseline）" class="headerlink" title="3.2 基于热图检测（代表作：Simple Baseline）"></a>3.2 基于热图检测（代表作：Simple Baseline）</h3><p><code>Heatmap</code> 将每个骨骼节点的坐标分别都用一个概率图来表示，这个概率图的分辨率往往是原图的等比例缩放图（一般为 64x48），channel 的个数等于关键节点的数目。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002216.jpg"></p>
    
<p>热图中的每个像素位置都会给出一个概率，表示该点属于对应类别关键点的概率，比较自然的是，距离关键点位置越近的像素点的概率越接近 1，而距离关键点越远的像素点的概率越接近 0，具体可以通过相应函数进行模拟，如 Gaussian 函数等。</p>
<p align="center">
    <img width="45%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002230.jpg"></p>

<p>微软亚洲研究院的 xiao bin 于 2018 年提出了一种基于热图检测的极其简单的单人姿态估计网络。由于它简单有效，所以作者称之为 <a target="_blank" rel="noopener" href="https://github.com/microsoft/human-pose-estimation.pytorch">simple baseline</a>。该网络只是在 ResNet 后面添加一些反卷积层，甚至都没有 skip connection，这种结构可以说是从 deep 和 low 分辨率特征生成热图的最简单也是最熟悉的方法。</p>
<p align="center">
    <img width="38%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002336.jpg"></p>
    
<p>最后加入 1×1 的卷积层，生成 k 个关键点的预测热图，与 ground truth 热图计算 MSE 损失。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JointsMSELoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, use_target_weight</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(JointsMSELoss, self).__init__()</span><br><span class="line">        self.criterion = nn.MSELoss(size_average=<span class="literal">True</span>)</span><br><span class="line">        self.use_target_weight = use_target_weight</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, output, target, target_weight</span>):</span></span><br><span class="line">        batch_size = output.size(<span class="number">0</span>)</span><br><span class="line">        num_joints = output.size(<span class="number">1</span>)</span><br><span class="line">        heatmaps_pred = output.reshape((batch_size, num_joints, -<span class="number">1</span>)).split(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        heatmaps_gt = target.reshape((batch_size, num_joints, -<span class="number">1</span>)).split(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(num_joints):</span><br><span class="line">            heatmap_pred = heatmaps_pred[idx].squeeze()</span><br><span class="line">            heatmap_gt = heatmaps_gt[idx].squeeze()</span><br><span class="line">            <span class="keyword">if</span> self.use_target_weight:</span><br><span class="line">                loss += <span class="number">0.5</span> * self.criterion(</span><br><span class="line">                    heatmap_pred.mul(target_weight[:, idx]),</span><br><span class="line">                    heatmap_gt.mul(target_weight[:, idx])</span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                loss += <span class="number">0.5</span> * self.criterion(heatmap_pred, heatmap_gt)</span><br><span class="line">        <span class="keyword">return</span> loss / num_joints</span><br></pre></td></tr></table></figure>
<h2 id="4-改善网络结构"><a href="#4-改善网络结构" class="headerlink" title="4. 改善网络结构"></a>4. 改善网络结构</h2><p>后续 2D 人体姿态估计方法几乎都是围绕 heatmap 这种形式来做的：通过使用神经网络来获得更好的特征表示，同时把关键点的空间位置关系隐式地 encode 在 heatmap 中进行监督学习。随着基于热图检测的方法成为标准，往后越来越多的工作聚焦在了网络结构设计上。</p>
<h3 id="4-1-CPM（Convolutional-Pose-Machines）"><a href="#4-1-CPM（Convolutional-Pose-Machines）" class="headerlink" title="4.1 CPM（Convolutional Pose Machines）"></a>4.1 CPM（Convolutional Pose Machines）</h3><p>经典的卷积姿态机（Convolutional Pose Machines，CPM）是CMU Yaser Sheikh组的工作，后续非常有名的 openpose 也是他们的工作。CPM 在初始阶段 (stage 1) 只对输入图片进行卷积，输出所有关节点的 heatmap。在后面的每个阶段 中，CPM 首先设计了一个特征提取器 (FeatureExtractor) 将上一个阶段输出的 heatmap 和原始图像的 feature map 级联起来，然后将这种预处理过的融合 feature map 输入 本阶段的 FCN 进行处理，最终得到新的关节点 heatmap。 </p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002418.jpg"></p>

<blockquote>
<p>CPM 由多个 stage 网络级联而成，每个 stage 设计一个小型网络，用于提取 feature。中间层的信息可以给后续层提供 context，后续 stage 可以认为是基于前面的 stage 去做 refinement。这个 refinement 过程需要感受野提供更多的 context 信息，因此感受野会越来越大。</p>
</blockquote>
<p align="center">
    <img width="60%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002502.jpg"></p>

<p>但是通过不断增加卷积层来改变感受野会给网络产生较大的训练负担，造成梯度消失等问题。为避免加大感受野带来的副作用，CPM 采用中继监督训练，将各个阶段产生的 heatmap 与 Ground Truth 产生的误差累加起来作为总误差进行迭代，同时将梯度从各个阶段网络的输 出层反向传播，避免梯度消失，最后得到各个阶段修正后的响应 feature map，即置信图 (belief map)。CPM 在测试时以最后一个阶段的响应图输出为准。</p>
<h3 id="4-2-Stacked-Hourglass-Network"><a href="#4-2-Stacked-Hourglass-Network" class="headerlink" title="4.2 Stacked Hourglass Network"></a>4.2 Stacked Hourglass Network</h3><p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002540.png"></p>

<p>在 2016 年的 7 月份，Princeton 的 Deng Jia 组放出了另外一个非常棒的人体姿态估计工作，堆叠式沙漏网络（Stacked Hourglass Network）。它最大的特点就是，网络既简单优美又准确高效。从上图可以看出，网络由很多重复堆叠的 u-shape 模块（如下图所示）所组成。并且在每个 u-shape 模块的旁路都添加了残差模块（Residual Module）分支，它在网络的深层梯度传导和防止过拟合方面起到关键性作用。</p>
<p align="center">
    <img width="60%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002615.png"></p>

<p>作者在整个网络结构中堆叠了许多 hourglass 模块，从而使得网络能够不断重复自底向上和自顶向下的过程。类似于 CPM 的多阶段学习过程，堆叠式沙漏结构 同样采用了“配套的”中继训练。但它是对每一个阶段得到的关节点 heatmap 立即根据 Ground Truth 进行重绘 (remap)，并得到当前的 belief map，而不是像 CPM 累 加所有阶段的总误差再进行迭代反馈。</p>
<p align="center">
    <img width="60%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002654.jpg"></p>

<blockquote>
<p>Illustration of the intermediate supervision process. The network splits and produces a set of heatmaps (outlined in blue) where a loss can be applied. A 1x1 convolution remaps the heatmaps to match the number of channels of the intermediate features. These are added together along with the features from the preceding hourglass.</p>
</blockquote>
<h3 id="4-3-HRNet（High-Resolution-Network）"><a href="#4-3-HRNet（High-Resolution-Network）" class="headerlink" title="4.3 HRNet（High-Resolution Network）"></a>4.3 HRNet（High-Resolution Network）</h3><p>从2012年以来随着 AlexNet 横空出世，神经网络在计算机视觉领域成为主流的方法。2014年谷歌发明出了 GoogleNet，牛津大学发明了 VGGNet，2015 年微软发明了 ResNet，2016 年康奈尔大学和清华大学发明了 DenseNet，以上都是围绕分类任务而发明的网络结构。这些网络结构的一个共同的特征便是：逐步减小空间的大小，最终得到一个低分辨率的表征。低分辨率的表征在图像分类任务中是足够的，因为在图像分类里面，只需要给一个全局的标签，而不需要详细的空间信息，我们称之为空间粗粒表征的学习。</p>
<p>但是在其它任务中，比如检测，我们需要知道检测框的空间位置，比如分割，我们需要每个像素的标签，在人脸和人体的关键点的检测中，我们需要关键点的空间位置，这样一系列的任务实际上需要空间精度比较高的表征，我们称之为高分辨率表征。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002725.jpg"></p>

<p>到了 2019 年， MSRA 的 wang jingdong 组出了一个很好的工作，提出了 spatial resolution 的重要性。他认为以前网络的套路都是 feature map 的分辨率一开始虽然很高，但是会慢慢降低，然后又慢慢升高。在这个过程中，先失去了空间精度，然后慢慢恢复，最终学到的特征空间精度较弱。</p>
<p><font color=Red>因此作者提出了一种新的网络结构：分成多个层级，但是始终保留着最精细的 spaital 那一层的信息，通过 fuse 从高分辨率到低分辨率的子网络输出，来获得更多的 context 以及语义层面信息。</font>它能够在整个过程中保持高分辨率的表示。以高分辨率子网络开始作为第一阶段，逐步增加高分辨率到低分辨率的子网络。以此内推形成更多阶段，并将多分辨率子网络并行连接。在整个过程中，通过在并行的多分辨率子网络上反复交换信息来进行多尺度的重复融合。</p>
<p align="center">
    <img width="70%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002926.jpg"></p>

<blockquote>
<p>一直保持高分辨率的特征能够很好地保留空间位置信息，而逐步增加高分辨率到低分辨率的子网络是为了添加更多丰富的语义信息。<font color=Red>在低分辨率方面，它可以学习到很好的语义信息；在高分辨率里，它的空间精度非常强。在整个过程中，高中低分辨率不停地交互，使得高分辨率可以拿到低分辨率语义性比较强的表征，低分辨率可以拿到高分辨率的空间精度比较强的表征，不停地融合，最终取得更强的高分辨率表征。</font></p>
</blockquote>
<h2 id="5-多人姿态估计"><a href="#5-多人姿态估计" class="headerlink" title="5. 多人姿态估计"></a>5. 多人姿态估计</h2><p>通过单人体态估计的方法可以得到人体的2D关节点坐标，但是在一张多人图像中，模型在区别不同人体 的关节点，避免不同人体的关节点之间进行误连时，需 要额外的策略作为指导。为此，多人姿态估计的相关研究大致提供了两种思路: 自顶向下(Top-Down) 和自底向上(Bottom-Up)的。</p>
<h3 id="5-1-自底向上（Bottom-Up）"><a href="#5-1-自底向上（Bottom-Up）" class="headerlink" title="5.1 自底向上（Bottom-Up）"></a>5.1 自底向上（Bottom-Up）</h3><p>自底向上的思路为：首先用单人姿态估计的方法构建部件检测器将图片中所有的人体关节点全部检测出来，然后在第二个阶段对不同人体的关节点聚成一类并拼接在一起。这类的代表作为 Open Pose，它在 2016 年的 COCO 比赛中一举夺得第一名。 CMU 团队基于 CPM 为组件，先找到图片中每个 joint 的位置，然后提出部件亲和场（Part Affinity Field，PAF) 来做人体关键点的组装。</p>
<p align="center">
    <img width="100%" src="https://cdn.jsdelivr.net/gh/YunYang1994/blogimgs/2D人体姿态估计的总结和梳理-20210509002952.jpg"></p>

<p>PAF 的基本原理是在两个相邻关键点之间，建立一个有向场，比如左手腕，左手肘。我们把 CPM 找到的所有的左手腕以及左手肘拿出来建立一个二分图，边权就是基于 PAF 场来计算的。然后进行匹配，匹配成功就认为是同一个人的关节。依次类别，对所有相邻点做此匹配操作，最后就得到每个人的所有关键点。</p>
<h3 id="5-2-自顶向下（Top-Down）"><a href="#5-2-自顶向下（Top-Down）" class="headerlink" title="5.2 自顶向下（Top-Down）"></a>5.2 自顶向下（Top-Down）</h3><p>虽然 2016 年 bottom-up 是一个丰富时间点，但是从 2017 年开始，越来的工作开始围绕top-down 展开，一个直接的原因是 top-down 的效果往往更有潜力。top-down 相比bottom-up 效果好的原因可以认为有两点。首先是人的 recall 往往更好。因为 top-down 是先做人体检测，人体往往会比 part 更大，所以从检测角度来讲会更简单，相应找到的recall 也会更高。其次是关键点的定位精度会更准，这部分原因是基于 crop 的框，对空间信息有一定的 align，同时因为在做单人姿态估计的时候，可以获得一些中间层的 context 信息，这对于点的定位是很有帮助的。当然，top-down 往往会被认为速度比 bottom-up 会更慢（特别是人数较多的场景）。所以在很多要求实时速度，特别是手机端上的很多算法都是基于 openpose 来做修改的。不过这个也要例外，我们自己也有做手机端上的多人姿态估计，但是我们是基于 top-down 来做的，主要原因是人体检测器可以做的非常快。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a target="_blank" rel="noopener" href="https://jishuin.proginn.com/p/763bfbd299d2">[1] 姿态估计：人体骨骼关键点检测综述（2016-2020）</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/ZXF_1991/article/details/104279387">[2] 人体姿态估计－评价指标（一）</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.01423">[3] Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.06278">[4] Distribution-Aware Coordinate Representation for Human Pose Estimation</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/85506259">[5] 人体姿态估计的过去，现在，未来</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.06208">[6] Simple Baselines for Human Pose Estimation and Tracking. ECCV 2018</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1602.00134.pdf">[7] Convolutional Pose Machines. CVPR 2016</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.06937">[8] Stacked Hourglass Networks for Human Pose Estimation. ECCV 2016</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.09212">[9] Deep High-Resolution Representation Learning for Human Pose Estimation. CVPR 2019</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.08008">[10] OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. CVPR 2017</a></li>
<li><a target="_blank" rel="noopener" href="https://www.msra.cn/zh-cn/news/features/a-universal-architecture-for-visual-recognition">[11] 王井东：下一代视觉识别的通用网络结构是什么样的？</a></li>
<li><a target="_blank" rel="noopener" href="https://www.msra.cn/zh-cn/news/features/cvpr-2019-hrnet">[12] 告别低分辨率网络，微软提出高分辨率深度神经网络HRNet</a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2020/10/10/2D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%80%BB%E7%BB%93%E5%92%8C%E6%A2%B3%E7%90%86/" data-id="ckw5vsrdv00461vrahrp5amjn" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASYAAAEmCAAAAADqr2IGAAAEf0lEQVR42u3bS27bQBAFQN//0skmCwOB6Pe6KUBDl1aGRHGGpUVPf/z1Fb/+fHtdf/r9mv//zq+8Xv36PvlaN78wYcKECdNHMs1ufb3Flub6AZLV20/zZ8eECRMmTM9gym93vZXrLW5ibv5zXj9wcjjAhAkTJkyY6mXib20S3fx9TJgwYcKEKU99kwD8vpr0bJ+YMGHChOm3MeUpZXtEyLfVptN5mr1pf2LChAkTptOZNkH39L/fMt+ECRMmTJg+hmnTOLwOzG06nTcg81brvkX67z6YMGHChOlYpjaIbsZi7i3yztLdWXqPCRMmTJiewZTfaDYcc1dg3pRrZ21XTJgwYcJ0OlMbbmfNvzz8J2l28n7S/mxXwYQJEyZM5zLNyrI5a16QzR9ydjTJKV+enjBhwoQJ0yFMeejdQMxC8r3F37ZI/bK8iwkTJkyYHseUJ4p56J0lpZuy72ygBxMmTJgwnc60L8Ju2o15aL+37Fs0LDFhwoQJ0+FMebF1MwST3OcdP8Bde8aECRMmTM9gyh9y0/5si7z7Q8B+t5gwYcKE6XSmWem2fdT2u0ni2hZth2k5JkyYMGE6linZVj7+MsOqE9FR6XlzOMCECRMmTOcyJS3DHCVKI+NP8+LvLPFO1sWECRMmTKcz5Ztog2tCH22uPNfMfoCoHIAJEyZMmB7K1C68aW3OyrXtt/KfEBMmTJgwPYOpvmjx2O1as8T7rifChAkTJkynM+WDO7PQ3t4zb4u240fJHl5iYcKECROmY5naZHXW+MyHeJKN5t/dPFdxbsKECRMmTB/P1AbpTUk3H82ZgW7W+iFhxoQJEyZMBzIV1d94NGfGOlurBVoVeTFhwoQJ0+FMs21tKPdJb97ObAeMMGHChAnTU5lmR4R2sGaWsm4OK3ka/7LriwkTJkyYDmeaBelNEps3Sq/XaoN93YLFhAkTJkyHM+UL50Ors0PGLGC3K9aHDEyYMGHCdCxTHmiHE69xwJ4F9RnirACNCRMmTJhOZMqHbDaxMt/cZnioTc6LQw8mTJgwYXoQU5vo7ovF+3/bSBqWm08xYcKECdO5TPsGYVv8nTUdZ8eOvHAcHYYwYcKECdOBTPloaX5oiEZh4iJvS7ZPdH8Y3MGECRMmTEcxtQ3FWZKctELba/YF6PowhAkTJkyYHsHUNh1n6W6Osi/m5oeAaLIJEyZMmDAdxZRsdwY0TC/flu7OjiyYMGHChOlJTLPibxu8N6MzbSJ92+ARJkyYMGE6lmlWrt0XYd9x5b0rDlubmDBhwoTpw5juCuH5ESFPU/ejRe3+oyo4JkyYMGE6iqkNqHc9Xp58tgnq5j43YGHChAkTpo9kuqvtlzcUZ3l5eyBoS8M/IGLChAkTpsOZ2gA8K+8mD3bXdze7enkgwIQJEyZMj2a6XjhPQe8qELdDPKuCMiZMmDBh+gVMs2QyCc95gL+rbVkfCzBhwoQJ0+FM7bBOHqqTO+QDNPn77SjSzbVwTJgwYcL0MUx5mNy3KpPQvmlDtsXfNr3HhAkTJkxHMf0F1YLoGfIA3msAAAAASUVORK5CYII=">分享</a><div class="tags"><a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/"><i class="fa fa-tag"></i>hourglass 网络</a></div><div class="post-nav"><a class="pre" href="/2020/12/19/%E7%94%A8-Python-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%95%E7%9B%AE-Slam-%E4%BE%8B%E5%AD%90/">随手用 Python 撸一个单目视觉里程计的例子</a><a class="next" href="/2020/02/16/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B9%8B%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/">人脸识别之人脸矫正</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/">杂七杂八</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/">目标跟踪</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/anchor-free/" style="font-size: 15px;">anchor free</a> <a href="/tags/DeepSort/" style="font-size: 15px;">DeepSort</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/sort/" style="font-size: 15px;">sort</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/TensorRT-%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">TensorRT 部署</a> <a href="/tags/INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/" style="font-size: 15px;">INT8 加速原理</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/C-%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">C++ 编程学习</a> <a href="/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/" style="font-size: 15px;">卡尔曼滤波</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/09/18/%E8%AE%B2%E4%B8%80%E8%AE%B2%E7%9B%AE%E5%89%8D%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%8D%95%E7%9B%AE%E7%9A%84%E4%B8%89%E7%BB%B4%E4%BA%BA%E4%BD%93%E9%87%8D%E5%BB%BA%E6%8A%80%E6%9C%AF/">讲一讲目前深度学习下基于单目的三维人体重建技术</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/09/FairMOT-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E9%87%8C%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%86%8D%E8%AF%86%E5%88%AB%E7%9A%84%E5%85%AC%E5%B9%B3%E6%80%A7/">FairMOT：讨论多目标跟踪里检测与再识别的公平性</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/08/%E6%BB%9A%E8%9B%8B%E5%90%A7-Anchor-%E5%90%9B-%E6%97%B7%E8%A7%86%E6%96%B0%E7%A7%91%E6%8A%80-YOLOX/">滚蛋吧，Anchor 君！旷视新科技，YOLOX</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/07/%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Distilling-the-Knowledge-in-a-Neural-Network/">知识蒸馏：Distilling the Knowledge in a Neural Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/05/UnitBox%E4%B8%80%E7%A7%8D%E6%96%B0%E7%9A%84-iou-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E6%8A%8A%20box-%E5%BD%93%E4%BD%9C%E4%B8%80%E4%B8%AA%E6%95%B4%E4%BD%93%E5%8E%BB%E9%A2%84%E6%B5%8B/">UnitBox：一种新的 IoU 损失函数，把 box 当作一个整体去预测</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/03/%E8%A7%A3%E5%86%B3%E6%AD%A3%E8%B4%9F%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%9D%87%E8%A1%A1%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94Focal-Loss/">老生常谈 Focal Loss —— 解决正负样本不均衡问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/02/FCOS-Fully-Convolutional-One-Stage-Object-Detection/">FCOS：Fully Convolutional One-Stage Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CenterNet-Objects-as-Points/">CenterNet 和 CenterTrack：以点代物，同时进行目标检测和跟踪</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/09/01/CornerNet-Detecting-Objects-as-Paired-Keypoints/">CornerNet：Detecting Objects as Paired Keypoints</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/08/27/DeepSort-%E5%A4%9A%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95-SORT-%E7%9A%84%E8%BF%9B%E9%98%B6%E7%89%88/">DeepSort：多目标跟踪算法 Sort 的进化版</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.zhihu.com/people/yang-xiao-yun-tong-xue" title="我的知乎" target="_blank">我的知乎</a><ul></ul><a href="https://github.com/YunYang1994" title="我的 GitHub" target="_blank">我的 GitHub</a><ul></ul><a href="https://leetcode-cn.com/u/yunyang1994/" title="我的力扣" target="_blank">我的力扣</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的随写.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>