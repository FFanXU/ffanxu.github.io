<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Stay hungry, Stay foolish"><title>YOLOv3 算法的一点理解 | 四一的世界</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/normalize.css/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/npm/purecss/build/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//cdn.jsdelivr.net/npm/clipboard/dist/clipboard.min.js"></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/codeseven/toastr/build/toastr.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">YOLOv3 算法的一点理解</h1><a id="logo" href="/.">四一的世界</a><p class="description">Stay hungry, Stay foolish</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">YOLOv3 算法的一点理解</h1><div class="post-meta">2018-12-28<span> | </span><span class="category"><a href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 8.7k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 35</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="post-content"><p>今天讲一讲 YOLOv3, 目标检测网络的巅峰之作, 疾如风，快如闪电。</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/sayit.jpg">
</p>

<span id="more"></span>

<h2 id="算法背景"><a href="#算法背景" class="headerlink" title="算法背景"></a>算法背景</h2><p>假设我们想对下面这张 416 X 416 大小的图片进行预测，把图中 dog、bicycle 和 car 三种物体给框出来，这涉及到以下三个过程：</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/dog.png">
</p>

<ul>
<li>怎么在图片上找出很多有价值的候选框？</li>
<li>接着判断候选框里有没有物体？</li>
<li>如果有物体的话，那么它属于哪个类别？</li>
</ul>
<p>听起来就像把大象装进冰箱，分三步走。事实上，目前的 anchor-based 机制算法例如 RCNN、Faster rcnn 以及 YOLO 算法都是这个思想。最早的时候，RCNN 是这么干的，它首先利用 Selective Search 的方法通过图片上像素之间的相似度和纹理特征进行区域合并，然后提出很多候选框并喂给 CNN 网络提取出特征向量 (embeddings)，最后利用特征向量训练 SVM 来对目标和背景进行分类。</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/RCNN.jpg" alt="image"></p>
<p>这是最早利用神经网络进行目标检测的开山之作，虽然现在看来有不少瑕疵，例如：</p>
<ul>
<li>Selective Search 会在图片上提取2000个候选区域，每个候选区域都会喂给 CNN 进行特征提取，这个过程太冗余啦，其实这些候选区域之间很多特征其实是可以共享的；</li>
<li>由于 CNN 最后一层是全连接层，因此输入图片的尺寸大小也有限制，只能进行 Crop 或者 Warp，这样一来图片就会扭曲、变形和失真；</li>
<li>在利用 SVM 分类器对候选框进行分类的时候，每个候选框的特征向量都要保留在磁盘上，很浪费空间！</li>
</ul>
<p>尽管如此，但仍不可否认它具有划时代的意义，至少告诉后人我们是可以利用神经网络进行目标检测的。后面，一些大神们在此基础上提出了很多改进，从 Fast RCNN 到 Faster RCNN 再到 Mask RCNN, 目标检测的 region proposal 过程变得越来越有针对性，并提出了著名的 RPN 网络去学习如何给出高质量的候选框，然后再去判断所属物体的类别。简单说来就是: 提出候选框，然后分类，这就是我们常说的 two-stage 算法。two-stage 算法的好处就是精度较高，但是检测速度满足不了实时性的要求。</p>
<p>在这样的背景下，YOLO 算法横空出世，江湖震惊！</p>
<h2 id="YOLO-算法简介"><a href="#YOLO-算法简介" class="headerlink" title="YOLO 算法简介"></a>YOLO 算法简介</h2><h3 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a>发展历程</h3><p>2015 年 Redmon J 等提出 YOLO 网络, 其特点是将生成候选框与分类回归合并成一个步骤, 预测时特征图被分成 7x7 个 cell, 对每个 cell 进行预测, 这就大大降低了计算复杂度, 加快了目标检测的速度, 帧率最高可达 45 fps！</p>
<p>时隔一年，Redmon J 再次提出了YOLOv2, 与前代相比, 在VOC2007 测试集上的 mAP 由 67.4% 提高到 78.6%, 然而由于一个 cell 只负责预测一个物体, 面对重叠性的目标的识别得并不够好。</p>
<p>最终在 2018 年 4 月, 作者又发布了第三个版本 YOLOv3, 在 COCO 数据集上的 mAP-50 由 YOLOv2 的 44.0% 提高到 57.9%, 与 mAP 61.1% 的 RetinaNet 相比, RetinaNet 在输入尺寸 500×500 的情况下检测速度约 98 ms/帧, 而 YOLOv3 在输入尺寸 416×416 时检测速 度可达 29 ms/帧。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/yolov3_speed.jpg">
</p>

<p>上面这张图足以秒杀一切, 说明 YOLOv3 在保证速度的前提下, 也达到了很高的准确率。</p>
<h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>作者在YOLO算法中把物体检测（object detection）问题处理成回归问题，并将图像分为S×S的网格。如果一个目标的中心落入格子，该格子就负责检测该目标。</p>
<blockquote>
<p>If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.</p>
</blockquote>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/grid.jpg">
</p>

<p>每个网格都会输出 bounding box，confidence 和 class probability map。其中：</p>
<ul>
<li>bounding box 包含4个值：x，y，w，h，（x，y）代表 box 的中心。（w，h）代表 box 的宽和高；</li>
<li>confidence 表示这个预测框中包含物体的概率，其实也是预测框与真实框之间的 iou 值;</li>
<li>class probability 表示的是该物体的类别概率，在 YOLOv3 中采用的是二分类的方法。</li>
</ul>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>下面这幅图就是 YOLOv3 网络的整体结构，在图中我们可以看到：尺寸为 416X416 的输入图片进入 Darknet-53 网络后得到了 3 个分支，这些分支在经过一系列的卷积、上采样以及合并等操作后最终得到了三个尺寸不一的 feature map，形状分别为 [13, 13, 255]、[26, 26, 255] 和 [52, 52, 255]。</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/levio.png" alt="YOLOv3 的网络结构"></p>
<p>讲了这么多，还是不如看代码来得亲切。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">YOLOv3</span>(<span class="params">input_layer</span>):</span></span><br><span class="line">    <span class="comment"># 输入层进入 Darknet-53 网络后，得到了三个分支</span></span><br><span class="line">    route_1, route_2, conv = backbone.darknet53(input_layer)</span><br><span class="line">    <span class="comment"># 见上图中的橘黄色模块(DBL)，一共需要进行5次卷积操作</span></span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1024</span>,  <span class="number">512</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>,  <span class="number">512</span>, <span class="number">1024</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1024</span>,  <span class="number">512</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>,  <span class="number">512</span>, <span class="number">1024</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1024</span>,  <span class="number">512</span>))</span><br><span class="line">    conv_lobj_branch = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">512</span>, <span class="number">1024</span>))</span><br><span class="line">    <span class="comment"># conv_lbbox 用于预测大尺寸物体，shape = [None, 13, 13, 255]</span></span><br><span class="line">    conv_lbbox = common.convolutional(conv_lobj_branch, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1024</span>, <span class="number">3</span>*(NUM_CLASS + <span class="number">5</span>)), </span><br><span class="line">                                                            activate=<span class="literal">False</span>, bn=<span class="literal">False</span>)</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>,  <span class="number">512</span>,  <span class="number">256</span>))</span><br><span class="line">    <span class="comment"># 这里的 upsample 使用的是最近邻插值方法，这样的好处在于上采样过程不需要学习，从而减少了网络参数</span></span><br><span class="line">    conv = common.upsample(conv)</span><br><span class="line">    conv = tf.concat([conv, route_2], axis=-<span class="number">1</span>)</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">768</span>, <span class="number">256</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">512</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>, <span class="number">256</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">512</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>, <span class="number">256</span>))</span><br><span class="line">    conv_mobj_branch = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">512</span>))</span><br><span class="line">    <span class="comment"># conv_mbbox 用于预测中等尺寸物体，shape = [None, 26, 26, 255]</span></span><br><span class="line">    conv_mbbox = common.convolutional(conv_mobj_branch, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">512</span>, <span class="number">3</span>*(NUM_CLASS + <span class="number">5</span>)),</span><br><span class="line">                                                            activate=<span class="literal">False</span>, bn=<span class="literal">False</span>)</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">128</span>))</span><br><span class="line">    conv = common.upsample(conv)</span><br><span class="line">    conv = tf.concat([conv, route_1], axis=-<span class="number">1</span>)</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">384</span>, <span class="number">128</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">256</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">128</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">256</span>))</span><br><span class="line">    conv = common.convolutional(conv, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">128</span>))</span><br><span class="line"></span><br><span class="line">    conv_sobj_branch = common.convolutional(conv, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">256</span>))</span><br><span class="line">    <span class="comment"># conv_sbbox 用于预测小尺寸物体，shape = [None, 52, 52, 255]</span></span><br><span class="line">    conv_sbbox = common.convolutional(conv_sobj_branch, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">256</span>, <span class="number">3</span>*(NUM_CLASS +<span class="number">5</span>)), </span><br><span class="line">                                                            activate=<span class="literal">False</span>, bn=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> [conv_sbbox, conv_mbbox, conv_lbbox]</span><br></pre></td></tr></table></figure>

<h3 id="Darknet53-结构"><a href="#Darknet53-结构" class="headerlink" title="Darknet53 结构"></a>Darknet53 结构</h3><p>Darknet-53 的主体框架如下图所示，它主要由 Convolutional 和 Residual 结构所组成。需要特别注意的是，最后三层 Avgpool、Connected 和 softmax layer 是用于在 Imagenet 数据集上作分类训练用的。当我们用 Darknet-53 层对图片提取特征时，是不会用到这三层的。</p>
<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/darknet53.png">
</p>

<p>Darknet-53 有多牛逼？看看下面这张图，作者进行了比较，得出的结论是 Darknet-53 在精度上可以与最先进的分类器进行媲美，同时它的浮点运算更少，计算速度也最快。和 ReseNet-101 相比，Darknet-53 网络的速度是前者的1.5倍；虽然 ReseNet-152 和它性能相似，但是用时却是它的2倍以上。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/darknet_speed.png">
</p>


<p>此外，Darknet-53 还可以实现每秒最高的测量浮点运算，这就意味着网络结构可以更好地利用 GPU，从而使其测量效率更高，速度也更快。</p>
<h3 id="Convolutional-结构"><a href="#Convolutional-结构" class="headerlink" title="Convolutional 结构"></a>Convolutional 结构</h3><p>Convolutional 结构其实很简单，就是普通的卷积层，其实没啥讲的。但是对于 if downsample 的情况，初学者可能觉得有点陌生， ZeroPadding2D 是什么层？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional</span>(<span class="params">input_layer, filters_shape, downsample=<span class="literal">False</span>, activate=<span class="literal">True</span>, bn=<span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> downsample:</span><br><span class="line">        input_layer = tf.keras.layers.ZeroPadding2D(((<span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>)))(input_layer)</span><br><span class="line">        padding = <span class="string">&#x27;valid&#x27;</span></span><br><span class="line">        strides = <span class="number">2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        strides = <span class="number">1</span></span><br><span class="line">        padding = <span class="string">&#x27;same&#x27;</span></span><br><span class="line">    conv = tf.keras.layers.Conv2D(filters=filters_shape[-<span class="number">1</span>], </span><br><span class="line">                        kernel_size = filters_shape[<span class="number">0</span>], </span><br><span class="line">                        strides=strides, padding=padding, use_bias=<span class="keyword">not</span> bn, </span><br><span class="line">                        kernel_regularizer=tf.keras.regularizers.l2(<span class="number">0.0005</span>),</span><br><span class="line">                        kernel_initializer=tf.random_normal_initializer(stddev=<span class="number">0.01</span>),</span><br><span class="line">                        bias_initializer=tf.constant_initializer(<span class="number">0.</span>))(input_layer)</span><br><span class="line">    <span class="keyword">if</span> bn: conv = BatchNormalization()(conv)</span><br><span class="line">    <span class="keyword">if</span> activate == <span class="literal">True</span>: conv = tf.nn.leaky_relu(conv, alpha=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> conv</span><br></pre></td></tr></table></figure>

<p>讲到 ZeroPadding2D层，我们得先了解它是什么，为什么有这个层。对于它的定义，Keras 官方给了很好的解释:</p>
<blockquote>
<p>keras.layers.convolutional.ZeroPadding2D(padding=(1, 1), data_format=None) 说明: 对2D输入（如图片）的边界填充0，以控制卷积以后特征图的大小</p>
</blockquote>
<p align="center">
    <img width="15%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/zeropadding.gif">
</p>

<p>其实就是对图片的上下左右四个边界填充0而已，padding=((top_pad, bottom_pad), (left_pad, right_pad))。 很简单吧，快打开你的 ipython 试试吧！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: x=tf.keras.layers.Input([<span class="number">416</span>,<span class="number">416</span>,<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: tf.keras.layers.ZeroPadding2D(padding=((<span class="number">1</span>,<span class="number">0</span>),(<span class="number">1</span>,<span class="number">0</span>)))(x)</span><br><span class="line">Out[<span class="number">3</span>]: &lt;tf.Tensor <span class="string">&#x27;zero_padding2d/Identity:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">417</span>, <span class="number">417</span>, <span class="number">3</span>) dtype=float32&gt;</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: tf.keras.layers.ZeroPadding2D(padding=((<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">1</span>)))(x)</span><br><span class="line">Out[<span class="number">4</span>]: &lt;tf.Tensor <span class="string">&#x27;zero_padding2d_1/Identity:0&#x27;</span> shape=(<span class="literal">None</span>, <span class="number">418</span>, <span class="number">418</span>, <span class="number">3</span>) dtype=float32&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Residual-残差模块"><a href="#Residual-残差模块" class="headerlink" title="Residual 残差模块"></a>Residual 残差模块</h3><p>残差模块最显著的特点是使用了 short cut 机制（有点类似于电路中的短路机制）来缓解在神经网络中增加深度带来的梯度消失问题，从而使得神经网络变得更容易优化。它通过恒等映射(identity mapping)的方法使得输入和输出之间建立了一条直接的关联通道，从而使得网络集中学习输入和输出之间的残差。</p>
<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/residual.png">
</p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">residual_block</span>(<span class="params">input_layer, input_channel, filter_num1, filter_num2</span>):</span></span><br><span class="line">    short_cut = input_layer</span><br><span class="line">    conv = convolutional(input_layer, filters_shape=(<span class="number">1</span>, <span class="number">1</span>, input_channel, filter_num1))</span><br><span class="line">    conv = convolutional(conv       , filters_shape=(<span class="number">3</span>, <span class="number">3</span>, filter_num1,   filter_num2))</span><br><span class="line">    residual_output = short_cut + conv</span><br><span class="line">    <span class="keyword">return</span> residual_output</span><br></pre></td></tr></table></figure>

<h2 id="提取特征"><a href="#提取特征" class="headerlink" title="提取特征"></a>提取特征</h2><p>要想详细地知道 YOLO 的预测过程，就非常有必要先来了解一下什么是特征映射 (feature map) 和特征向量 (embeddings)。</p>
<h3 id="特征映射"><a href="#特征映射" class="headerlink" title="特征映射"></a>特征映射</h3><p>当我们谈及 CNN 网络，总能听到 feature map 这个词。它也叫<em>特征映射，简单说来就是输入图像在与卷积核进行卷积操作后得到图像特征</em>。</p>
<p>一般而言，CNN 网络在对图像自底向上提取特征时，feature map 的数量(其实也对应的就是卷积核的数目) 会越来越多，而空间信息会越来越少，其特征也会变得越来越抽象。比如著名的 VGG16 网络，它的 feature map 变化就是这个样子。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/vgg16.jpg">
</p>

<blockquote>
<p>feature map 在空间尺寸上越来越小，但在通道尺寸上变得越来越深，这就是 VGG16 的特点。</p>
</blockquote>
<h3 id="特征向量"><a href="#特征向量" class="headerlink" title="特征向量"></a>特征向量</h3><p>讲到 feature map 哦，就不得不提一下人脸识别领域里经常提到的 embedding. 一般来说，它其实就是 feature map 被最后一层全连接层所提取到特征向量。早在2006年，深度学习鼻祖 hinton 就在《SCIENCE》上发表了一篇论文，首次利用自编码网络对 mnist 手写数字提取出了特征向量(一个2维或3维的向量)。值得一提的是，也是这篇论文揭开了深度学习兴起的序幕。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/mnist.jpg">
</p>

<p>下面就是上面这张图片里的数字在 CNN 空间里映射后得到的特征向量在2维和3维空间里的样子:</p>
<p>前面我们提到：CNN 网络在对图像自底向上提取特征时，得到的 feature map 一般都是在空间尺寸上越来越小，而在通道尺寸上变得越来越深。 那么，为什么要这么做？</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/ROI.jpg">
</p>

<p>其实，这就与 ROI (感兴趣区域)映射到 Feature Map 有关。在上面这幅图里：原图里的一块 ROI 在 CNN 网络空间里映射后，在 feature map 上空间尺寸会变得更小，甚至是一个点, 但是这个点的通道信息会很丰富，这些通道信息是 ROI 区域里的图片信息在 CNN 网络里映射得到的特征表示。由于图像中各个相邻像素在空间上的联系很紧密，这在空间上造成具有很大的冗余性。因此，我们往往会通过在空间上降维，而在通道上升维的方式来消除这种冗余性，尽量以最小的维度来获得它最本质的特征。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/prediction.jpg">
</p>


<blockquote>
<p>原图左上角红色 ROI 经 CNN 映射后在 feature map 空间上只得到了一个点，但是这个点有85个通道。那么，ROI的维度由原来的 [32, 32, 3] 变成了现在的 85 维，这难道又不是降维打击么？👊</p>
</blockquote>
<p>按照我的理解，这其实就是 CNN 网络对 ROI 进行特征提取后得到的一个 85 维的特征向量。这个特征向量前4个维度代表的是候选框信息，中间这个维度代表是判断有无物体的概率，后面80个维度代表的是对 80 个类别的分类概率信息。</p>
<h2 id="如何检测"><a href="#如何检测" class="headerlink" title="如何检测"></a>如何检测</h2><h3 id="多尺度检测"><a href="#多尺度检测" class="headerlink" title="多尺度检测"></a>多尺度检测</h3><p>YOLOv3 对输入图片进行了粗、中和细网格划分，以便分别实现对大、中和小物体的预测。假如输入图片的尺寸为 416X416, 那么得到粗、中和细网格尺寸分别为 13X13、26X26 和 52X52。这样一算，那就是在长宽尺寸上分别缩放了 32、16 和 8 倍。</p>
<p><img src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/multi-scale_pred.png" alt="image"></p>
<h3 id="decode-处理"><a href="#decode-处理" class="headerlink" title="decode 处理"></a>decode 处理</h3><p>YOLOv3 网络的三个分支输出会被送入 decode 函数中对 Feature Map 的通道信息进行解码。 在下面这幅图里：黑色虚线框代表先验框(anchor)，蓝色框表示的是预测框.</p>
<p align="center">
    <img width="45%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/decode_anchor.png">
</p>


<ul>
<li>$b_{h}$ 和 $b_{w}$ 分别表示预测框的长宽，$P_{h}$ 和 $P_{w}$ 分别表示先验框的长和宽。</li>
<li>$t_{x}$ 和 $t_{y}$ 表示的是物体中心距离网格左上角位置的偏移量，$C_{x}$ 和 $C_{y}$ 则代表网格左上角的坐标。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">conv_output, i=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="comment"># 这里的 i=0、1 或者 2， 以分别对应三种网格尺度</span></span><br><span class="line">    conv_shape  = tf.shape(conv_output)</span><br><span class="line">    batch_size  = conv_shape[<span class="number">0</span>]</span><br><span class="line">    output_size = conv_shape[<span class="number">1</span>]</span><br><span class="line">    conv_output = tf.reshape(conv_output, (batch_size, output_size, </span><br><span class="line">                                           output_size, <span class="number">3</span>, <span class="number">5</span> + NUM_CLASS))</span><br><span class="line">    conv_raw_dxdy = conv_output[:, :, :, :, <span class="number">0</span>:<span class="number">2</span>] <span class="comment"># 中心位置的偏移量</span></span><br><span class="line">    conv_raw_dwdh = conv_output[:, :, :, :, <span class="number">2</span>:<span class="number">4</span>] <span class="comment"># 预测框长宽的偏移量</span></span><br><span class="line">    conv_raw_conf = conv_output[:, :, :, :, <span class="number">4</span>:<span class="number">5</span>] <span class="comment"># 预测框的置信度</span></span><br><span class="line">    conv_raw_prob = conv_output[:, :, :, :, <span class="number">5</span>: ] <span class="comment"># 预测框的类别概率</span></span><br><span class="line">    <span class="comment"># 好了，接下来需要画网格了。其中，output_size 等于 13、26 或者 52</span></span><br><span class="line">    y = tf.tile(tf.<span class="built_in">range</span>(output_size, dtype=tf.int32)[:, tf.newaxis], [<span class="number">1</span>, output_size])</span><br><span class="line">    x = tf.tile(tf.<span class="built_in">range</span>(output_size, dtype=tf.int32)[tf.newaxis, :], [output_size, <span class="number">1</span>])</span><br><span class="line">    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-<span class="number">1</span>)</span><br><span class="line">    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">    xy_grid = tf.cast(xy_grid, tf.float32) <span class="comment"># 计算网格左上角的位置</span></span><br><span class="line">    <span class="comment"># 根据上图公式计算预测框的中心位置</span></span><br><span class="line">    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]</span><br><span class="line">    <span class="comment"># 根据上图公式计算预测框的长和宽大小</span></span><br><span class="line">    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]</span><br><span class="line">    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-<span class="number">1</span>) </span><br><span class="line">    pred_conf = tf.sigmoid(conv_raw_conf) <span class="comment"># 计算预测框里object的置信度</span></span><br><span class="line">    pred_prob = tf.sigmoid(conv_raw_prob) <span class="comment"># 计算预测框里object的类别概率</span></span><br><span class="line">    <span class="keyword">return</span> tf.concat([pred_xywh, pred_conf, pred_prob], axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="NMS-处理"><a href="#NMS-处理" class="headerlink" title="NMS 处理"></a>NMS 处理</h3><p>非极大值抑制（Non-Maximum Suppression，NMS），顾名思义就是抑制不是极大值的元素，说白了就是去除掉那些重叠率较高并且 score 评分较低的边界框。 NMS 的算法非常简单，迭代流程如下:</p>
<ul>
<li>流程1: 判断边界框的数目是否大于0，如果不是则结束迭代；</li>
<li>流程2: 按照 socre 排序选出评分最大的边界框 A 并取出；</li>
<li>流程3: 计算这个边界框 A 与剩下所有边界框的 iou 并剔除那些 iou 值高于阈值的边界框，重复上述步骤；</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 流程1: 判断边界框的数目是否大于0</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(cls_bboxes) &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># 流程2: 按照 socre 排序选出评分最大的边界框 A</span></span><br><span class="line">    max_ind = np.argmax(cls_bboxes[:, <span class="number">4</span>])</span><br><span class="line">    <span class="comment"># 将边界框 A 取出并剔除</span></span><br><span class="line">    best_bbox = cls_bboxes[max_ind]</span><br><span class="line">    best_bboxes.append(best_bbox)</span><br><span class="line">    cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + <span class="number">1</span>:]])</span><br><span class="line">    <span class="comment"># 流程3: 计算这个边界框 A 与剩下所有边界框的 iou 并剔除那些 iou 值高于阈值的边界框</span></span><br><span class="line">    iou = bboxes_iou(best_bbox[np.newaxis, :<span class="number">4</span>], cls_bboxes[:, :<span class="number">4</span>])</span><br><span class="line">    weight = np.ones((<span class="built_in">len</span>(iou),), dtype=np.float32)</span><br><span class="line">    iou_mask = iou &gt; iou_threshold</span><br><span class="line">    weight[iou_mask] = <span class="number">0.0</span></span><br><span class="line">    cls_bboxes[:, <span class="number">4</span>] = cls_bboxes[:, <span class="number">4</span>] * weight</span><br><span class="line">    score_mask = cls_bboxes[:, <span class="number">4</span>] &gt; <span class="number">0.</span></span><br><span class="line">    cls_bboxes = cls_bboxes[score_mask]</span><br></pre></td></tr></table></figure>

<p>最后所有取出来的边界框 A 就是我们想要的。不妨举个简单的例子：假如5个边界框及评分为: A: 0.9，B: 0.08，C: 0.8, D: 0.6，E: 0.5，设定的评分阈值为 0.3，计算步骤如下。</p>
<ul>
<li>步骤1: 边界框的个数为5，满足迭代条件；</li>
<li>步骤2: 按照 socre 排序选出评分最大的边界框 A 并取出；</li>
<li>步骤3: 计算边界框 A 与其他 4 个边界框的 iou，假设得到的 iou 值为：B: 0.1，C: 0.7, D: 0.02, E: 0.09, 剔除边界框 C;</li>
<li>步骤4: 现在只剩下边界框 B、D、E，满足迭代条件；</li>
<li>步骤5: 按照 socre 排序选出评分最大的边界框 D 并取出；</li>
<li>步骤6: 计算边界框 D 与其他 2 个边界框的 iou，假设得到的 iou 值为：B: 0.06，E: 0.8，剔除边界框 E；</li>
<li>步骤7: 现在只剩下边界框 B，满足迭代条件；</li>
<li>步骤8: 按照 socre 排序选出评分最大的边界框 B 并取出；</li>
<li>步骤9: 此时边界框的个数为零，结束迭代。</li>
</ul>
<p>最后我们得到了边界框 A、B、D，但其中边界框 B 的评分非常低，这表明该边界框是没有物体的，因此应当抛弃掉。在代码中:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># # (5) discard some boxes with low scores</span></span><br><span class="line">classes = np.argmax(pred_prob, axis=-<span class="number">1</span>)</span><br><span class="line">scores = pred_conf * pred_prob[np.arange(<span class="built_in">len</span>(pred_coor)), classes]</span><br><span class="line">score_mask = scores &gt; score_threshold</span><br></pre></td></tr></table></figure>

<blockquote>
<p>在 YOLO 算法中，NMS 的处理有两种情况：一种是所有的预测框一起做 NMS 处理，另一种情况是分别对每个类别的预测框做 NMS 处理。后者会出现一个预测框既属于类别 A 又属于类别 B 的现象，这比较适合于一个小单元格中同时存在多个物体的情况。</p>
</blockquote>
<h2 id="anchor-响应机制"><a href="#anchor-响应机制" class="headerlink" title="anchor 响应机制"></a>anchor 响应机制</h2><h3 id="K-means-聚类"><a href="#K-means-聚类" class="headerlink" title="K-means 聚类"></a>K-means 聚类</h3><p>首先需要抛出一个问题：先验框 anchor 是怎么来的？对于这点，作者在 YOLOv2 论文里给出了很好的解释：</p>
<blockquote>
<p>we run k-means clustering on the training set bounding boxes to automatically find good priors.</p>
</blockquote>
<p>其实就是使用 k-means 算法对训练集上的 boudnding box 尺度做聚类。此外，考虑到训练集上的图片尺寸不一，因此对此过程进行归一化处理。</p>
<p>k-means 聚类算法有个坑爹的地方在于，类别的个数需要人为事先指定。这就带来一个问题，先验框 anchor 的数目等于多少最合适？一般来说，anchor 的类别越多，那么 YOLO 算法就越能在不同尺度下与真实框进行回归，但是这样就会导致模型的复杂度更高，网络的参数量更庞大。</p>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/k-means.png">
</p>

<blockquote>
<p>We choose k = 5 as a good tradeoff between model complexity and high recall. If we use 9 centroids we see a much higher average IOU. This indicates that using k-means to generate our bounding box starts the model off with a better representation and makes the task easier to learn.</p>
</blockquote>
<p>在上面这幅图里，作者发现 k = 5 时就能较好地实现高召回率与模型复杂度之间的平衡。由于在 YOLOv3 算法里一共有3种尺度预测，因此只能是3的倍数，所以最终选择了 9 个先验框。这里还有个问题需要解决，k-means 度量距离的选取很关键。距离度量如果使用标准的欧氏距离，大框框就会比小框产生更多的错误。在目标检测领域，我们度量两个边界框之间的相似度往往以 IOU 大小作为标准。因此，这里的度量距离也和 IOU 有关。需要特别注意的是，这里的IOU计算只用到了 boudnding box 的长和宽。在我的代码里，是认为两个先验框的左上角位置是相重合的。(其实在这里偏移至哪都无所谓，因为聚类的时候是不考虑 anchor 框的位置信息的。)</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/MommyTalk1600745437831.jpg">
</p>

<p>如果两个边界框之间的IOU值越大，那么它们之间的距离就会越小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span>(<span class="params">boxes, k, dist=np.median,seed=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculates k-means clustering with the Intersection over Union (IoU) metric.</span></span><br><span class="line"><span class="string">    :param boxes: numpy array of shape (r, 2), where r is the number of rows</span></span><br><span class="line"><span class="string">    :param k: number of clusters</span></span><br><span class="line"><span class="string">    :param dist: distance function</span></span><br><span class="line"><span class="string">    :return: numpy array of shape (k, 2)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    rows = boxes.shape[<span class="number">0</span>]</span><br><span class="line">    distances     = np.empty((rows, k)) <span class="comment">## N row x N cluster</span></span><br><span class="line">    last_clusters = np.zeros((rows,))</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    <span class="comment"># initialize the cluster centers to be k items</span></span><br><span class="line">    clusters = boxes[np.random.choice(rows, k, replace=<span class="literal">False</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 为每个点指定聚类的类别（如果这个点距离某类别最近，那么就指定它是这个类别)</span></span><br><span class="line">        <span class="keyword">for</span> icluster <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            distances[:,icluster] = <span class="number">1</span> - iou(clusters[icluster], boxes)</span><br><span class="line">        nearest_clusters = np.argmin(distances, axis=<span class="number">1</span>)</span><br><span class="line">	<span class="comment"># 如果聚类簇的中心位置基本不变了，那么迭代终止。</span></span><br><span class="line">        <span class="keyword">if</span> (last_clusters == nearest_clusters).<span class="built_in">all</span>():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 重新计算每个聚类簇的平均中心位置，并它作为聚类中心点</span></span><br><span class="line">        <span class="keyword">for</span> cluster <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">            clusters[cluster] = dist(boxes[nearest_clusters == cluster], axis=<span class="number">0</span>)</span><br><span class="line">        last_clusters = nearest_clusters</span><br><span class="line">    <span class="keyword">return</span> clusters,nearest_clusters,distances</span><br></pre></td></tr></table></figure>

<h3 id="正负样本分配"><a href="#正负样本分配" class="headerlink" title="正负样本分配"></a>正负样本分配</h3><ul>
<li>如果 Anchor 与 Ground-truth Bounding Boxes 的 IoU &gt; 0.3，标定为正样本;</li>
<li>在第 1 种规则下基本能够产生足够多的样本，但是如果它们的 iou 不大于 0.3，那么只能把 iou 最大的那个 Anchor 标记为正样本，这样便能保证每个 Ground-truth 框都至少匹配一个先验框。</li>
</ul>
<p>按照上述原则，一个 ground-truth 框会同时与多个先验框进行匹配。记得之前有人问过我，为什么不能只用 iou 最大的 anchor 去负责预测该物体？其实我想回答的是，如果按照这种原则去分配正负样本，那么势必会导致正负样本的数量极其不均衡（正样本特别少，负样本特别多），这将使得模型在预测时会出现大量漏检的情况。实际上很多目标检测网络都会避免这种情况，并且尽量保持正负样本的数目相平衡。例如，SSD 网络就使用了 hard negative mining 的方法对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差较大的 top-k 作为训练的负样本，以保证正负样本的比例接近1:3。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>): <span class="comment"># 针对 3 种网格尺寸</span></span><br><span class="line">    <span class="comment"># 设定变量，用于存储每种网格尺寸下 3 个 anchor 框的中心位置和宽高</span></span><br><span class="line">    anchors_xywh = np.zeros((self.anchor_per_scale, <span class="number">4</span>))</span><br><span class="line">    <span class="comment"># 将这 3 个 anchor 框都偏移至网格中心</span></span><br><span class="line">    anchors_xywh[:, <span class="number">0</span>:<span class="number">2</span>] = np.floor(bbox_xywh_scaled[i, <span class="number">0</span>:<span class="number">2</span>]).astype(np.int32) + <span class="number">0.5</span></span><br><span class="line">    <span class="comment"># 填充这 3 个 anchor 框的宽和高</span></span><br><span class="line">    anchors_xywh[:, <span class="number">2</span>:<span class="number">4</span>] = self.anchors[i]</span><br><span class="line">    <span class="comment"># 计算真实框与 3 个 anchor 框之间的 iou 值</span></span><br><span class="line">    iou_scale = self.bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)</span><br><span class="line">    iou.append(iou_scale)</span><br><span class="line">    <span class="comment"># 找出 iou 值大于 0.3 的 anchor 框</span></span><br><span class="line">    iou_mask = iou_scale &gt; <span class="number">0.3</span></span><br><span class="line">    exist_positive = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">any</span>(iou_mask): <span class="comment"># 规则 1: 对于那些 iou &gt; 0.3 的 anchor 框，做以下处理</span></span><br><span class="line">    	<span class="comment"># 根据真实框的坐标信息来计算所属网格左上角的位置</span></span><br><span class="line">        xind, yind = np.floor(bbox_xywh_scaled[i, <span class="number">0</span>:<span class="number">2</span>]).astype(np.int32)</span><br><span class="line">        label[i][yind, xind, iou_mask, :] = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 填充真实框的中心位置和宽高</span></span><br><span class="line">        label[i][yind, xind, iou_mask, <span class="number">0</span>:<span class="number">4</span>] = bbox_xywh</span><br><span class="line">        <span class="comment"># 设定置信度为 1.0，表明该网格包含物体</span></span><br><span class="line">        label[i][yind, xind, iou_mask, <span class="number">4</span>:<span class="number">5</span>] = <span class="number">1.0</span></span><br><span class="line">        <span class="comment"># 设置网格内 anchor 框的类别概率，做平滑处理</span></span><br><span class="line">        label[i][yind, xind, iou_mask, <span class="number">5</span>:] = smooth_onehot</span><br><span class="line">        exist_positive = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exist_positive: <span class="comment"># 规则 2: 所有 iou 都不大于0.3， 那么只能选择 iou 最大的</span></span><br><span class="line">    	best_anchor_ind = np.argmax(np.array(iou).reshape(-<span class="number">1</span>), axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>按照上面两种规则标记出正样本后，剩下的都是负样本了。这些负样本是不会参与到边界框损失和分类损失的计算中去，而只会参与到置信度损失的计算（因为你需要告诉神经网络什么是负样本）。在这里，你不必纠结 Anchor 是否能够准确地框到物体。你只要关心 Anchor 能不能框到物体，如果框到很多了(比如iou&gt;0.3)，那么它就是个正样本了，否则就不是了。 后面的损失函数会进一步告诉神经网络怎么去做精确的尺寸和位置回归，并给出一个置信度评分。最后，那些评分比较低和重叠度较高的预测框就会被 NMS 算法给过滤掉。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>在 YOLOv3 中，作者将目标检测任务看作目标区域预测和类别预测的回归问题, 因此它的损失函数也有些与众不同。对于损失函数, Redmon J 在论文中并 没有进行详细的讲解。但通过对 darknet 源代码的解读，可以总结得到 YOLOv3 的损失函数如下:</p>
<ul>
<li>置信度损失，判断预测框有无物体；</li>
<li>框回归损失，仅当预测框内包含物体时计算；</li>
<li>分类损失，判断预测框内的物体属于哪个类别</li>
</ul>
<h3 id="置信度损失"><a href="#置信度损失" class="headerlink" title="置信度损失"></a>置信度损失</h3><p>YOLOv3 直接优化置信度损失是为了让模型去学习分辨图片的背景和前景区域，这类似于在 Faster rcnn 里 RPN 功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])</span><br><span class="line"><span class="comment"># 找出与真实框 iou 值最大的预测框</span></span><br><span class="line">max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-<span class="number">1</span>), axis=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 如果最大的 iou 小于阈值，那么认为该预测框不包含物体,则为背景框</span></span><br><span class="line">respond_bgd = (<span class="number">1.0</span> - respond_bbox) * tf.cast( max_iou &lt; IOU_LOSS_THRESH, tf.float32 )</span><br><span class="line">conf_focal = tf.<span class="built_in">pow</span>(respond_bbox - pred_conf, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 计算置信度的损失（我们希望假如该网格中包含物体，那么网络输出的预测框置信度为 1，无物体时则为 0。</span></span><br><span class="line">conf_loss = conf_focal * (</span><br><span class="line">     respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)</span><br><span class="line">            +</span><br><span class="line">     respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>判定的规则很简单：如果一个预测框与所有真实框的 iou 都小于某个阈值，那么就判定它是背景，否则为前景（包含物体）。</p>
<h3 id="分类损失"><a href="#分类损失" class="headerlink" title="分类损失"></a>分类损失</h3><p>这里分类损失采用的是二分类的交叉熵，即把所有类别的分类问题归结为是否属于这个类别，这样就把多分类看做是二分类问题。这样做的好处在于排除了类别的互斥性，特别是解决了因多个类别物体的重叠而出现漏检的问题。</p>
<p align="center">
    <img width="40%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/MommyTalk1600745936778.jpg">
</p>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">respond_bbox  = label[:, :, :, :, <span class="number">4</span>:<span class="number">5</span>]</span><br><span class="line">prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)</span><br></pre></td></tr></table></figure>

<h3 id="框回归损失"><a href="#框回归损失" class="headerlink" title="框回归损失"></a>框回归损失</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">respond_bbox  = label[:, :, :, :, <span class="number">4</span>:<span class="number">5</span>]  <span class="comment"># 置信度，判断网格内有无物体</span></span><br><span class="line">...</span><br><span class="line">bbox_loss_scale = <span class="number">2.0</span> - <span class="number">1.0</span> * label_xywh[:, :, :, :, <span class="number">2</span>:<span class="number">3</span>] * label_xywh[:, :, :, :, <span class="number">3</span>:<span class="number">4</span>] / (input_size ** <span class="number">2</span>)</span><br><span class="line">giou_loss = respond_bbox * bbox_loss_scale * (<span class="number">1</span> - giou)</span><br></pre></td></tr></table></figure>

<ul>
<li>边界框的尺寸越小，bbox_loss_scale 的值就越大。实际上，我们知道 YOLOv1 里作者在 loss 里对宽高都做了开根号处理，这是为了弱化边界框尺寸对损失值的影响；</li>
<li>respond_bbox 的意思是如果网格单元中包含物体，那么就会计算边界框损失；</li>
<li>两个边界框之间的 GIoU 值越大，giou 的损失值就会越小, 因此网络会朝着预测框与真实框重叠度较高的方向去优化。</li>
</ul>
<p>受 g-darknet 所启示，将原始 iou loss 替换成了 giou loss ，检测精度提高了大约 1 个百分点。 GIoU 的好处在于，改进了预测框与先验框的距离度量方式。</p>
<h4 id="GIoU-的背景介绍"><a href="#GIoU-的背景介绍" class="headerlink" title="GIoU 的背景介绍"></a>GIoU 的背景介绍</h4><p>这篇论文 出自于 CVPR 2019，这篇论文提出了一种优化边界框的新方式 —— GIoU (Generalized IoU，广义 IoU )。边界框一般由左上角和右下角坐标所表示，即 (x1,y1,x2,y2)。那么，你发现这其实也是一个向量。向量的距离一般可以 L1 范数或者 L2 范数来度量。但是在L1及L2范数取到相同的值时，实际上检测效果却是差异巨大的，直接表现就是预测和真实检测框的IoU值变化较大，这说明L1和L2范数不能很好的反映检测效果。</p>
<blockquote>
<p>L1 范数：向量元素的绝对值之和；<br>L2 范数：即欧几里德范数，常用于计算向量的长度；</p>
</blockquote>
<p align="center">
    <img width="80%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/giou.png">
</p>

<p>当 L1 或 L2 范数都相同的时候，发现 IoU 和 GIoU 的值差别都很大，这表明使用 L 范数来度量边界框的距离是不合适的。在这种情况下，学术界普遍使用 IoU 来衡量两个边界框之间的相似性。作者发现使用 IoU 会有两个缺点，导致其不太适合做损失函数:</p>
<ul>
<li>预测框和真实框之间没有重合时，IoU 值为 0， 导致优化损失函数时梯度也为 0，意味着无法优化。例如，场景 A 和场景 B 的 IoU 值都为 0，但是显然场景 B 的预测效果较 A 更佳，因为两个边界框的距离更近( L 范数更小)。</li>
</ul>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/giou_AB.png">
</p>

<blockquote>
<p>尽管场景 A 和场景 B 的 IoU 值都为 0，但是场景 B 的预测效果较 A 更佳，这是因为两个边界框的距离更近。</p>
</blockquote>
<ul>
<li>即使预测框和真实框之间相重合且具有相同的 IoU 值时，检测的效果也具有较大差异，如下图所示。</li>
</ul>
<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/gt_pred.png">
</p>

<blockquote>
<p>上面三幅图的 IoU = 0.33， 但是 GIoU 值分别是 0.33, 0.24 和 -0.1， 这表明如果两个边界框重叠和对齐得越好，那么得到的 GIoU 值就会越高。</p>
</blockquote>
<h4 id="GIoU-的计算公式"><a href="#GIoU-的计算公式" class="headerlink" title="GIoU 的计算公式"></a>GIoU 的计算公式</h4><p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/giou_algorithm.png">
</p>

<p>the smallest enclosing convex object C 指的是最小闭合凸面 C，例如在上述场景 A 和 B 中，C 的形状分别为:</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/new_AB.png">
</p>

<blockquote>
<p>图中绿色包含的区域就是最小闭合凸面 C，the smallest enclosing convex object。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bbox_giou</span>(<span class="params">boxes1, boxes2</span>):</span></span><br><span class="line">	......</span><br><span class="line">    <span class="comment"># 计算两个边界框之间的 iou 值</span></span><br><span class="line">    iou = inter_area / union_area</span><br><span class="line">    <span class="comment"># 计算最小闭合凸面 C 左上角和右下角的坐标</span></span><br><span class="line">    enclose_left_up = tf.minimum(boxes1[..., :<span class="number">2</span>], boxes2[..., :<span class="number">2</span>])</span><br><span class="line">    enclose_right_down = tf.maximum(boxes1[..., <span class="number">2</span>:], boxes2[..., <span class="number">2</span>:])</span><br><span class="line">    enclose = tf.maximum(enclose_right_down - enclose_left_up, <span class="number">0.0</span>)</span><br><span class="line">    <span class="comment"># 计算最小闭合凸面 C 的面积</span></span><br><span class="line">    enclose_area = enclose[..., <span class="number">0</span>] * enclose[..., <span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 根据 GIoU 公式计算 GIoU 值</span></span><br><span class="line">    giou = iou - <span class="number">1.0</span> * (enclose_area - union_area) / enclose_area</span><br><span class="line">    <span class="keyword">return</span> giou</span><br></pre></td></tr></table></figure>

<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p>训练神经网络尤其是深度神经网络所面临的一个问题是，梯度消失或梯度爆炸，也就是说 当你训练深度网络时，导数或坡度有时会变得非常大，或非常小甚至以指数方式变小，这个时候我们看到的损失就会变成了 NaN。假设你正在训练下面这样一个极深的神经网络，为了简单起见，这里激活函数 g(z) = z 并且忽略偏置参数。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/x1x2.png">
</p>

<p>这里我们首先假定 g(z)=z, b[l]=0，所以对目标输出有：</p>
<p align="center">
    <img width="35%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/MommyTalk1600746076683.jpg">
</p>

<p>其实这里直观的理解是：如果权重 W 只比 1 略大一点，或者说只比单位矩阵大一点，深度神经网络的输出将会以爆炸式增长，而如果 W 比 1 略小一点，可能是 0.9, 0.9，每层网络的输出值将会以指数级递减。因此合适的初始化权重值就显得尤为重要! 下面就写个简单的代码给大家演示一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x = np.random.randn(<span class="number">2000</span>, <span class="number">800</span>) * <span class="number">0.01</span> <span class="comment"># 制作输入数据</span></span><br><span class="line">stds = [<span class="number">0.1</span>, <span class="number">0.05</span>, <span class="number">0.01</span>, <span class="number">0.005</span>, <span class="number">0.001</span>] <span class="comment"># 尝试使用不同标准差，这样初始权重大小也不一样</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, std <span class="keyword">in</span> <span class="built_in">enumerate</span>(stds):</span><br><span class="line">    <span class="comment"># 第一层全连接层</span></span><br><span class="line">    dense_1 = tf.keras.layers.Dense(<span class="number">750</span>, kernel_initializer=tf.random_normal_initializer(std), activation=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">    output_1 = dense_1(x)</span><br><span class="line">    <span class="comment"># 第二层全连接层</span></span><br><span class="line">    dense_2 = tf.keras.layers.Dense(<span class="number">700</span>, kernel_initializer=tf.random_normal_initializer(std), activation=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">    output_2 = dense_2(output_1)</span><br><span class="line">    <span class="comment"># 第三层全连接层</span></span><br><span class="line">    dense_3 = tf.keras.layers.Dense(<span class="number">650</span>, kernel_initializer=tf.random_normal_initializer(std), activation=<span class="string">&#x27;tanh&#x27;</span>)</span><br><span class="line">    output_3 = dense_3(output_2).numpy().flatten()</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="built_in">len</span>(stds), i+<span class="number">1</span>)</span><br><span class="line">    plt.hist(output_3, bins=<span class="number">60</span>, <span class="built_in">range</span>=[-<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;std = %.3f&#x27;</span> %std)</span><br><span class="line">    plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="60%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/NAN.png">
</p>

<p>我们可以看到当标准差较大( std = 0.1 和 0.05 )时，几乎所有的输出值集中在 -1 或1 附近，这表明此时的神经网络发生了梯度爆炸；当标准差较小( std = 0.005 和 0.001）时，我们看到输出值迅速向 0 靠拢，这表明此时的神经网络发生了梯度消失。其实笔者也曾在 YOLOv3 网络里做过实验，初始化权重的标准差如果太大或太小，都容易出现 NaN 。</p>
<h3 id="学习率的设置"><a href="#学习率的设置" class="headerlink" title="学习率的设置"></a>学习率的设置</h3><p>学习率是最影响性能的超参数之一，如果我们只能调整一个超参数，那么最好的选择就是它。 其实在我们的大多数的炼丹过程中，遇到 loss 变成 NaN 的情况大多数是由于学习率选择不当引起的。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/lr.png">
</p>


<p>有句话讲得好啊，步子大了容易扯到蛋。由于神经网络在刚开始训练的时候是非常不稳定的，因此刚开始的学习率应当设置得很低很低，这样可以保证网络能够具有良好的收敛性。但是较低的学习率会使得训练过程变得非常缓慢，因此这里会采用以较低学习率逐渐增大至较高学习率的方式实现网络训练的“热身”阶段，称为 warmup stage。但是如果我们使得网络训练的 loss 最小，那么一直使用较高学习率是不合适的，因为它会使得权重的梯度一直来回震荡，很难使训练的损失值达到全局最低谷。因此最后采用了这篇论文里[8]的 cosine 的衰减方式，这个阶段可以称为 consine decay stage。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> global_steps &lt; warmup_steps:</span><br><span class="line">    lr = global_steps / warmup_steps *cfg.TRAIN.LR_INIT</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    lr = cfg.TRAIN.LR_END + <span class="number">0.5</span> * (cfg.TRAIN.LR_INIT - cfg.TRAIN.LR_END) * (</span><br><span class="line">        (<span class="number">1</span> + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p align="center">
    <img width="37%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/yolov3_lr.png">
</p>

<h3 id="加载预训练模型"><a href="#加载预训练模型" class="headerlink" title="加载预训练模型"></a>加载预训练模型</h3><p>目前针对目标检测的主流做法是基于 Imagenet 数据集预训练的模型来提取特征，然后在 COCO 数据集进行目标检测fine-tunning训练（比如 yolo 算法)，也就是大家常说的迁移学习。其实迁移学习是建立在数据集分布相似的基础上的，像 yymnist 这种与 COCO 数据集分布完全不同的情况，就没有必要加载 COCO 预训练模型的必要了吧。</p>
<p>在 tensorflow-yolov3 版本里，由于 README 里训练的是 VOC 数据集，因此推荐加载预训练模型。由于在 YOLOv3 网络的三个分支里的最后卷积层与训练的类别数目有关，因此除掉这三层的网络权重以外，其余所有的网络权重都加载进来了。</p>
<p>下面是 tensorflow-yolov3 在 PASCAL VOC 2012 上比赛刷的成绩，最后进了榜单的前十名。</p>
<p align="center">
    <img width="50%" src="https://gitee.com/yunyang1994/BlogSource/raw/master/hexo/source/images/YOLOv3/yolov3_mAP.png">
</p>


<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li>[1] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation</a>, CVPR 2014</li>
<li>[2] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.01497">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>, CVPR 2016</li>
<li>[3] Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.02640">You Only Look Once: Unified, Real-Time Object Detection</a>, CVPR 2016</li>
<li>[4] Joseph Redmon, Ali Farhadi. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1612.08242">YOLO9000: Better, Faster, Stronger</a>, CVPR 2017</li>
<li>[5] Joseph Redmon, Ali Farhadi. <a target="_blank" rel="noopener" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">YOLOv3: An Incremental Improvement</a></li>
<li>[6] Conv2DTranspose 层，<a target="_blank" rel="noopener" href="https://keras-cn.readthedocs.io/en/latest/layers/convolutional_layer/">Keras 中文文档</a>.</li>
<li>[7] Rezatofighi, Hamid. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1902.09630.pdf">Generalized Intersection over Union, A Metric and A Loss for Bounding Box Regression</a>, CVPR 2018</li>
<li>[8] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, Mu Li. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.01187.pdf">Bag of Tricks for Image Classification with Convolutional Neural Networks</a></li>
</ul>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a class="article-share-link" data-url="https://yunyang1994.github.io/2018/12/28/YOLOv3/" data-id="ckofiohwd0035n3rah4cwg15d" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACLElEQVR42u3aQW7DMAwEwPz/0+61QGt3V7ILWBqdgtiRMzkwFMnPJ17Ht/Xz/bM7f1693jP51NTCwMB4LeO4XPk97Q9xvX/9XAwMjA0YZw9OHjkfNK+fdX0/BgYGRs4YC6YYGBgYTzDGQmebAmJgYGCMHWLbgtpYYH38LI6BgfFCxl2HzCdeP9LfwMDAeBXjKFcbfOdbDtG3wsDAWJqRB7gZZNJOSA7Vfwx/YGBgbMzIryaAZIezwF1krxgYGMsx8oJ+W+5v25ljKSkGBsaejP/hnX2hvPSGgYGxD6NtQ44ddNsAXV/FwMBYlHGdbLXl/pwXlc/yHTAwMJZmtNvlNa68bZkX2ooyHAYGxkKMdgxiLB3Mf5ScEc2MYGBgLMHIM6s8ROZFtLbQdkOCiIGB8VrGTNNxLBS2A2GDMyMYGBhLMNrhhpmk8AhW20wtMlwMDIzXMmaGLWYOvTMtBAwMjD0Z7ZF1Jji2LdKCioGBsQEjbxy2ATQv5OWNzMFyGwYGxkKMvG3QpoN3DZn9shsGBsY2jLxxOFYUS0Lq4D4YGBibMeZLbzOkNtxjYGCszTjK1R5E2x3GQjwGBsbajPlo3TYax5LLseEzDAyMlRh3jWq1oXxm5OK0MYCBgbE0Y6agXwxvTf8DFMMWGBgYGzPa0NyOhSVp5aeN/RgYGJsxkgQupyahNgr3GBgYGzDyUn7bmBys8D1XbsPAwHgho92uDbgzx+C8VYCBgbEo4wsQP/oIYFBjxAAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/YOLOv3/"><i class="fa fa-tag"></i>YOLOv3</a></div><div class="post-nav"><a class="pre" href="/2019/05/16/Tensorflow-%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8C%96-tflite/">TensorFlow 模型转化 tflite</a><a class="next" href="/2018/11/12/Unet/">医学图片分割网络 —— Unet</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/">图像分割</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/">图像处理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/">姿态估计</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">目标检测</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/">立体视觉</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/" style="font-size: 15px;">仿射变换</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E8%A7%86%E8%A7%89/" style="font-size: 15px;">立体视觉</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E5%8F%82%E6%95%B0/" style="font-size: 15px;">相机参数</a> <a href="/tags/%E7%9B%B8%E6%9C%BA%E4%BD%8D%E5%A7%BF/" style="font-size: 15px;">相机位姿</a> <a href="/tags/%E8%A7%86%E8%A7%89-Slam/" style="font-size: 15px;">视觉 Slam</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">深度估计</a> <a href="/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/" style="font-size: 15px;">角点检测</a> <a href="/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">全卷积网络</a> <a href="/tags/Skip-Connection/" style="font-size: 15px;">Skip Connection</a> <a href="/tags/Faster-rcnn/" style="font-size: 15px;">Faster-rcnn</a> <a href="/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/" style="font-size: 15px;">梯度下降</a> <a href="/tags/%E8%A7%86%E5%B7%AE%E4%BC%B0%E8%AE%A1/" style="font-size: 15px;">视差估计</a> <a href="/tags/%E7%AB%8B%E4%BD%93%E5%8C%B9%E9%85%8D/" style="font-size: 15px;">立体匹配</a> <a href="/tags/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB/" style="font-size: 15px;">汉明距离</a> <a href="/tags/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/" style="font-size: 15px;">三维重建</a> <a href="/tags/%E5%A4%9A%E5%8D%A1GPU%E8%AE%AD%E7%BB%83/" style="font-size: 15px;">多卡GPU训练</a> <a href="/tags/Unet-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/" style="font-size: 15px;">Unet 网络结构</a> <a href="/tags/%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/" style="font-size: 15px;">移动端部署</a> <a href="/tags/%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/" style="font-size: 15px;">人脸矫正</a> <a href="/tags/mnist-%E5%88%86%E7%B1%BB/" style="font-size: 15px;">mnist 分类</a> <a href="/tags/rotated-object-detection/" style="font-size: 15px;">rotated object detection</a> <a href="/tags/%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">可变形卷积网络</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/%E7%B2%BE%E7%A1%AE%E7%8E%87%E3%80%81%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CROC%E6%9B%B2%E7%BA%BF/" style="font-size: 15px;">精确率、召回率和ROC曲线</a> <a href="/tags/hourglass-%E7%BD%91%E7%BB%9C/" style="font-size: 15px;">hourglass 网络</a> <a href="/tags/%E6%9C%AC%E8%B4%A8%E7%9F%A9%E9%98%B5/" style="font-size: 15px;">本质矩阵</a> <a href="/tags/YOLOv3/" style="font-size: 15px;">YOLOv3</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/04/20/%E4%BF%AE%E6%94%B9-YOLOv5-%E6%BA%90%E7%A0%81%E5%9C%A8-DOTAv1.5-%E9%81%A5%E6%84%9F%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%9B%E8%A1%8C%E6%97%8B%E8%BD%AC%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/">修改 YOLOv5 源码在 DOTAv1.5 遥感数据集上进行旋转目标检测</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/12/19/%E7%94%A8-Python-%E6%89%8B%E6%92%B8%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%8D%95%E7%9B%AE-Slam-%E4%BE%8B%E5%AD%90/">用 Python 手撸一个单目视觉里程计的例子</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/10/10/2D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E8%BF%87%E5%8E%BB%EF%BC%8C%E7%8E%B0%E5%9C%A8%E5%92%8C%E6%9C%AA%E6%9D%A5/">2D人体姿态估计的总结和梳理</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/16/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E4%B9%8B%E4%BA%BA%E8%84%B8%E7%9F%AB%E6%AD%A3/">人脸识别之人脸矫正</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/07/TensorFlow-%E7%9A%84%E5%A4%9A%E5%8D%A1-GPU-%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%88%B6/">TensorFlow 的多卡 GPU 训练机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/22/AffineTransformation/">说说图像的仿射变换</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/21/SGD/">能不能用梯度下降法求解平方根 ？</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/20/SGM_02/">手写双目立体匹配 SGM 算法（下)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/17/SGM_01/">手写双目立体匹配 SGM 算法（上)</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/29/StereoVision/">双目测距和三维重建</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/YunYang1994" title="YunYang1994's GitHub" target="_blank">YunYang1994's GitHub</a><ul></ul><a href="https://blog.devtang.com" title="猿辅导创始人唐巧的博客" target="_blank">猿辅导创始人唐巧的博客</a><ul></ul><a href="http://pengzhihui.xyz" title="稚晖的个人站" target="_blank">稚晖的个人站</a><ul></ul><a href="https://wizyoung.github.io" title="CaptainChen" target="_blank">CaptainChen</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">四一的世界.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox/dist/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js" successtext="复制成功!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>